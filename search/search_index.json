{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"Databases/redis/","title":"Redis","text":""},{"location":"Databases/redis/#setup","title":"Setup","text":""},{"location":"Databases/redis/#docker","title":"Docker","text":"<pre><code># pull the image from docker hub\n$ docker run --name redis-learn -p 6370:6370 -d redis \n\n# connect to container and redis shell\n$ docker exec -it redis-learn redis-cli\n</code></pre>"},{"location":"Databases/redis/#to-benchmark","title":"To benchmark","text":"<pre><code>docker exec -it redis-learn redis-benchmark -n 1000 -d 10000\n# -d for bytes of data\n\nredis-benchmark -n 1000 -d 10000\n</code></pre>"},{"location":"Databases/redis/#to-set-max-memory-limit","title":"To set max memory limit","text":"<pre><code>&gt; config set maxmemory 128M\n</code></pre>"},{"location":"Databases/redis/#set-get-value","title":"Set &amp; Get value","text":"<ul> <li>set key value</li> <li>get key value</li> <li>check whether key exists, returns integer</li> <li>0 - false | 1 - true</li> </ul> <pre><code>&gt; set name \"uday\"\nOK\n\n&gt; get name\n\"uday\"\n\n&gt; exists name\n(integer) 1\n</code></pre>"},{"location":"Databases/redis/#key-related-ops","title":"Key related ops","text":"<ul> <li>Get all keys</li> <li>Delete all keys ( sync | async )</li> </ul> <pre><code>&gt; keys *\n1) \"name\"\n\n&gt; flushall \n# options : async|sync\nOK\n</code></pre>"},{"location":"Databases/redis/#set-key-with-expiry-time","title":"Set key with expiry time","text":"<pre><code># After 5 seconds, this  key will be deleted\n&gt; set key \"value\" EX [expiry time in seconds]\n&gt; get key\n&gt; exists key\n\n# expire after X seconds\n&gt; set key \"value\"\n&gt; get key\n&gt; expire key X\n\n# to check time remaining \n&gt; ttl key\n\n# Another way, to expire after 10 seconds\n&gt; setex key 10 \"value\"\n</code></pre>"},{"location":"Databases/redis/#delete-key","title":"Delete Key","text":"<pre><code>&gt; del name\n(integer) 1\n</code></pre>"},{"location":"Databases/redis/#set-get-multiple-values","title":"Set &amp; Get multiple values","text":"<pre><code>&gt; mset first \"uday\" last \"yadav\"\n\n&gt; mget first last\n1) \"uday\"\n2) \"yadav\"\n</code></pre>"},{"location":"Databases/redis/#miscellaneous","title":"Miscellaneous","text":"<pre><code>&gt; set name \"uday yadav\"\n\n# Specifies the range : from 0th char to 3rd char\n&gt; getrange name 0 3\n\"uda\"\n&gt; strlen name\n10\n\n# Append to key\n&gt; set name \"uday\"\n&gt; append name \" yadav\"\n&gt; get name\n\"uday yadav\"\n</code></pre>"},{"location":"Databases/redis/#maths-operations","title":"Maths operations","text":"<pre><code>&gt; set count 1\n\n&gt; incr count\n(integer) 2\n\n&gt; incrby count 10\n(integer) 12\n\n&gt; decr count \n(integer) 11\n\n&gt; decrby count 2\n(integer) 9\n\n&gt; set pi 3.14\n&gt; incrbyfloat pi 0.1\n\"3.24\"\n</code></pre>"},{"location":"Databases/redis/#lists-in-redis","title":"Lists in Redis","text":"<pre><code># lpush adds value infront\n&gt; lpush country india\n&gt; lpush country USA\n&gt; lpush country UK\n\n# get the element at index, start from 0\n&gt; lindex country 2\n\n# to add values to the bottom\n&gt; rpush country Australia\n\n# to get all values in list\n&gt; lrange country 0 -1\n1) \"UK\"\n2) \"USA\"\n3) \"india\"\n4) \"Australia\"\n\n# to get first 2 values\n&gt; lrange country 0 1\n\n# get list length\n&gt; llen country\n\n# use lpop and rpop to remove the data from top and bottom\n# and it returns the element\n&gt; lpop country\n&gt; rpop country\n\n# to change a key's value\n&gt; lset country 0 germany\n\n# to insert values before and after \n&gt; linsert country before \"germany\" \"new zealand\"\n&gt; linsert country after \"germany\" \"UAE\"\n\n# use lpushx and rpushx to add key to list only if it exists\n# else returns 0\n</code></pre>"},{"location":"Databases/redis/#sorting-list","title":"Sorting List","text":"<pre><code># Alpha is needed only for strings, nothing for integers\n&gt; sort country ALPHA\n&gt; sort country desc ALPHA\n</code></pre>"},{"location":"Databases/redis/#sets-in-redis","title":"Sets in Redis","text":"<pre><code>&gt; sadd tech golang\n(integer) 1\n\n# when you do it again\n&gt; sadd tech golang\n(integer) 0\n\n&gt; sadd tech postgis python aws\n&gt; sadd tech1 aws python mysql nodejs\n\n# to see all members\n&gt; smembers tech\n\n# to get the length of set\n&gt; scard tech\n\n# to search the set\n&gt; sismember tech aws\n1\n\n# to get the diff between to sets\n&gt; sdiff tech tech1\n\n# to store the difference btw 2 sets to a new set \n&gt; sdiffstore tech tech1 [more set]\n\n# to check intersection\n&gt; sinter tech tech1\n</code></pre>"},{"location":"Databases/redis/#sorted-set-redis","title":"Sorted Set Redis","text":"<pre><code># add key values\n&gt; zadd users 10 uday \n&gt; zadd users 5 uday1 8 uday2\n\n# to get all users\n&gt; zrange users 0 -1\n&gt; zrange users 0 -1 withscores\n# in reverse order\n&gt; zrevrange users 0 -1\n\n# to get the length of the string\n&gt; zcard users\n3\n\n# to get key's value over a range\n&gt; zcount users 0 2\n\n# to remove member\n&gt; zrem users uday\n</code></pre>"},{"location":"Databases/redis/#hashes-in-redis","title":"Hashes in Redis","text":"<pre><code># add keys to a set\n&gt; hset myhash name uday\n&gt; hset myhash email dev117uday@gmail.com\n\n# get all keys from hashset\n&gt; hkeys myhash\n\n# to get all values\n&gt; hvals myhash\n\n# get value\n&gt; hget myhash name\n\n# to check for keys \n&gt; hexists myhash name\n\n# check the length    \n&gt; hlen myhash\n\n# to set multiple values at once\n&gt; hmset myhash country india phone_no 9810039759 age 24\n\n# to get multiple values\n&gt; hmget myhash country name email\n\n# increment some value\n&gt; hincrby myhash age 2\n\n# to delete key from set\n&gt; hdel myhash age\n\n# to avoid over writting the values\n&gt; hsetnx myhash name Uday\n</code></pre>"},{"location":"Databases/redis/#transaction","title":"Transaction","text":"<pre><code># to go in transaction mode\n&gt; multi\n(TX)&gt; set key1 uday\n(TX)&gt; set key2 yadav\n(TX)&gt; set key3 dev117uday\n(TX)&gt; exec\n1) OK\n2) OK\n3) OK\n\n# to discard transaction\ndiscard\n</code></pre> <p>Pub/Sub</p> <pre><code># To listen to a channel\n&gt; subscribe my-chat\n\n# To publish to a channel\n&gt; publish my-chat \"hello world\"\n\n# to subscribe to channel with patterns in name\n&gt; psubscribe chats*\n&gt; psubscribe h?llo\n</code></pre> <ul> <li>If no one is sub to the channel you specify in publish, it returns 0</li> </ul>"},{"location":"Databases/redis/#geospatial-data","title":"GeoSpatial Data","text":"<pre><code># add geo spatial data in long : lat\n&gt; GEOADD maps 77.216721 28.644800 delhi\n&gt; GEOADD maps 72.877426 19.076090 mumbai\n\n# data is tored in sorted set data structure\n&gt; zrange maps 0 -1\n1) \"mumbai\"\n2) \"delhi\"\n\n# to get the geohash of city\n&gt; GEOHASH maps delhi\n1) \"ttnfvnes010\"\n\n# to get long:lat\n&gt; GEOPOS maps delhi\n1) 1) \"77.21672326326370239\"\n   2) \"28.64479890853065314\"\n\n# to get distance, in meter (default)\n&gt; GEODIST maps delhi mumbai\n\"1151873.1929\"\n\n&gt; GEODIST maps delhi mumbai km\n\"1151.8732\"\n\n# within distance\n\n&gt; GEORADIUS maps 77.216721 28.644800 2000 km\n1) \"delhi\"\n2) \"mumbai\n\n&gt; GEORADIUS maps 77.216721 28.644800 2000 km withcoord\n1) 1) \"delhi\"\n   2) 1) \"77.21672326326370239\"\n      2) \"28.64479890853065314\"\n2) 1) \"mumbai\"\n   2) 1) \"72.87742406129837036\"\n      2) \"19.07608965708350723\"\n\n&gt; GEORADIUS maps 77.216721 28.644800 2000 km withcoord withdist\n1) 1) \"delhi\"\n   2) \"0.0003\"\n   3) 1) \"77.21672326326370239\"\n      2) \"28.64479890853065314\"\n2) 1) \"mumbai\"\n   2) \"1151.8732\"\n   3) 1) \"72.87742406129837036\"\n      2) \"19.07608965708350723\"\n\n&gt; GEORADIUSBYMEMBER maps delhi 1300 km\n1) \"delhi\"\n2) \"mumbai\"\n\n&gt; GEORADIUSBYMEMBER maps delhi 1300 km withcoord withdist desc|asc\n1) 1) \"mumbai\"\n   2) \"1151.8732\"\n   3) 1) \"72.87742406129837036\"\n      2) \"19.07608965708350723\"\n2) 1) \"delhi\"\n   2) \"0.0000\"\n   3) 1) \"77.21672326326370239\"\n      2) \"28.64479890853065314\"\n</code></pre>"},{"location":"Databases/cassandra/","title":"Cassandra","text":""},{"location":"Databases/cassandra/#setup-docker","title":"Setup : Docker","text":"<pre><code>docker run --name Node_X -d scylladb/scylla:4.5.0 --overprovisioned 1 --smp 1\n\ndocker run --name Node_Y -d scylladb/scylla:4.5.0  \\ \n    --seeds=\"$(docker inspect --format='{{ .NetworkSettings.IPAddress }}' Node_X)\" \\\n    --overprovisioned 1 --smp 1\n\ndocker run --name Node_Z -d scylladb/scylla:4.5.0 \\ \n    --seeds=\"$(docker inspect --format='{{ .NetworkSettings.IPAddress }}' Node_X)\" \\ \n    --overprovisioned 1 --smp 1\n</code></pre> <p>Scylla runs nodes in a hash ring. All nodes are equal:</p> <ul> <li>there are no master</li> <li>slave</li> <li>replica sets.</li> </ul>"},{"location":"Databases/cassandra/#replication-factor","title":"Replication Factor","text":"<p>The Replication Factor (RF) is equivalent to the number of nodes where data (rows and partitions) are replicated. Data is replicated to multiple (RF=N) nodes. An RF of one means there is only one copy of a row in a cluster, and there is no way to recover the data if the node is compromised or goes down. RF=2 means that there are two copies of a row in a cluster. An RF of at least three is used in most systems</p>"},{"location":"Databases/cassandra/#consistency-level","title":"Consistency Level","text":"<p>The Consistency Level (CL) determines how many replicas in a cluster must acknowledge a read or write operation before it is considered successful.</p> <p>Some of the most common Consistency Levels used are:</p> <ul> <li><code>ANY</code> \u2013 A write must be written to at least one replica in the cluster. A read waits for a response from at least one replica. It provides the highest availability with the lowest consistency.</li> <li><code>QUORUM</code> \u2013 When a majority of the replicas respond, the request is honored. If RF=3, then 2 replicas respond. QUORUM can be calculated using the formula (n/2 +1) where n is the Replication Factor.</li> <li><code>ONE</code> \u2013 If one replica responds; the request is honored.</li> <li><code>LOCAL_ONE</code> \u2013 At least one replica in the local data center responds.</li> <li><code>LOCAL_QUORUM</code> \u2013 A quorum of replicas in the local datacenter responds.</li> <li><code>EACH_QUORUM</code> \u2013 (unsupported for reads) \u2013 A quorum of replicas in ALL datacenters must be written to.</li> <li><code>ALL</code> \u2013 A write must be written to all replicas in the cluster, a read waits for a response from all replicas. Provides the lowest availability with the highest consistency.</li> </ul>"},{"location":"Databases/cassandra/#sharding-in-scylla-db","title":"Sharding in Scylla DB","text":"<p>Each Scylla node consists of several independent shards, which contain their share of the node\u2019s total data. Scylla creates a one shard per core (technically, one shard per hyperthread, meaning some physical cores may have two or more virtual cores). Each shard operates on a shared-nothing architecture basis. This means each shard is assigned its RAM and its storage, manages its schedulers for the CPU and I/O, performs its compaction (more about compaction later on), and maintains its multi-queue network connection. Each shard runs as a single thread, and communicates asynchronously with its peers, without locking.</p>"},{"location":"Databases/cassandra/#other-important-concepts","title":"Other Important Concepts","text":""},{"location":"Databases/cassandra/#partition-key","title":"Partition Key","text":"<p>A Partition Key is one or more columns that are responsible for data distribution across the nodes. It determines in which nodes to store a given row. As we will see later on, typically, data is replicated, and copies are stored on multiple nodes. This means that even if one node goes down, the data will still be available. It ensures reliability and fault tolerance</p>"},{"location":"Databases/cassandra/#node","title":"Node","text":"<p>A Node is a unit of storage in Scylla. It is comprised of the Scylla database server software running on a computer server \u2014 a physical machine \u2014 and all its subsystems (CPUs, memory, storage, network interfaces and so on), or, in a visualized environment, a subset of a server\u2019s resources assigned to a container.</p>"},{"location":"Databases/cassandra/#cluster","title":"Cluster","text":"<p>A minimum Cluster typically consists of at least 3 nodes. Data is replicated across the cluster, depending on the Replication Factor</p>"},{"location":"Databases/cassandra/#table","title":"Table","text":"<p>A Table is how Scylla stores data and can be thought of as a set of rows and columns.</p>"},{"location":"Databases/cassandra/#keyspace","title":"Keyspace","text":"<p>A Keyspace is a collection of tables with attributes that define how data is replicated on nodes. It defines several options that apply to all the tables it contains, most prominently of which is the replication strategy used by the Keyspace. </p>"},{"location":"Databases/cassandra/#cql","title":"CQL","text":"<p>A query language for interacting with the Scylla (or Cassandra) database.</p>"},{"location":"Databases/cassandra/#partition-key_1","title":"Partition Key","text":"<p>One or more columns that are responsible for data distribution across the nodes. It determines in which nodes to store a given row</p> <p></p>"},{"location":"Databases/cassandra/#cql-shell","title":"CQL Shell","text":"<p>A command-line interface for interacting with Scylla through the Cassandra Query Language (CQL)</p>"},{"location":"Databases/cassandra/#replication","title":"Replication","text":"<p>The process of replicating data across Nodes in a Cluster.</p>"},{"location":"Databases/cassandra/#consistency-level_1","title":"Consistency Level","text":"<p>A configurable setting which dictates how many replicas in a Cluster must acknowledge read or write operations.</p>"},{"location":"Databases/cassandra/#tunable-consistency","title":"Tunable Consistency","text":"<p>The possibility for unique, per-query, Consistency Level settings. These are incremental and override fixed database settings intended to enforce data consistency. </p>"},{"location":"Databases/cassandra/#replication-factor_1","title":"Replication Factor","text":"<p>The total number of replica Nodes across a given Cluster. A Replication Factor of 1 means that the data will only exist on a single Node in the Cluster and this setup will not have any fault tolerance. </p>"},{"location":"Databases/cassandra/#cap-theorem","title":"CAP Theorem","text":"<p>The CAP Theorem is a concept that states that a distributed database system can only have 2 of the 3: Consistency, Availability, and Partition Tolerance.</p>"},{"location":"Databases/cassandra/#token-ranges","title":"Token Ranges","text":"<p>Each node in a ring is assigned a range. The hash function computes a token for a given partition key. The hash function determines the placement of the data in the cluster.</p> <p>Without using Vnodes or virtual nodes, each node could only support one token range. By using vnodes, each node can support multiple, non-contiguous token ranges. By doing this, we can think of each physical node as hosting many virtual nodes. By default, each node has 256 virtual nodes.</p>"},{"location":"Databases/cassandra/#gossips","title":"Gossips","text":"<p>For nodes to exchange information with each other. Gossip is decentralized, and there is no single point of failure. It\u2019s used for peer node discovery and metadata propagation. Gossip communication occurs periodically.</p> <p>Replication Strategy</p> <p>Simple Strategy \u2013 Places the first replica on the node selected by the partitioner. Remaining replicas are placed in the clockwise direction on the node ring. This replication strategy should not be used in production environments. </p> <p>Network Topology Strategy \u2013 Places replicas in a clockwise direction in the ring until it reaches the first node of a different rack. This is used for clusters deployed across multiple data centers. Using this strategy allows you to define the number of replicas for each DC</p>"},{"location":"Databases/cassandra/#cluster-level-readwrite-interaction","title":"Cluster Level Read/Write Interaction","text":"<p>So what happens when data is read or written at the cluster level? Note that what happens at the node level will be explained in another lesson.</p> <p>Since each node is equal in Scylla, any node can receive a read/write request. These are the main steps in the process:</p> <p></p> <ol> <li>A client connects to a Scylla node using CQL shell and performs a CQL request</li> <li>The node the client connected to is now designated as the Coordinator Node. The Coordinator Node, based on hashing the data, using the partition key and on the Replication Strategy, sends the request to the applicable nodes. Inter-node messages are sent through a messaging queue in an asynchronous way.</li> <li>The Consistency Level determines the number of nodes the coordinator needs to hear back from, in order for the request to be successful.</li> <li>The client is notified if the request was successful.</li> </ol>"},{"location":"Databases/cassandra/#core-principles-of-cassandra","title":"Core Principles of Cassandra","text":"<p>The database is designed around several core principles:</p> <ul> <li>High Scalability \u2013 the system must scale both horizontally (adding more nodes) as well as vertically (make optimal use of modern multi-core, multi-CPU node architectures, and high-capacity storage devices).</li> <li>High Availability \u2013 the system should have low latency and remain highly accessible for operations even if one or more nodes are in a failure state, or if there is a network failure.</li> <li>High performance \u2013 the system should run as close to the hardware as possible to deliver low and consistent latency as well as very high throughput.</li> <li>Low Maintenance \u2013 the system should include ease-of-use features, such as autonomous capabilities and automated facilities, for example, the ability to intelligently configure itself and tune its performance.</li> </ul>"},{"location":"Databases/cassandra/#replication-strategy","title":"Replication Strategy","text":"<ul> <li>SimpleStrategy </li> </ul> <ul> <li>NetworkTopologyStrategy</li> </ul>"},{"location":"Databases/cassandra/cassandra-administration/","title":"Cassandra Administration","text":""},{"location":"Databases/cassandra/cassandra-administration/#cassandra-ports","title":"Cassandra Ports","text":"<p>Nodetool has 2 types of commands </p> <ul> <li>Informative</li> <li>Management</li> </ul> <p>Cassandra Stress:</p> <pre><code>cassandra-stress write no-warmup n=100000\n</code></pre>"},{"location":"Databases/cassandra/cassandra-features/","title":"Cassandra Features","text":"<ul> <li>User defined Data Type : https://university.scylladb.com/courses/data-modeling/lessons/advanced-data-modeling/topic/user-defined-types-udt/</li> <li>Denormalization : https://university.scylladb.com/courses/data-modeling/lessons/advanced-data-modeling/topic/denormalization/</li> </ul>"},{"location":"Databases/cassandra/cassandra-features/#time-to-live","title":"Time to Live","text":"<p>more info : https://university.scylladb.com/courses/data-modeling/lessons/advanced-data-modeling/topic/expiring-data-with-ttl-time-to-live/</p> <p>Can be set when </p> <ul> <li>When defining table</li> <li>insert or update operation</li> </ul> <p>Example : Insert and Update operations</p> <pre><code>CREATE TABLE heartrate (\n    pet_chip_id  uuid,\n    name text,\n    heart_rate int,\n    PRIMARY KEY (pet_chip_id)\n);\n\nINSERT INTO heartrate(pet_chip_id, name, heart_rate) \nVALUES (123e4567-e89b-12d3-a456-426655440b23, 'Duke', 90);\n\n-- check for ttl of heart rate\nSELECT name, TTL(heart_rate)\nFROM heartrate WHERE  pet_chip_id = 123e4567-e89b-12d3-a456-426655440b23;\n\n-- set ttl for heart rate\nUPDATE heartrate USING TTL 600 SET heart_rate =\n110 WHERE pet_chip_id = 123e4567-e89b-12d3-a456-426655440b23;\n\n-- for the entire row\nINSERT INTO heartrate(pet_chip_id, name, heart_rate) \nVALUES (c63e71f0-936e-11ea-bb37-0242ac130002, 'Rocky', 87) USING TTL 30;\n</code></pre> <p>Example : For Table</p> <pre><code>CREATE TABLE heartrate_ttl (\n    pet_chip_id  uuid,\n    name text,\n    heart_rate int,\n    PRIMARY KEY (pet_chip_id))\nWITH default_time_to_live = 500;\n</code></pre>"},{"location":"Databases/cassandra/cassandra-features/#counter","title":"Counter","text":"<p>more info : https://university.scylladb.com/courses/data-modeling/lessons/advanced-data-modeling/topic/counters/</p> <p>It\u2019s a special data type (column) that only allows its value to be incremented, decremented, read or deleted. As a type, counters are a 64-bit signed integer. Updates to counters are atomic, making them perfect for counting and avoiding the issue of possible concurrent updates on the same value.</p> <p>Counters can only be defined in a dedicated table that includes:</p> <ul> <li>The primary key (can be compound)</li> <li>The counter column</li> </ul> <pre><code>CREATE TABLE pet_type_count (\n    pet_type text PRIMARY KEY, \n    pet_counter counter\n);\n</code></pre> <p>Loading data to a counter table is different than other tables, it\u2019s done with an UPDATE operation</p> <pre><code>UPDATE pet_type_count SET pet_counter = pet_counter + 6 \nWHERE pet_type = 'dog';\n</code></pre>"},{"location":"Databases/cassandra/cql/","title":"CQL","text":""},{"location":"Databases/cassandra/cql/#create-keyspace","title":"Create KeySpace","text":"<p>( same as db in SQL )</p> <pre><code>CREATE KEYSPACE mykeyspace WITH REPLICATION = { \n        'class' : 'NetworkTopologyStrategy', \n        'replication_factor' : 3\n};\n\nuse mykeyspace;\n\nDESCRIBE mykeyspace;\n\nCREATE TABLE users ( \n    user_id int, \n    fname text, \n    lname text, \n    PRIMARY KEY((user_id))\n); \n</code></pre>"},{"location":"Databases/cassandra/cql/#create-table-insert-data","title":"Create Table &amp; Insert Data","text":"<pre><code>CREATE TABLE users ( \n    user_id int, \n    fname text, \n    lname text, \n    PRIMARY KEY((user_id))\n); \n\ninsert into users(user_id, fname, lname) values (1, 'rick', 'sanchez'); \ninsert into users(user_id, fname, lname) values (4, 'rust', 'cohle'); \n\nselect * from users;\n\nCONSISTENCY QUORUM | ALL\n</code></pre>"},{"location":"Databases/cassandra/cql/#more-queries","title":"More Queries","text":"<pre><code>SELECT * from \n    heartrate_v1 \nWHERE \n    pet_chip_id = 123e4567-e89b-12d3-a456-426655440b23;\n\nDELETE FROM \n    heartrate_v1 \nWHERE \n    pet_chip_id = 123e4567-e89b-12d3-a456-426655440b23;\n</code></pre> <p>All inserts in Scylla DB (and Cassandra) are actually upserts (insert/update). There can be only one set of values for each unique primary key. If we insert again with the same primary key, the values will be updated.</p>"},{"location":"Databases/cassandra/cql/#data-modelling","title":"Data Modelling","text":"<p>https://docs.scylladb.com/stable/cql/types.html</p>"},{"location":"Databases/cassandra/cql/#map","title":"Map","text":"<pre><code>CREATE TABLE pets_v1 (\n    pet_chip_id text PRIMARY KEY,\n    pet_name text,\n    favorite_things map&lt;text, text&gt; // A map of text keys, and text values\n);\n\nINSERT INTO pets_v1 (pet_chip_id, pet_name, favorite_things)\n           VALUES ('123e4567-e89b-12d3-a456-426655440b23', \n           'Rocky', { 'food' : 'Turkey', 'toy' : 'Tennis Ball' });\n</code></pre>"},{"location":"Databases/cassandra/cql/#set","title":"Set","text":"<pre><code>CREATE TABLE pets_v2 (\n    pet_name text PRIMARY KEY,\n    address text,\n    vaccinations set&lt;text&gt; \n);\n</code></pre> <pre><code>INSERT INTO pets_v2 (pet_name, address, vaccinations)\n            VALUES ('Rocky', '11 Columbia ave, New York NY', \n            { 'Heartworm', 'Canine Hepatitis' });\n</code></pre>"},{"location":"Databases/cassandra/cql/#list","title":"List","text":"<pre><code>CREATE TABLE pets_v3 (\n    pet_name text PRIMARY KEY,\n    address text,\n    vaccinations list&lt;text&gt;\n);\n\nINSERT INTO pets_v3 (pet_name, address, vaccinations)\n            VALUES ('Rocky', '11 Columbia ave, New York NY',  \n            ['Heartworm', 'Canine Hepatitis', 'Heartworm']);\n</code></pre>"},{"location":"Databases/cassandra/data-modelling/","title":"Data Modelling","text":"<p>In Scylla, as opposed to relational databases, the data model is based around the queries and not just around the domain entities. When creating the data model, we take into account both the conceptual data model and the application workflow: which queries will be performed by which users and how often.</p> 1 Query-based: Application -&gt; Data -&gt; Model Entity-based: Data -&gt; Model -&gt; Application 4 Denormalization Support for foreign-keys, Joins 5 CAP Theorem, Eventual Consistency ACID Guarantee 6 Distributed Architecture Mostly single point of failure"},{"location":"Databases/cassandra/data-modelling/#things-to-keep-in-mind-when-design-tables","title":"Things to keep in mind when design Tables","text":"<ul> <li>Even data distribution: data should be evenly spread across the cluster so that every node holds roughly the same amount of data. Scylla determines which node should store the data based on hashing the partition key. Therefore, choosing a suitable partition key is crucial. More on this later on.</li> <li>To minimize the number of partitions accessed in a read query: To make reads faster, we\u2019d ideally have all the data required in a read query stored in a single Table. Although it\u2019s fine to duplicate data across tables, in terms of performance, it\u2019s better if the data needed for a read query is in one table.</li> </ul>"},{"location":"Databases/cassandra/data-modelling/#things-we-should-not-focus-on","title":"Things we should NOT focus on:","text":"<ul> <li>Avoiding data duplication: To get efficient reads, we sometimes have to duplicate data. </li> <li>Minimizing the number of writes: writes in Scylla DB aren\u2019t free, but they are very efficient and \u201ccheap.\u201d Scylla DB is optimized for high write throughput. Reads, while still very fast, are usually more expensive than writes and are harder to fine-tune. </li> <li>We\u2019d usually be ready to increase the number of writes to increase read efficiency. Keep in mind that the number of tables also affects consistency. </li> </ul>"},{"location":"Databases/cassandra/data-modelling/#important","title":"Important","text":"<ul> <li>ScyllaDB data modeling is query-based. That is, we think of the application workflow and the queries early on in the data model process</li> <li>A Keyspace is a top-level container that stores tables</li> <li>A Table is how ScyllaDB stores data and can be thought of as a set of rows and columns</li> <li>The Primary Key is composed of the Partition Key and Clustering Key</li> <li>One of our goals in data modeling is even data distribution. For that, we need to select a partition key correctly</li> <li>Selecting the Primary Key is very important and has a huge impact on query performance</li> </ul>"},{"location":"Databases/cassandra/data-modelling/#partition-and-clustering-keys","title":"Partition and Clustering Keys","text":"<p><code>composite primary key = partition key + clustering key</code></p> <p>The Partition Key is responsible for data distribution across the nodes. It determines which node will store a given row. It can be one or more columns. Without this, query wont execute</p> <p>The Clustering Key is responsible for sorting the rows within the partition. It can be zero or more column</p> <p>To query without the partition key, cluster would have to perform full scan which is super expensive.</p>"},{"location":"Databases/cassandra/data-modelling/#example","title":"Example:","text":"<pre><code>CREATE TABLE heartrate_v1 (\n   pet_chip_id uuid,\n   time timestamp,\n   heart_rate int,\n   PRIMARY KEY (pet_chip_id)\n);\n\nINSERT INTO heartrate_v1(pet_chip_id, time, heart_rate) \nVALUES (123e4567-e89b-12d3-a456-426655440b23, '2019-03-04 07:02:30', 130);\n</code></pre> <p>To run this query : (wont work)</p> <pre><code>SELECT * from heartrate_v1 \nWHERE pet_chip_id = 123e4567-e89b-12d3-a456-426655440b23 \n    AND time &gt;='2019-03-04 07:01:00' \n    AND time &lt;='2019-03-04 07:02:00';\n</code></pre> <p>we partition and clustering key</p> <pre><code>CREATE TABLE heartrate_v2 (\n   pet_chip_id uuid,\n   time timestamp,\n   heart_rate int,\n   PRIMARY KEY (pet_chip_id, time)\n);\n\nINSERT INTO heartrate_v2(pet_chip_id, time, heart_rate)\n   VALUES (123e4567-e89b-12d3-a456-426655440b23, '2019-03-04 07:01:05', 100); \nINSERT INTO heartrate_v2(pet_chip_id, time, heart_rate) \n   VALUES (123e4567-e89b-12d3-a456-426655440b23, '2019-03-04 07:01:10', 90); \nINSERT INTO heartrate_v2(pet_chip_id, time, heart_rate) \n   VALUES (123e4567-e89b-12d3-a456-426655440b23, '2019-03-04 07:01:50', 96); \nINSERT INTO heartrate_v2(pet_chip_id, time, heart_rate) \n   VALUES (123e4567-e89b-12d3-a456-426655440b23, '2019-04-04 07:01:50', 99); \n\nSELECT * from heartrate_v2 \nWHERE pet_chip_id = 123e4567-e89b-12d3-a456-426655440b23 \n   AND time &gt;='2019-03-04 07:01:00' \n   AND time &lt;='2019-03-04 07:02:00';\n</code></pre> <p>This also eliminates the problem of storing just one value for a primary key. Partition Key can be more than one column.</p> <p>If we have multiple partition keys</p> <pre><code>CREATE TABLE heartrate_v3 (\n   pet_chip_id uuid,\n   time timestamp,\n   heart_rate int,\n   pet_name text,\n   PRIMARY KEY ((pet_chip_id, time), pet_name)\n);\n\nINSERT INTO heartrate_v3(pet_chip_id, time, heart_rate, pet_name) \nVALUES (123e4567-e89b-12d3-a456-426655440b23, '2019-03-04 07:01:10', 90, 'Duke'); \n</code></pre> <p>this query wont work, cuz we need both the partition keys</p> <pre><code>SELECT * from heartrate_v3 \nWHERE pet_chip_id = 123e4567-e89b-12d3-a456-426655440b23;\n</code></pre> <p>----------------------------</p> <p>it is possible to define (do this):</p> <pre><code>CREATE TABLE heartrate_v4 (\n   pet_chip_id uuid,\n   time timestamp,\n   heart_rate int,\n   pet_name text,\n   PRIMARY KEY (pet_chip_id, pet_name, heart_rate)\n);\n</code></pre> <ul> <li>If there is more than one column in the Clustering Key (pet_name and heart_rate in the example above), the order of these columns defines the clustering order. For a given partition, all the rows are physically ordered inside ScyllaDB by the clustering order. This order determines what select query you can efficiently run on this partition.</li> <li>In this example, the ordering is first by pet_name and then by heart_rate.</li> <li>In addition to the Partition Key columns, a query may include the Clustering Key. If it does include the Clustering Key columns they must be used in the same order as they were defined.</li> </ul> <p>It fails, as <code>pet_name</code> comes before <code>heart_rate</code> sin the clustering key.</p> <pre><code>SELECT * from heartrate_v4 \nWHERE pet_chip_id = 123e4567-e89b-12d3-a456-426655440b23 \nAND heart_rate = 100;\n</code></pre> <p>By default, sorting is based on the natural (ASC) order of the clustering columns.</p> <p>In order to get query in DESC order, we would need to define the table in the same way</p> <pre><code>CREATE TABLE heartrate_v5 (\n   pet_chip_id uuid,\n   time timestamp,\n   heart_rate int,\n   PRIMARY KEY (pet_chip_id, time)\n) WITH CLUSTERING ORDER BY (time DESC);\n</code></pre>"},{"location":"Databases/cassandra/data-modelling/#creating-partitions","title":"Creating Partitions","text":"<p>We don't need partitions size to grow too big, so we can break them by using composite partition key. For example </p> <pre><code>CREATE TABLE heartrate_v2 (\n    pet_chip_id  uuid,\n    date text,\n    time timestamp,\n    heart_rate int,\n    PRIMARY KEY ((pet_chip_id,date), time));\n</code></pre> <p>In this case, we would be able to query by pet_chip, for a given day, without having large partitions. The partition size will be limited by the day. Every day, a new partition will be created for each pet.</p>"},{"location":"Databases/cassandra/data-modelling/advance-data-modelling/","title":"Advance Data Modelling","text":"<p>In ScyllaDB (and Apache Cassandra), data is divided into partitions, rows, and values, which can be identified by a partition key. Sometimes the application needs to find a value by the value of another column. Doing this efficiently without scanning all of the partitions requires indexing</p> <p>There are three indexing options available in ScyllaDB: </p> <ul> <li>Materialized Views</li> <li>Global Secondary Indexes</li> <li>Local Secondary Indexes</li> </ul> <p>Materialized Views (MV) are a global index. When a new MV is declared, a new table is created and is distributed to the different nodes using the standard table distribution mechanisms. The new MV table can have a different primary key from the base table, allowing for faster searches on a different set of columns.</p> <p>Global Secondary Indexes (also called \u201cSecondary Indexes\u201d) are another mechanism in ScyllaDB which allows efficient searches on non-partition keys by creating an index. Rather than creating an index on the entire partition key, this index is created on specific columns.</p> <p>Local Secondary Indexes are an enhancement to Global Secondary Indexes, which allow ScyllaDB to optimize workloads where the partition key of the base table and the index are the same key. Like their global counterparts, Cassandra's local indexes are based on Materialized Views.</p>"},{"location":"Databases/cassandra/data-modelling/advance-data-modelling/#materialized-views","title":"Materialized Views","text":"<ul> <li>Materialized Views (MV) are a global index. When a new MV is declared, a new table is created and distributed to the different nodes using the standard table distribution mechanisms. It\u2019s scalable, just like normal tables. It is populated by a query running against the base table. It\u2019s not possible to directly update a MV; it\u2019s updated when the base table is updated.</li> <li> <p>Each Materialized View is a set of rows that correspond to rows present in the underlying, or base, table specified in the materialized view\u2019s SELECT statement.\\</p> </li> <li> <p>Reads from a Materialized View are just as fast as regular reads from a table and just as scalable. But as expected, updates to a table with Materialized Views are slower than regular updates since these updates need to update both the original table and the Materialized View and ensure the consistency of both updates. However, doing those in the application without server help would have been even slower.</p> </li> </ul> <p>Some common use cases for MV are Indexing with denormalization, different sort orders, and filtering (pre-computed queries).</p> <p>Example : </p> <pre><code>CREATE TABLE buildings (\n    name text,\n    city text,\n    built_year smallint,\n    height_meters smallint,\n    PRIMARY KEY (name)\n);\n\n-- materialized view\nCREATE MATERIALIZED VIEW building_by_city AS\n SELECT * FROM buildings\n WHERE city IS NOT NULL\n PRIMARY KEY(city, name);\n\n -- delete queries wont work\n DELETE FROM building_by_city WHERE city='Taipei';\n</code></pre> <p>Materialized Views and Indexes : https://university.scylladb.com/courses/data-modeling/lessons/materialized-views-secondary-indexes-and-filtering/topic/materialized-views-and-secondary-indexes-hands-on-updated/</p>"},{"location":"Databases/cassandra/data-modelling/advance-data-modelling/#global-secondary-indexes","title":"Global Secondary Indexes","text":"<p>Secondary indexes are created for one main purpose: to allow querying by a column that is not a key. CQL tables have strict schemas that define which columns form a primary key, and fundamentally we should use these keys to extract data from the database. But, in practice, we may want to occasionally query by a different, regular column, or several of them. How to achieve that? One of the ways is to create an index on that column. </p> <p>These indexes are implemented on top of materialized views. It implies that there\u2019s a storage overhead for creating an index \u2013 it will use another table to store the data it needs. Indexes can be global or local, and let\u2019s find out what that means.</p> <p>local secondary indexes : https://university.scylladb.com/courses/data-modeling/lessons/materialized-views-secondary-indexes-and-filtering/topic/local-secondary-indexes-and-combining-both-types-of-indexes/</p>"},{"location":"Databases/mongodb/","title":"MongoDB","text":""},{"location":"Databases/mongodb/#mongodb-native","title":"MongoDB native","text":"<p>To connect to database</p> <p>use the connect settings in mongodb atlas</p> <p>Starting local database server</p> <p>After installing, you can start the mongod by</p> <pre><code>sudo systemctl start mongod\n</code></pre> <p>if you receive an error : Failed to start mongod.service: Unit mongod.service not found.</p> <p>Run the following command first:</p> <pre><code>sudo systemctl daemon-reload\n</code></pre> <p>Verify that MongoDB has started successfully.</p> <pre><code>sudo systemctl status mongod\n</code></pre> <p>You can optionally ensure that MongoDB will start following a system reboot by issuing the following command:</p> <pre><code>sudo systemctl enable mongod\n</code></pre> <p>Stop MongoDB.</p> <p>As needed, you can stop the <code>mongod</code> process by issuing the following command:</p> <pre><code>sudo systemctl stop mongod\n</code></pre> <p>Restart MongoDB.</p> <p>You can restart the <code>mongod</code> process by issuing the following command:</p> <pre><code>sudo systemctl restart mongod\n</code></pre>"},{"location":"Databases/mongodb/#mongodb-setup-docker","title":"MongoDB setup Docker","text":"<pre><code>sudo docker run --name mongo --network mongonet -d \\\n -p 27017:27017 \\\n -e MONGO_INITDB_ROOT_USERNAME=admin \\\n -e MONGO_INITDB_ROOT_PASSWORD=pass  \\\n mongo\n\nsudo docker exec -it some-mongo sh\n</code></pre> <p>Begin using MongoDB.</p> <p>Start a <code>mongo</code> shell on the same host machine as the <code>mongod</code>. You can run the <code>mongo</code> shell without any command-line options to connect to a <code>mongod</code> that is running on your localhost with default port 27017:</p> <pre><code>mongo \"mongodb+srv://&lt;username&gt;:&lt;password&gt;@&lt;url&gt;:&lt;port&gt;/&lt;db&gt;\"\n</code></pre> <p>To show collections</p> <pre><code>show dbs\nuse &lt;name of db&gt;\n\nshow collections\n</code></pre>"},{"location":"Databases/mongodb/#basic-commands","title":"Basic Commands","text":"<ul> <li>List all databases : <code>show dbs</code></li> <li>to switch to db : <code>use &lt;name_of_db&gt;</code></li> <li>to run a query : <code>db.&lt;name_of_collection&gt;.[function name]</code></li> <li>to iterate over many results : <code>it</code></li> <li>add : <code>.pretty()</code> to see json better</li> <li>to find any one document from collection, just use <code>.findOne()</code></li> <li>to create new collection : <code>db.createCollection(\"employees\")</code></li> </ul> <p>To shutdown db server</p> <pre><code>use admin\ndb.shutdownServer()\nexit\n</code></pre>"},{"location":"Databases/mongodb/#to-delete","title":"To delete","text":"<pre><code>drop collection\ndb.inspection.drop()\n</code></pre>"},{"location":"Databases/mongodb/mongo-administration/","title":"Mongo Administration","text":""},{"location":"Databases/mongodb/mongo-administration/#dbpath","title":"<code>dbpath</code>","text":"<p>The dbpath is the directory where all the data files for your database are stored. The dbpath also contains journaling logs to provide durability in case of a crash. As we saw before, the default dbpath is /data/db; however, you can specify any directory that exists on your machine. The directory must have read/write permissions since database and journaling files will be written to the directory.</p>"},{"location":"Databases/mongodb/mongo-administration/#port","title":"<code>port</code>","text":"<p>The port option allows us to specify the port on which mongod will listen for client connections. If we don't specify a port, it will default to 27017. Database clients should specify the same port to connect to mongod.</p>"},{"location":"Databases/mongodb/mongo-administration/#auth","title":"<code>auth</code>","text":"<p>auth enables authentication to control which users can access the database. When auth is specified, all database clients who want to connect to mongod first need to authenticate.</p> <p>Before any database users have been configured, a Mongo shell running on localhost will have access to the database. We can then configure users and their permission levels using the shell. Once one or more users have been configured, the shell will no longer have default access</p>"},{"location":"Databases/mongodb/mongo-administration/#bind_ip","title":"<code>bind_ip</code>","text":"<p>The bind_ip option allows us to specify which IP addresses mongod should bind to. When mongod binds to an IP address, clients from that address are able to connect to mongod</p> <p>Sample Config File</p> <pre><code>storage:\n  dbPath: \"/data/db\"\nsystemLog:\n  path: \"/data/log/mongod.log\"\n  destination: \"file\"\nreplication:\n  replSetName: M103\nnet:\n  bindIp : \"127.0.0.1,192.168.103.100\"\ntls:\n  mode: \"requireTLS\"\n  certificateKeyFile: \"/etc/tls/tls.pem\"\n  CAFile: \"/etc/tls/TLSCA.pem\"\nsecurity:\n  keyFile: \"/data/keyfile\"\n</code></pre>"},{"location":"Databases/mongodb/mongo-administration/#user-management-commands","title":"User management commands","text":"<pre><code>db.createUser()\ndb.dropUser()\n\n# Collection management commands:\n\ndb.&lt;collection&gt;.renameCollection()\ndb.&lt;collection&gt;.createIndex()\ndb.&lt;collection&gt;.drop()\n\n# Database management commands:\n\ndb.dropDatabase()\ndb.createCollection()\n\n# Database status command:\n\ndb.serverStatus()\n\n# Creating index with Database Command:\n\ndb.runCommand(\n  { \"createIndexes\": &lt;collection&gt; },\n  { \"indexes\": [\n    {\n      \"key\": { \"product\": 1 }\n    },\n    { \"name\": \"name_index\" }\n    ]\n  }\n)\n\n# Creating index with Shell Helper:\n\ndb.&lt;collection&gt;.createIndex(\n  { \"product\": 1 },\n  { \"name\": \"name_index\" }\n)\n\n# Introspect a Shell Helper:\n\ndb.&lt;collection&gt;.createIndex\n</code></pre>"},{"location":"Databases/mongodb/mongo-administration/#file-structure","title":"File Structure","text":"<pre><code># List --dbpath directory:\nls -l /data/db\n\n# List diagnostics data directory:\nls -l /data/db/diagnostic.data\n\n# List journal directory:\nls -l /data/db/journal\n\n# List socket file:\nls /tmp/mongodb-27017.sock\n</code></pre>"},{"location":"Databases/mongodb/mongo-administration/#create-new-user","title":"Create new user","text":"<pre><code>use admin\n\ndb.createUser({\n  user: \"root\",\n  pwd: \"root123\",\n  roles : [ \"root\" ]\n})\n</code></pre>"},{"location":"Databases/mongodb/mongo-administration/#create-security-officer","title":"Create security officer","text":"<pre><code>db.createUser(\n\n  { user: \"security_officer\",\n    pwd: \"h3ll0th3r3\",\n    roles: [ { db: \"admin\", role: \"userAdmin\" } ]\n  }\n)\n\n# Create database administrator:\n\ndb.createUser(\n\n  { user: \"dba\",\n    pwd: \"c1lynd3rs\",\n    roles: [ { db: \"admin\", role: \"dbAdmin\" } ]\n  }\n)\n\n# Grant role to user:\n\ndb.grantRolesToUser( \"dba\",  [ { db: \"playground\", role: \"dbOwner\"  } ] )\n\n# Show role privileges\n\ndb.runCommand( { rolesInfo: { role: \"dbOwner\", db: \"playground\" }, showP\n</code></pre>"},{"location":"Databases/mongodb/mongo-administration/#basic-replication-functions","title":"Basic Replication functions","text":"<pre><code>rs.add\nrs.initiate\nrs.remove\nrs.config\nrs.reconfig\nrs.isMaster\nrs.printReplicationInfo\n</code></pre> <p><code>rs.status</code> :</p> <ul> <li>reports health on replica set nodes</li> <li>uses data from heartbeats</li> </ul> <p><code>rs.isMaster</code> :</p> <ul> <li>descibe's a node's role in a replica set</li> </ul> <p><code>rs.printReplicationInfo</code> :</p> <ul> <li>only returns oplogs data relative to current node</li> <li>contains timestamps for first and oplog events</li> </ul> <p><code>oplog.rs</code> :</p> <ul> <li>central point of replication</li> <li>keeps track of all statements getting replicated</li> <li>its is a capped collection : size of collection is limited</li> <li>by defualt, it takes 5% of the available disk</li> <li>it appends statements ( used in replication ), till the file cap is reached</li> <li>once full, it starts to over write operations from top</li> <li>replication windows is proportional to the  system load</li> <li>size of oplog.rs will determine how much time a secondary node has to join in before it thorws itself in recovery mode</li> </ul> <pre><code># Display collections from the local database (this displays more collections from a replica set than from a standalone node):\n\nuse local\nshow collections\n\n# Query the oplog after connected to a replica set:\n\nuse local\ndb.oplog.rs.find()\n\n# Get information about the oplog (remember the oplog is a capped collection).\n# Store oplog stats as a variable called stats:\n\nvar stats = db.oplog.rs.stats()\n\n# Verify that this collection is capped (it will grow to a pre-configured size before it starts to overwrite the oldest entries with newer ones):\n\nstats.capped\n\n# Get current size of the oplog:\n\nstats.size\n\n# Get size limit of the oplog:\n\nstats.maxSize\n\n# Get current oplog data (including first and last event times, and configured oplog size):\n\nrs.printReplicationInfo()\n</code></pre>"},{"location":"Databases/mongodb/mongo-administration/#failover","title":"Failover :","text":"<ul> <li>Primary node is the first point of contact from client</li> <li>we first upgrade the secondary nodes</li> <li>then we step down the primary node to become secondary ( using rs.stepDown() ), once the election is complete, we can then safely upgrade the primary (which is now secondary) and connect it back to the replica set</li> </ul> <p>Elections :</p> <ul> <li>Happens when the primary node becomes unavailable or primary node wants to step down</li> <li>Next primary node will be elected keeping the following things in mind :</li> <li>Which ever node has the latest copy of data, it will run for election and automatically vote for itself</li> <li>If two node ( in a cluster of 3) has same recent data, the third node will cast a vote for any one of them to become primary. This becomes a problem in even node replica set</li> <li>Priority : likelihood that a node will become primary in case of election</li> <li>default priority is 1</li> <li>priority 1 or higher will give the node a higher chance of winning the election</li> <li>set priority of node to 0 if we dont want node to become primary</li> <li>node that cannot become primary are known as passive nodes</li> </ul>"},{"location":"Databases/mongodb/mongo-administration/#write-concerns","title":"write concerns","text":"<ul> <li>Commands under write concern</li> <li>insert</li> <li>update</li> <li>delete</li> <li>find or modify</li> <li>ACK mechanism added to write ops to provide stronger durability garuntee</li> <li>MAX : majority of nodes : roundup([num of nodes]/2)</li> <li>more durability requires more time to achieve</li> <li>write concerns level</li> <li>0 : dont wait for ack</li> <li>1 : wait for primary to ack</li> <li> <p>=2 : wait for primary and one or more secondary to ack</p> </li> <li>\"majority\" : wait for majority to ack</li> <li>write concern options</li> <li>wtimeout : time to wait before marking operation as failed</li> <li>j [true|false] : requires node to commit  the write operation to the journal before returning the ack</li> <li>setting write concern higher make ops slower</li> </ul>"},{"location":"Databases/mongodb/mongo-administration/#read-concern-preference","title":"read concern / preference","text":"<ul> <li>specifies durability during a read operation</li> <li>read concern level</li> <li>local : returns from the primary node</li> <li>available;</li> <li>majority;</li> <li>read preference allows you to redirect read operation to specific members of replica set</li> <li>read preference may return stale data;</li> <li>read preference modes</li> <li>primary : default</li> <li>primaryPreferred : can route read ops to secondary in case primary in not available</li> <li>secondary : routes read ops to sec. nodes only</li> <li>secondaryPreferred : if sec. not available, routes read ops to primary</li> <li>nearest : routes to least network latency from the host, ignores primary or secondary</li> </ul>"},{"location":"Databases/mongodb/mongo-administration/#sharding","title":"Sharding","text":"<ul> <li>There is upper limit to vertical scaling</li> <li>Sharding means adding more machines and dividing the dataset into multiple pieces</li> <li>For each shard, we add more replica to make sure we dont lose data</li> <li>a sharded cluster contains a config server, that store the metadata about each shard. This config server are responsible to distributing the queries to the shard containing the data.</li> <li>to make config servers highly available, they are deployed in a replica set configuration</li> <li>We use mongos to route the queries to each shard</li> </ul>"},{"location":"Databases/mongodb/mongo-administration/#when-to-shard-a-database","title":"When to shard a database","text":"<ul> <li>if not economically viable to scale up the throughput, speed and volume</li> <li>scaling horizontally will add more cost to backup, restore and initial sync time, when not feasible, shard the database</li> <li>Max a server should contain 2tb to 5tb data (factor in CPU and RAM usage)</li> <li>when geographically distributed database is required</li> <li>when single threaded operation needs to be parallelised</li> </ul>"},{"location":"Databases/mongodb/mongo-administration/#sharding-architecture","title":"Sharding Architecture","text":"<ul> <li>client do not connect to sharded cluster directly</li> <li>they connect to a process called mongos that routes queries to shards</li> </ul>"},{"location":"Databases/mongodb/mongo-administration/#how-mongos-figures-out-where-to-route-the-queries","title":"How mongos figures out where to route the queries","text":"<p>Let's say we have 3 shard containing the data about football players</p> <ul> <li>shard 1 : A-J</li> <li>shard 2 : K-Q</li> <li>shard 3 : R-Z</li> </ul> <p>When you send a query to find details about player name Messi, it will know which shard contains the data about that player ( as all shards store data about specific player only ). The config server will contain the metadata, for example : shard 1 contains names from A-J, shard 2 contains names from K-Q, shard 3 contains name from R-Z which helps mongos the route the queries</p> <p>There can be one mongos process routing queries to 3 shards or there can be multiple mongos process routing queries to shards and takign request also from multiple clients</p> <p>If size of one shard grows more than other, the data needs to be moved from the bigger shard to the smaller ones to keep consist storage capacity. This update will also be reflected in config server.</p> <p>As not all collections in a database needs to be sharded, there is one shard in the cluster that will act as primary shard keeping all the non sharded collections</p> <p>If we query the database, lets say where age is in 28-30, then mongos will not be able to route the query to specific shard, rather it will send it to all shards to find out the data, then SHARD_MERGE stage takes place. This stage can take place on mongos or a randomly chosen shard in the cluster.</p>"},{"location":"Databases/mongodb/mongo-administration/#the-config-database","title":"The Config database","text":"<ul> <li>maintained and used internally by mongodb, dont touch it i not necessary</li> </ul> <p>Switch to config DB:</p> <pre><code>use config\n\n# Query config.databases:\ndb.databases.find().pretty()\n\n# Query config.collections:\ndb.collections.find().pretty()\n\n# Query config.shards:\ndb.shards.find().pretty()\n\n# Query config.chunks:\ndb.chunks.find().pretty()\n\n# Query config.mongos:\ndb.mongos.find().pretty()\n</code></pre>"},{"location":"Databases/mongodb/mongo-administration/#shard-key","title":"Shard Key","text":"<ul> <li>It is the indexed field that mongodb uses to partition data in a sharded collection and distribute it across the shards in your cluster</li> <li>You need to create index first before you can select your shard key.</li> <li>MongoDB uses these shard keys to distribute data across sharded clusters. This groupings are also known as chunks</li> <li>shard key should be present in every document in the collection (if not already) or in every new document that is inserted</li> <li>shard keys are immutable, cannot change shard key post-sharding</li> <li>you cannot change the values of shard key fields post-sharding</li> <li>sharded collections are irreversible, you cannot unshard a collection, once sharded.</li> </ul>"},{"location":"Databases/mongodb/mongo-administration/#how-to-shard","title":"How to shard","text":"<ul> <li>use <code>sh.enableSharding(\"database\")</code> to enable sharding for the specific database.</li> <li>use <code>db.collections.createIndex()</code> to create the index for your shard key field</li> <li>use <code>sh.shardCollections(\"&lt;database&gt;.&lt;collections&gt;\",{shard_key})</code> to shard the collection</li> </ul>"},{"location":"Databases/mongodb/mongo-administration/#picking-a-good-shard-key","title":"Picking a Good shard key","text":"<ul> <li>Cardinality</li> <li>High Cardinality = many possible unique shard key values.</li> <li>Low Frequency  = low repetition of a given unique shard key value.</li> <li>Avoid shard keys that changes monotonically (keeping incrementing), choosing <code>_id</code> or <code>timestamp</code> or not a great options.</li> </ul>"},{"location":"Databases/mongodb/mongo-administration/#hashed-shard-keys","title":"Hashed Shard Keys","text":"<ul> <li>shard key where the underlying index is hashed</li> <li>mongodb uses a hashing function to calculate the hash shard key and then you out where the data is located</li> <li>the data is not changed in the docuement, instead the underlying index backing the shard key itself is hashed</li> <li>As monotonically changind value like <code>_id</code> or <code>timestamp</code> can be hashed, because the output from the hash function can prevent hotspotting</li> <li>this can make data highly distribute, so in case where you need</li> <li>you cannot support geographically isolated read operations using zoned sharding</li> <li>use <code>sh.enableSharding(\"database\")</code> to enable sharding for the specified</li> <li>use <code>db.collection.createIndex({\"field\":\"hashed\"})</code> to create the index for your shard key field</li> <li>use <code>sh.shardCollection(\"&lt;database&gt;.&lt;collection&gt;\",{ shard_key : \"hashed\" })</code> to shard the collection</li> </ul>"},{"location":"Databases/mongodb/mongo-administration/#lab-shard-a-collection","title":"Lab shard a collection","text":"<pre><code>mongoimport --drop /dataset/products.json --port 26000 -u \"m103-admin\" -p \"m103-pass\" --authenticationDatabase \"admin\" --db m103 --collection products\n\nmongo --port 26000 --username m103-admin --password m103-pass --authenticationDatabase admin\n\nuse m103\n\nsh.enableSharding(\"m103\")\ndb.products.createIndex({\"sku\":1})\nsh.shardCollection(\"m103.products\",{ \"sku\" : 1 })\n</code></pre>"},{"location":"Databases/mongodb/mongo-administration/#chunks","title":"Chunks","text":"<ul> <li>group of documents, who information is store in mongos determining which data belongs to which chunk and which shard contains it</li> <li>re balancing of chunks is preformed by primary of config server replica set</li> <li>Default Chunk Size : 64MB</li> </ul> <pre><code>use config\ndb.settings.save({_id: \"chunksize\", value: 2})\n</code></pre>"},{"location":"Databases/mongodb/mongo-administration/#targeted-queries-vs-scatter-gather","title":"Targeted Queries vs Scatter Gather","text":"<ul> <li>Each Shard contains chunks of sharded data, where each chunk represents a inclusive lower bound and upper bound.</li> <li>The config server replica set keeps maintains the primary record of where all the chunks are present.</li> <li>Mongos keeps a cached copy of the data chunks</li> <li>If the query contains the shard key, then mongos knows where to target the query. This is known as targeted query.</li> <li>targeted query are much faster and always used in query data request preformed by the customer.</li> <li>if the shard key isn't present in the query, them mongos will send the query to all shards and merge back the results from each shard. this is known as scatter gather.</li> <li>scatter gather query sometimes could be extremely slow, hence on admins performing analytics queries should be allowed to run them.</li> </ul>"},{"location":"Databases/mongodb/mongo-administration/#in-case-of-composite-key","title":"In case of composite key","text":"<p>example shard key : <code>{\"sku\":1,\"type\":1,\"name\":1}</code></p> <p>Valid Targeted Queries</p> <pre><code>db.products.find( { \"sku\": ... } )\ndb.products.find( { \"sku\": ... , \"type\": ... } )\ndb.products.find( { \"sku\": ... , \"type\": ... , \"name\": ... } )\n</code></pre> <p>Scatter Gather</p> <pre><code>db.products.find( { \"type\": ... } )\ndb.products.find( { \"name\": ... } )\n</code></pre>"},{"location":"Databases/mongodb/mongodb-aggregation/","title":"MongoDB Aggregation","text":""},{"location":"Databases/mongodb/mongodb-aggregation/#aggregate-framework","title":"Aggregate Framework","text":"<ul> <li>Queries are written inside <code>[]</code> operator, denoting the order in which hey execute</li> <li><code>$group</code> : An operator that takes in multiple streams of data and distributes it into multiple reservoirs</li> </ul> <pre><code># MQL Query\ndb.listingsAndReviews.find({ \"amenities\": \"Wifi\" },\n                           { \"price\": 1, \"address\": 1, \"_id\": 0 }).pretty()\n\n# MQL Query ith aggregation framework\ndb.listingsAndReviews.aggregate(\n    [\n      { \"$match\": { \"amenities\": \"Wifi\" } },\n      { \"$project\": { \"price\": 1,\n                      \"address\": 1,\n                      \"_id\": 0 }}\n    ]).pretty()\n</code></pre> <pre><code># Find one document in the collection \n# and only include the address field in the resulting cursor.\ndb.listingsAndReviews.findOne({ },{ \"address\": 1, \"_id\": 0 })\n\n# Project only the address field value for each document, \n# then group all documents into one document per address.country value.\ndb.listingsAndReviews.aggregate(\n    [   { \"$project\": { \"address\": 1, \"_id\": 0 }},\n        { \"$group\": { \"_id\": \"$address.country\" }}\n    ])\n\n# Project only the address field value for each document, \n# then group all documents into one document per address.country value, \n# and count one for each document in each group.\ndb.listingsAndReviews.aggregate(\n    [\n          { \"$project\": { \"address\": 1, \"_id\": 0 }},\n          { \"$group\": { \"_id\": \"$address.country\",\n                        \"count\": { \"$sum\": 1 } } }\n    ])\n</code></pre>"},{"location":"Databases/mongodb/mql/","title":"MQL","text":""},{"location":"Databases/mongodb/mql/#mongodb","title":"MongoDB","text":"<ul> <li>Data is stored in documents</li> <li>Documents are stored in Collections</li> <li>Document here refers to JSON</li> <li>Redundant copies of data are stored in replica set</li> <li>JSON is stored as BSON internally in MongoDB</li> </ul> <pre><code># FOR BSON, use dump (backup) and restore (restore backup)\n\nmongodump --uri \"mongodb+srv://&lt;your username&gt;:&lt;your password&gt;@&lt;your cluster&gt;.mongodb.net/&lt;database name&gt;\"\n\n# drop will delete the stuff already in and create the new object from restore\n\nmongorestore --uri \"mongodb+srv://&lt;your username&gt;:&lt;your password&gt;@&lt;your cluster&gt;.mongodb.net/&lt;database name&gt;\"  --drop dump\n\n# FOR JSON, use export (backup) and import (import backup)\n\n# collection to specify which collection\n# out to specify the file name to export to\n\nmongoexport --uri=\"mongodb+srv://&lt;your username&gt;:&lt;your password&gt;@&lt;your cluster&gt;.mongodb.net/&lt;database name&gt;\" --collection=sales --out=sales.json\n\nmongoimport --uri=\"mongodb+srv://&lt;your username&gt;:&lt;your password&gt;@&lt;your cluster&gt;.mongodb.net/&lt;database name&gt;\" --drop sales.json\n</code></pre> <pre><code># to look at all databases available\nshow dbs\n\n# to connect to a database\nuse sample_training\n\n# to look at collections inside a database\nshow collections\n</code></pre>"},{"location":"Databases/mongodb/mql/#queries","title":"Queries","text":""},{"location":"Databases/mongodb/mql/#find","title":"Find","text":"<pre><code>db.zips.find({\"state\": \"NY\"})\n\n# Use 'it' : iterates through a cursor\n# cursor : A pointer to a result set of query\n# pointer : A direct address of memory location\n\ndb.zips.find({\"state\": \"NY\"}).count()\n\ndb.zips.find({\"state\": \"NY\", \"city\": \"ALBANY\"})\n\ndb.zips.find({\"state\": \"NY\", \"city\": \"ALBANY\"}).pretty()\n\n# get random one\ndb.inspections.findOne()\n</code></pre> <ul> <li>Each Document has a unique object <code>_id</code> which is set by default if not specfied</li> </ul>"},{"location":"Databases/mongodb/mql/#insert","title":"Insert","text":"<pre><code>db.inspections.insert({\n      \"_id\" : ObjectId(\"56d61033a378eccde8a8354f\"),\n      \"id\" : \"10021-2015-ENFO\",\n      \"certificate_number\" : 9278806,\n      \"business_name\" : \"ATLIXCO DELI GROCERY INC.\",\n      \"date\" : \"Feb 20 2015\",\n      \"result\" : \"No Violation Issued\",\n      \"sector\" : \"Cigarette Retail Dealer - 127\",\n      \"address\" : {\n              \"city\" : \"RIDGEWOOD\",\n              \"zip\" : 11385,\n              \"street\" : \"MENAHAN ST\",\n              \"number\" : 1712\n         }\n  })\n\ndb.inspections.insert({\n      \"id\" : \"10021-2015-ENFO\",\n      \"certificate_number\" : 9278806,\n      \"business_name\" : \"ATLIXCO DELI GROCERY INC.\",\n      \"date\" : \"Feb 20 2015\",\n      \"result\" : \"No Violation Issued\",\n      \"sector\" : \"Cigarette Retail Dealer - 127\",\n      \"address\" : {\n              \"city\" : \"RIDGEWOOD\",\n              \"zip\" : 11385,\n              \"street\" : \"MENAHAN ST\",\n              \"number\" : 1712\n         }\n  })\n\ndb.inspections.find(\n      {\"id\" : \"10021-2015-ENFO\", \"certificate_number\" : 9278806}\n).pretty()\n</code></pre>"},{"location":"Databases/mongodb/mql/#insert-conflicts","title":"Insert conflicts","text":"<pre><code># Insert three test documents\ndb.inspections.insert([ { \"test\": 1 }, { \"test\": 2 }, { \"test\": 3 } ])\n\n# Insert three test documents but specify the _id values\n# Error in 2 docs\n# Insert operation halts when an error is in-countered\ndb.inspections.insert([{ \"_id\": 1, \"test\": 1 },{ \"_id\": 1, \"test\": 2 },\n                       { \"_id\": 3, \"test\": 3 }])\n\n# Find the documents with _id: 1\ndb.inspections.find({ \"_id\": 1 })\n\n# Insert multiple documents specifying the _id values, \n# and using the \"ordered\": false option\n# Ordered False will allow to insert all docs where id doesnt match, \n# and give errors for those which failed\ndb.inspections.insert([{ \"_id\": 1, \"test\": 1 },{ \"_id\": 1, \"test\": 2 },\n                       { \"_id\": 3, \"test\": 3 }],{ \"ordered\": false })\n\n# Insert multiple documents with _id: 1 with the default \"ordered\": true setting\ndb.inspection.insert([{ \"_id\": 1, \"test\": 1 },{ \"_id\": 3, \"test\": 3 }])\n</code></pre>"},{"location":"Databases/mongodb/mql/#updates","title":"Updates","text":"<p>https://docs.mongodb.com/manual/reference/operator/update/#id1</p> <pre><code># Find all documents in the zips collection \n# where the zip field is equal to \"12434\".\n\ndb.zips.find({ \"zip\": \"12534\" }).pretty()\n\n# Find all documents in the zips collection \n# where the city field is equal to \"HUDSON\".\n\ndb.zips.find({ \"city\": \"HUDSON\" }).pretty()\n\n# Update all documents in the zips collection \n# where the city field is equal to \"HUDSON\" \n# by adding 10 to the current value of the \"pop\" field.\n# Increment Operation\n\ndb.zips.updateMany({ \"city\": \"HUDSON\" }, { \"$inc\": { \"pop\": 10 } })\n\n# Update a single document in the zips \n# collection where the zip field is \n# equal to \"12534\" by setting the value \n# of the \"pop\" field to 17630.\n# Update / Set operation\n\ndb.zips.updateOne({ \"zip\": \"12534\" }, { \"$set\": { \"pop\": 17630 } })\n\n# Update a single document in the zips \n# collection where the zip field is equal \n# to \"12534\" by setting the value of \n# the \"popupation\" field to 17630.\n# Update / Set operation\n\ndb.zips.updateOne({ \"zip\": \"12534\" }, { \"$set\": { \"population\": 17630 } })\n\n# Find all documents in the grades \n# collection where the student_id \n# field is 151 , and the class_id field is 339.\n\ndb.grades.find({ \"student_id\": 151, \"class_id\": 339 }).pretty()\n\n# Update one document in the grades \n# collection where the student_id is \n# `250` *, and the class_id field is 339, \n# by adding a document element to the \"scores\" array.\n\ndb.grades.updateOne({ \"student_id\": 250, \"class_id\": 339 },\n                    { \"$push\": { \"scores\": { \"type\": \"extra credit\",\n                                             \"score\": 100 }\n                                }\n                     })\n\ndb.grades.find({ \"student_id\": 250, \"class_id\": 339 })\n</code></pre>"},{"location":"Databases/mongodb/mql/#upsert","title":"Upsert","text":"<pre><code>db.iot.updateOne({ \"sensor\": r.sensor, \"date\": r.date,\n                   \"valcount\": { \"$lt\": 48 } },\n                         { \"$push\": { \"readings\": { \"v\": r.value, \"t\": r.time } },\n                        \"$inc\": { \"valcount\": 1, \"total\": r.value } },\n                 { \"upsert\": true })\n</code></pre>"},{"location":"Databases/mongodb/mql/#delete","title":"Delete","text":"<pre><code># Look at all the docs that have test field equal to 1.\ndb.inspections.find({ \"test\": 1 }).pretty()\n\n# Delete all the documents that have test field equal to 1.\ndb.inspections.deleteMany({ \"test\": 1 })\n\n# Delete one document that has test field equal to 3.\ndb.inspections.deleteOne({ \"test\": 3 })\n\n# Inspect what is left of the inspection collection.\ndb.inspection.find().pretty()\n# View what collections are present in the sample_training collection.\nshow collections\n\n# Drop the inspection collection.\ndb.inspection.drop()\n</code></pre>"},{"location":"Databases/mongodb/mql/#operators","title":"Operators","text":"<pre><code># Find all documents where the tripduration \n# was less than or equal to 70 seconds and \n# the usertype was not Subscriber:\n# LESS THAN EQUAL\n# NOT EQUAL\n\ndb.trips.find({ \"tripduration\": { \"$lte\" : 70 },\n                \"usertype\": { \"$ne\": \"Subscriber\" } }).pretty()\n\n# Find all documents where the tripduration\n# was less than or equal to 70 seconds and \n# the usertype was Customer using a redundant equality operator:\n# LESS THAN EQUAL\n# EQUAL\n\ndb.trips.find({ \"tripduration\": { \"$lte\" : 70 },\n                \"usertype\": { \"$eq\": \"Customer\" }}).pretty()\n\n# Find all documents where the tripduration\n# was less than or equal to 70 seconds and \n# the usertype was Customer using the implicit equality operator:\n# LESS THAN EQUAL\n\ndb.trips.find({ \"tripduration\": { \"$lte\" : 70 },\n                \"usertype\": \"Customer\" }).pretty()\n\n# Find all documents where airplanes CR2 or A81 \n# left or landed in the KZN airport:\n\n# AND, OR operator\n\ndb.routes.find({ \"$and\": [ { \"$or\" :[ { \"dst_airport\": \"KZN\" },\n                                    { \"src_airport\": \"KZN\" }\n                                  ] },\n                          { \"$or\" :[ { \"airplane\": \"CR2\" },\n                                     { \"airplane\": \"A81\" } ] }\n                         ]}).pretty()\n</code></pre> <ul> <li>AND operator is present in your qureies when not specified</li> </ul>"},{"location":"Databases/mongodb/mql/#expr","title":"EXPR","text":"<pre><code># Find all documents where the trip started\n# and ended at the same station:\n# here $ denotes the value of the field specified\n\ndb.trips.find(\n    { \"$expr\": { \"$eq\": [ \"$end station id\", \"$start station id\"] }\n}).count()\n\n# replacing id with name\n\ndb.trips.find(\n    { \"$expr\": { \"$eq\": [ \"$end station name\", \"$start station name\"]}\n}).count()\n\n\n# Find all documents where the trip lasted \n# longer than 1200 seconds, and started \n# and ended at the same station:\n\ndb.trips.find({ \"$expr\": { \"$and\": [ { \"$gt\": [ \"$tripduration\", 1200 ]},\n                         { \"$eq\": [ \"$end station id\", \"$start station id\" ]}\n                       ]}}).count()\n</code></pre>"},{"location":"Databases/mongodb/mql/#array","title":"Array","text":"<pre><code># using ALL operator\n\ndb.listingsAndReviews.find(\n    { \"amenities\": \n        {\n          \"$size\": 20,\n          \"$all\": [ \"Internet\", \"Wifi\",  \"Kitchen\",\n                   \"Heating\", \"Family/kid friendly\",\n                   \"Washer\", \"Dryer\", \"Essentials\",\n                   \"Shampoo\", \"Hangers\",\n                   \"Hair dryer\", \"Iron\",\n                   \"Laptop friendly workspace\" ]\n        }\n    }).pretty()\n</code></pre>"},{"location":"Databases/mongodb/mql/#array-operators-and-projection","title":"Array operators and Projection","text":"<pre><code># Find all documents with exactly 20 amenities which include \n# all the amenities listed in the query array, \n# and display their price and address:\n\ndb.listingsAndReviews.find({ \"amenities\":\n        { \"$size\": 20, \"$all\": [ \"Internet\", \"Wifi\",  \"Kitchen\", \"Heating\",\n                                 \"Family/kid friendly\", \"Washer\", \"Dryer\",\n                                 \"Essentials\", \"Shampoo\", \"Hangers\",\n                                 \"Hair dryer\", \"Iron\",\n                                 \"Laptop friendly workspace\" ] } },\n                            {\"price\": 1, \"address\": 1}).pretty()\n\n# Find all documents that have Wifi as one of the amenities \n# only include price and address in the resulting cursor:\n\ndb.listingsAndReviews.find({ \"amenities\": \"Wifi\" },\n                           { \"price\": 1, \"address\": 1, \"_id\": 0 }).pretty()\n\n# Find all documents that have Wifi as one of the amenities \n# only include price and address in the resulting cursor, \n# also exclude ``\"maximum_nights\"``. **This will be an error:*\n\ndb.listingsAndReviews.find({ \"amenities\": \"Wifi\" },\n                           { \"price\": 1, \"address\": 1,\n                             \"_id\": 0, \"maximum_nights\":0 }).pretty()\n\n# nested projection\n\ndb.listingsAndReviews.find({ \"amenities\": \"Wifi\" }, { \"price\": 1, \"address\": { \"country\" : 1 }, \"_id\": 0 }).pretty()\n\n\n# Switch to this database:\nuse sample_training\n\n# Get one document from the collection:\ndb.grades.findOne()\n\n# Elematch Example\n\n# Find all documents where the student in class 431 received \n# a grade higher than 85 for any type of assignment:\n\ndb.grades.find({ \"class_id\": 431 },\n               { \"scores\": { \"$elemMatch\": { \"score\": { \"$gt\": 85 } } }\n             }).pretty()\n\n# Find all documents where the student had an extra credit score:\n\ndb.grades.find({ \"scores\": { \"$elemMatch\": { \"type\": \"extra credit\" } }\n               }).pretty()\n</code></pre> <ul> <li><code>Elematch</code> : matches documents that contains an array field with at least one element that matches the specified query creteria</li> <li><code>Elematch</code> : Projects only the array elements with at least one element that matches the specified criteria</li> </ul>"},{"location":"Databases/mongodb/mql/#array-operators-and-sub-documents","title":"Array Operators and Sub-Documents","text":"<pre><code>use sample_training\n\ndb.trips.findOne({ \"start station location.type\": \"Point\" })\n\ndb.companies.find({ \"relationships.0.person.last_name\": \"Zuckerberg\" },\n                  { \"name\": 1 }).pretty()\n\ndb.companies.find({ \"relationships.0.person.first_name\": \"Mark\",\n                    \"relationships.0.title\": { \"$regex\": \"CEO\" } },\n                  { \"name\": 1 }).count()\n\n\ndb.companies.find({ \"relationships.0.person.first_name\": \"Mark\",\n                    \"relationships.0.title\": {\"$regex\": \"CEO\" } },\n                  { \"name\": 1 }).pretty()\n\ndb.companies.find({ \"relationships\":\n                      { \"$elemMatch\": { \"is_past\": true,\n                                        \"person.first_name\": \"Mark\" } } },\n                  { \"name\": 1 }).pretty()\n\ndb.companies.find({ \"relationships\":\n                      { \"$elemMatch\": { \"is_past\": true,\n                                        \"person.first_name\": \"Mark\" } } },\n                  { \"name\": 1 }).count()\n</code></pre>"},{"location":"Databases/mongodb/mql/#sort-and-limit","title":"Sort and Limit","text":"<ul> <li>Use sort before limit always</li> </ul> <pre><code>db.zips.find().sort({ \"pop\": 1 }).limit(1)\n\ndb.zips.find({ \"pop\": 0 }).count()\n\ndb.zips.find().sort({ \"pop\": -1 }).limit(1)\n\ndb.zips.find().sort({ \"pop\": -1 }).limit(10)\n\ndb.zips.find().sort({ \"pop\": 1, \"city\": -1 })\n</code></pre>"},{"location":"Databases/sql/5-Order-of-sql-execution/","title":"Order of SQL Execution","text":""},{"location":"Databases/sql/5-Order-of-sql-execution/#1-from-and-joins","title":"1. <code>FROM</code> and <code>JOIN</code>s","text":"<p>The <code>FROM</code> clause, and subsequent <code>JOIN</code>s are first executed to determine the total working set of data that is being queried. This includes sub-queries in this clause, and can cause temporary tables to be created under the hood containing all the columns and rows of the tables being joined.</p>"},{"location":"Databases/sql/5-Order-of-sql-execution/#2-where","title":"2. <code>WHERE</code>","text":"<p>Once we have the total working set of data, the first-pass <code>WHERE</code> constraints are applied to the individual rows, and rows that do not satisfy the constraint are discarded. Each of the constraints can only access columns directly from the tables requested in the <code>FROM</code> clause. Aliases in the <code>SELECT</code> part of the query are not accessible in most databases since they may include expressions dependent on parts of the query that have not yet executed.</p>"},{"location":"Databases/sql/5-Order-of-sql-execution/#3-group-by","title":"3. <code>GROUP BY</code>","text":"<p>The remaining rows after the <code>WHERE</code> constraints are applied are then grouped based on common values in the column specified in the <code>GROUP BY</code> clause. As a result of the grouping, there will only be as many rows as there are unique values in that column. Implicitly, this means that you should only need to use this when you have aggregate functions in your query.</p>"},{"location":"Databases/sql/5-Order-of-sql-execution/#4-having","title":"4. <code>HAVING</code>","text":"<p>If the query has a <code>GROUP BY</code> clause, then the constraints in the <code>HAVING</code> clause are then applied to the grouped rows, discard the grouped rows that don't satisfy the constraint. Like the <code>WHERE</code> clause, aliases are also not accessible from this step in most databases.</p>"},{"location":"Databases/sql/5-Order-of-sql-execution/#5-select","title":"5. <code>SELECT</code>","text":"<p>Any expressions in the <code>SELECT</code> part of the query are finally computed.</p>"},{"location":"Databases/sql/5-Order-of-sql-execution/#6-distinct","title":"6. <code>DISTINCT</code>","text":"<p>Of the remaining rows, rows with duplicate values in the column marked as <code>DISTINCT</code> will be discarded.</p>"},{"location":"Databases/sql/5-Order-of-sql-execution/#7-order-by","title":"7. <code>ORDER BY</code>","text":"<p>If an order is specified by the <code>ORDER BY</code> clause, the rows are then sorted by the specified data in either ascending or descending order. Since all the expressions in the <code>SELECT</code> part of the query have been computed, you can reference aliases in this clause.</p>"},{"location":"Databases/sql/5-Order-of-sql-execution/#8-limit-offset","title":"8. <code>LIMIT</code> / <code>OFFSET</code>","text":"<p>Finally, the rows that fall outside the range specified by the <code>LIMIT</code> and <code>OFFSET</code> are discarded, leaving the final set of rows to be returned from the query.</p>"},{"location":"Databases/sql/1%20Getting%20Started/","title":"Getting Started","text":""},{"location":"Databases/sql/1%20Getting%20Started/#starting-database-with-docker","title":"Starting Database with Docker","text":"<pre><code>docker run --name &lt;docker_name&gt; \n -e POSTGRES_PASSWORD=&lt;password&gt;\n -d -p 5432:5432 postgres:13.3\n\ndocker exec -it &lt;docker_name&gt; bash\n\npsql -U postgres\n</code></pre>"},{"location":"Databases/sql/1%20Getting%20Started/#installing-postgresql-on-windows","title":"Installing PostgreSQL on windows","text":"<ul> <li>https://www.postgresql.org/download/windows/</li> </ul>"},{"location":"Databases/sql/1%20Getting%20Started/#installing-postgresql-on-macos","title":"Installing PostgreSQL on MacOS","text":"<p>https://www.postgresql.org/download/macosx/</p>"},{"location":"Databases/sql/1%20Getting%20Started/#setup-and-basics-using-apt","title":"Setup and Basics : using apt","text":"<p>Installation</p> <pre><code>sudo apt-get install postgresql\n</code></pre> <p>Usage commands</p> <pre><code>service postgresql\n</code></pre> <p>Switch to default user</p> <pre><code>sudo su postgres\n</code></pre>"},{"location":"Databases/sql/1%20Getting%20Started/#getting-started_1","title":"Getting Started","text":""},{"location":"Databases/sql/1%20Getting%20Started/#connect-to-a-database","title":"Connect to a database","text":"<pre><code>Connection options:\n  -h, --host=HOSTNAME      \n# database server host or socket directory \n# (default: \"local socket\")\n  -p, --port=PORT          \n# database server port \n# (default: \"5432\")\n  -U, --username=USERNAME  \n# database user name (default: \"root\")\n  -w, --no-password        \n# never prompt for password\n  -W, --password           \n  # force password prompt (should happen automatically)\n</code></pre> <ol> <li>Here port : 5432 is default and can be get from <code>psql --help</code></li> <li><code>postgres</code> is the super user. Create another user and connect using that.</li> <li>to connect to database with user :</li> </ol> <pre><code>\\c db_name user_name\n</code></pre>"},{"location":"Databases/sql/1%20Getting%20Started/#commands","title":"Commands","text":"<pre><code>-- to list all tables\n\\dt\n\n-- to list all databases\n\\l\n\n-- to list all table spaces\n\\db\n\n-- to list all schemas\n\\dn\n\n-- to list all indices\n\\di\n\n-- to list all sequences\n\\ds\n\n-- to list all roles\n\\dg\n\n-- to list data types\n\\dT\n\n-- to list all domain datatypes\n\\dD\n\n-- to list all views\n\\dv\n\n-- to list previous commands\n\\g\n\n-- to list Command History\n\\s\n\n-- to run command from file\n\\i filename \n-- it should be inside the server\n\n-- to list help\n\\h\n\\h create table\n\n-- to display null\n\\pset null (null)\n\n-- to make terminal better\n\\pset linestyle unicode\n\\pset border 2\n\n-- to watch a command in time\n\\watch 2\n\n-- to turn on timing\n\\timing\n</code></pre>"},{"location":"Databases/sql/1%20Getting%20Started/#load-data","title":"Load Data","text":"<p>Download the sample data file from here</p> <ul> <li>https://drive.google.com/file/d/1vsFVuybjNDacNaV5LmaSI__a6dCpEdmT/view?usp=sharing</li> </ul> <p></p>"},{"location":"Databases/sql/1%20Getting%20Started/administration-task/","title":"PgAdmin Tool","text":""},{"location":"Databases/sql/1%20Getting%20Started/administration-task/#creating-new-user","title":"Creating New User","text":""},{"location":"Databases/sql/1%20Getting%20Started/administration-task/#options","title":"Options","text":"<ul> <li>General</li> <li>Name : username</li> <li>Comment : description</li> <li>Definition</li> <li>Password : user password</li> <li>Account Expires</li> <li>Connection Limit : number of connections that user can have to DB, default -1</li> </ul> <p>Refer to documentation for more details</p> <ul> <li>https://www.pgadmin.org/docs/pgadmin4/development/role_dialog.html</li> </ul>"},{"location":"Databases/sql/1%20Getting%20Started/administration-task/#query-editor","title":"Query Editor","text":""},{"location":"Databases/sql/1%20Getting%20Started/administration-task/#load-data-using-pgadmin4","title":"Load Data using pgAdmin4","text":""},{"location":"Databases/sql/10%20Summarization/","title":"Summarization","text":"<ul> <li>When we are aggregating data, we generally remove the detail data that lies below the summarize table.</li> <li>The whole point of aggregation is to replace detail data with summaries. This is where subtotals comes in!</li> <li>A grouping set is a set of columns by which you group.</li> <li>The PostgreSQL <code>ROLLUP</code> is a subclass of the GROUP BY clause that offers shorthand for defining 'multiple grouping sets'</li> </ul>"},{"location":"Databases/sql/10%20Summarization/subqueries/","title":"SubQueries","text":"<ul> <li>Allows you to construct  a complex query</li> <li>A sub-query is nested inside another query</li> <li>can be nested inside <code>SELECT</code>, <code>INSERT</code>, <code>UPDATE</code>, <code>DELETE</code></li> </ul>"},{"location":"Databases/sql/10%20Summarization/subqueries/#subqueries-with-select-clause","title":"SubQueries with SELECT clause","text":"<pre><code>SELECT movie_name,\n       movie_length\nFROM movies mv\nWHERE movie_length &gt;= (\n    SELECT avg(movie_length)\n    FROM movies\n)\n\nSELECT movie_name,\n       movie_length\nFROM movies mv\nWHERE movie_length &gt;= (\n    SELECT avg(movie_length)\n    FROM movies\n    WHERE movie_lang = 'English'\n);\n</code></pre>"},{"location":"Databases/sql/10%20Summarization/subqueries/#subqueries-with-where-clause","title":"SubQueries With WHERE Clause","text":"<pre><code>SELECT first_name, last_name, date_of_birth\nFROM actors\nWHERE date_of_birth &gt; (\n    SELECT date_of_birth\n    FROM actors\n    WHERE first_name = 'Douglas' -- 1922-06-10\n);\n\n-- using IN operator\nSELECT movie_name,\n       movie_length\nFROM movies\nWHERE movie_id in (\n    SELECT movie_id\n    FROM movies_revenues\n    WHERE revenues_domestic &gt; 200\n);\n\nSELECT movie_id, movie_name\nFROM movies\nWHERE movie_id IN (\n    SELECT movie_id\n    FROM movies_revenues\n    WHERE revenues_domestic &gt; movies_revenues.revenues_international\n);\n</code></pre>"},{"location":"Databases/sql/10%20Summarization/subqueries/#subqueries-with-joins","title":"SubQueries with JOINS","text":"<pre><code>-- with joins\nSELECT d.director_id,\n       d.first_name || ' ' || d.last_name  as \"Director Name\",\n       SUM(r.revenues_international + r.revenues_domestic) as \"total_revenues\"\nFROM directors d\n         INNER JOIN movies mv ON mv.director_id = d.director_id\n         INNER JOIN movies_revenues r on r.movie_id = mv.movie_id\nWHERE (r.revenues_domestic + r.revenues_international) &gt;\n      (\n          SELECT avg(revenues_domestic + revenues_international) as \"avg_total_revenue\"\n          FROM movies_revenues\n      )\nGROUP BY d.director_id\nORDER BY total_revenues;\n\n\nSELECT d.director_id,\n       SUM(COALESCE(r.revenues_domestic, 0) + COALESCE(r.revenues_international, 0)) AS \"totaL_reveneues\"\nFROM directors d\n         INNER JOIN movies mv ON mv.director_id = d.director_id\n         INNER JOIN movies_revenues r ON r.movie_id = mv.movie_id\nWHERE COALESCE(r.revenues_domestic, 0) + COALESCE(r.revenues_international, 0) &gt;\n      (\n          SELECT AVG(COALESCE(r.revenues_domestic, 0) + COALESCE(r.revenues_international, 0)) as \"avg_total_reveneues\"\n          FROM movies_revenues r\n                   INNER JOIN movies mv ON mv.movie_id = r.movie_id\n          WHERE mv.movie_lang = 'English'\n      )\nGROUP BY d.director_id\nORDER BY 2 DESC, 1 ASC;\n</code></pre>"},{"location":"Databases/sql/10%20Summarization/subqueries/#subqueries-with-alias","title":"SubQueries with Alias","text":"<pre><code>-- as alias\nSELECT *\nFROM (\n         SELECT *\n         FROM movies\n     ) t1;\n\n-- query without FROM\nSELECT (\n           SELECT avg(revenues_domestic) as \"Average Revenue\"\n           FROM movies_revenues\n       ),\n       (\n           SELECT min(revenues_domestic) as \"MIN Revenue\"\n           FROM movies_revenues\n       ),\n       (\n           SELECT max(revenues_domestic) as \"MAX Revenue\"\n           FROM movies_revenues\n       )\n</code></pre>"},{"location":"Databases/sql/10%20Summarization/window/","title":"Window","text":""},{"location":"Databases/sql/10%20Summarization/window/#rollup","title":"Rollup","text":"<p>The ROLLUP option allows you to include extra rows that represent the subtotals, which are commonly referred to as super-aggregate rows, along with the grand total row. </p> <p>The ROLLUP assumes a hierarchy among the input columns.  For example, if the input column is (c1,c2), the hierarchy <code>c1 &gt; c2</code>. </p> <pre><code>-- SYNTAX\nSELECT \n    c1, c2, aggregate_function(c3)\nFROM\n    table\nGROUP BY ROLLUP (c1, c2);\n\n-- EXAMPLE\n\nSELECT region, round(avg(imports), 2)\nFROM trades\nGROUP BY rollup (region);\n\n-- OUTPUT\n\n    region      |      round      \n-----------------+-----------------\n                |  73325290066.17\nNORTH AMERICA   |  70993412846.58\nSOUTH AMERICA   | 152968951703.49\nASIA            | 115476296703.29\nCENTRAL AMERICA |  38853979545.30\n\n\nSELECT region, country, round(avg(imports), 2)\nFROM trades\nWHERE country in ('USA', 'Argentina', 'Singapore', 'Brazil')\nGROUP BY rollup (region, country)\norder by 1;\n\n    region     |  country  |      round       \n---------------+-----------+------------------\n ASIA          | Singapore |  209191973057.39\n ASIA          |           |  209191973057.39\n SOUTH AMERICA | Argentina |   40764435340.44\n SOUTH AMERICA | Brazil    |  103647680768.84\n SOUTH AMERICA | USA       | 1590283700017.03\n SOUTH AMERICA |           |  579677530557.70\n               |           |  482346579011.01\n\n\nSELECT region, country, round(avg(imports / 1000000))\nFROM trades\nWHERE country in ('USA', 'France', 'Germany')\nGROUP BY cube (region, country);\n\n    region     | country |  round  \n---------------+---------+---------\n               |         |  974454\n EUROPE        | France  |  481421\n SOUTH AMERICA | USA     | 1590284\n EUROPE        | Germany |  800653\n SOUTH AMERICA |         | 1590284\n EUROPE        |         |  649743\n               | USA     | 1590284\n               | Germany |  800653\n               | France  |  481421\n\n\nSELECT region, country, round(avg(imports) / 1000000, 2)\nFROM trades\nWHERE country in ('USA', 'FRANCE', 'Germany')\nGROUP BY\n    grouping sets ( (), country, region );\n\n    region     | country |   round    \n---------------+---------+------------\n               |         | 1195468.31\n               | USA     | 1590283.70\n               | Germany |  800652.92\n SOUTH AMERICA |         | 1590283.70\n EUROPE        |         |  800652.92\n\n\n\nSELECT region, country, round(avg(imports) / 1000000, 2)\nFROM trades\nWHERE country in ('USA', 'FRANCE', 'Germany')\nGROUP BY\n    grouping sets ( (), country, region );\n\n    region     | country |   round    \n---------------+---------+------------\n               |         | 1195468.31\n               | USA     | 1590283.70\n               | Germany |  800652.92\n SOUTH AMERICA |         | 1590283.70\n EUROPE        |         |  800652.92\n\n\nSELECT region,\n       avg(exports)                                     as avg_all,\n       avg(exports) filter ( WHERE trades.year &lt; 1995 ) as avg_old,\n       avg(exports) filter ( WHERE trades.year &gt;= 1995) as avg_latest\nFROM trades\nGROUP BY\n    rollup (region);\n\n     region      |        avg_all        |        avg_old        |      avg_latest       \n-----------------+-----------------------+-----------------------+-----------------------\n                 |  72443407670.13915858 |  42905138774.45808383 |  74914795899.38702405\n NORTH AMERICA   |  74417101760.04615385 |  62932794640.69230769 |  75693135884.41880342\n SOUTH AMERICA   | 109338636484.50140845 |  49195642528.37777778 | 118069071091.03548387\n ASIA            | 122562815407.87438424 |  59879433497.46250000 | 129413458239.61338798\n CENTRAL AMERICA |  34961597492.66203704 |  12904661532.05555556 |  36966773489.08080808\n\n\nSELECT AVG(imports), avg(exports)\nFROM trades;\n\n         avg          |         avg          \n----------------------+----------------------\n 73325290066.16504854 | 72443407670.13915858\n</code></pre>"},{"location":"Databases/sql/10%20Summarization/window/#more-queries","title":"More Queries","text":"<pre><code>SELECT country, year, imports, exports, avg(exports) OVER () as avg_exports\nFROM trades;\n\nSELECT country, year, imports, exports, avg(exports) OVER (partition by country) as avg_exports\nFROM trades;\n\nSELECT country, year, imports, exports, avg(exports) OVER (partition by year &lt; 2000 ) as avg_exports\nFROM trades;\n\n\nSELECT country, year, exports, min(exports) OVER (PARTITION BY country order by year)\nFROM trades\nWHERE year &gt; 2001\n  and country in ('USA', 'France');\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/","title":"Data Types","text":""},{"location":"Databases/sql/2%20Data%20Types/#data-types_1","title":"Data Types","text":""},{"location":"Databases/sql/2%20Data%20Types/#boolean-data","title":"Boolean Data","text":"<ul> <li>TRUE</li> <li>FALSE</li> <li>NULL</li> </ul> TRUE FALSE TRUE FALSE 'true' 'false' 't' 'f' 'yes' 'no' 'y' 'n' '1' '0' <pre><code>CREATE TABLE booltable (\n    id SERIAL PRIMARY KEY ,\n    is_enable BOOLEAN NOT NULL\n);\n\nINSERT INTO booltable (is_enable) VALUES (TRUE), ('true'), \n    ('y') , ('yes'), ('t'), ('1');\nINSERT INTO booltable (is_enable) VALUES (FALSE), ('false'), \n    ('n') , ('no'), ('f'), ('0');\n\nSELECT * FROM booltable;\n\nSELECT * FROM booltable WHERE is_enable = 'y';\n\nSELECT * FROM booltable WHERE NOT is_enable;\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/#character-data","title":"Character Data","text":"Character Type Notes CHARACTER (N), CHAR (N) fixed-length, blank padded CHARACTER VARYING (N), VARCHAR(N) variable length with length limit TEXT, VARCHAR variable unlimited length, max 1GB <ul> <li>n is default to 1</li> </ul> <pre><code>-- INPUT\nSELECT CAST('Uday' as character(10)) as \"name\";\n-- OUTPUT\n\"Uday      \"\n\n-- INPUT\nSELECT 'Uday'::character(10) as \"name\";\n-- OUTPUT\n\"Uday      \"\n\n-- INPUT\nSELECT 'uday'::varchar(10);\n-- OUTPUT\n\"uday\"\n\n-- INPUT\nSELECT 'lorem ipsum'::text;\n-- OUTPUT\n\"lorem ipsum\"\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/#numeric-data","title":"Numeric Data","text":"Types Notes Integers whole number, +ve and -ve Fixed-point, floating point for fractions of whole nu type size (bytes) min max smallint 2 -32678 32767 integer 4 -2,147,483,648 2,147,483,647 bigint 8 -9223372036854775808 9223372036854775807 type size range smallserial 2 1 to 32767 serial 4 1 to 2147483647 bigserial 8 1 to 9223372036854775807"},{"location":"Databases/sql/2%20Data%20Types/#fixed-point-data","title":"Fixed Point Data","text":"<p>numeric ( precision , scale ) | decimal ( precision , scale )</p> <ul> <li>precision : max number of digits to the left and right of the decimal point</li> <li>scale : number of digits allowable on the right of the decimal point</li> </ul>"},{"location":"Databases/sql/2%20Data%20Types/#floating-point-data","title":"Floating Point Data","text":"Type Notes Real allows precision to six decimal digits Double precision allows precision to 15 digits points of precision type size storage type Range numeric, decimal variable fixed point 131072 digits before decimal point and 16383 digits after the decimal point real 4 floating point 6 decimal digits precision double precision 8 floating point 15 decimal digits precision <pre><code>CREATE TABLE table_numbers (\n    col_numeric numeric(20,5),\n    col_real real,\n    col_double double precision\n);\n\nINSERT INTO table_numbers (col_numeric,col_real,col_double)\nVALUES (.9,.9,.9),\n       (3.34675,3.34675,3.34675),\n       (4.2345678910,4.2345678910,4.2345678910);\n\nSELECT * FROM table_numbers;\n\n-- OUTPUT\nlearning=# select * from table_numbers ;\n col_numeric | col_real | col_double  \n-------------+----------+-------------\n     0.90000 |      0.9 |         0.9\n     3.34675 |  3.34675 |     3.34675\n     4.23457 | 4.234568 | 4.234567891\n(3 rows)\n</code></pre> <p>Hierarchical order to SELECT best type : numeric &gt; decimal &gt; float</p>"},{"location":"Databases/sql/2%20Data%20Types/#date-time-data","title":"Date Time Data","text":"type stores low high Date date only 4713 BC 294276 AD Time time only 4713 BC 5874897 AD Timestamp date and time 4713 BC 294276 AD <code>Timestampz</code> date, time and timezone 4713 BC 294276 AD Interval difference btw time"},{"location":"Databases/sql/2%20Data%20Types/#date-type","title":"Date type","text":"<pre><code>CREATE TABLE table_dates (\n    id serial primary key,\n    employee_name varchar(100) not null,\n    hire_date DATE NOT NULL,\n    add_date DATE DEFAULT CURRENT_DATE\n);\n\nINSERT INTO table_dates (employee_name, hire_date)\n    VALUES ('uday','2020-02-02'),('another uday','2020-02-01');\n\nSELECT *\nFROM table_dates;\n\nSELECT NOW();\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/#time-type","title":"Time type","text":"<pre><code>CREATE TABLE table_time (\n    id serial primary key ,\n    class_name varchar(10) not null ,\n    start_time time not null ,\n    end_time time not null\n);\n\nINSERT INTO table_time (class_name, start_time, end_time) \n    VALUES ('maths','08:00:00','08:55:00'),\n           ('chemistry','08:55:00','09:00:00');\n\nSELECT * FROM table_time;\n\n-- OUTPUT\n\n id | class_name | start_time | end_time \n----+------------+------------+----------\n  1 | maths      | 08:00:00   | 08:55:00\n  2 | chemistry  | 08:55:00   | 09:00:00\n(2 rows)\n\n\nSELECT CURRENT_TIME;\n\n    current_time    \n--------------------\n 07:21:00.163354+00\n(1 row)\n\n\nSELECT CURRENT_TIME(2);\n\n  current_time  \n----------------\n 07:21:14.96+00\n(1 row)\n\n\nSELECT LOCALTIME;\n\n    localtime    \n-----------------\n 07:21:36.717509\n(1 row)\n\n\nSELECT time '12:10' - time '04:30' as RESULT;\n  result  \n----------\n 07:40:00\n(1 row)\n\n\n-- format : interval 'n type'\n-- n = number\n-- type : second, minute, hours, day, month, year ....\n\nSELECT CURRENT_TIME ,\n    CURRENT_TIME + INTERVAL '2 hours' as RESULT;\n\n    current_time    |       result       \n--------------------+--------------------\n 07:22:06.241919+00 | 09:22:06.241919+00\n(1 row)\n\n\nSELECT CURRENT_TIME ,\n    CURRENT_TIME + INTERVAL '-2 hours' as RESULT;\n\n    current_time    |       result       \n--------------------+--------------------\n 07:22:16.644727+00 | 05:22:16.644727+00\n(1 row)\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/#timestamp-and-timezone","title":"Timestamp and Timezone","text":"<ul> <li><code>timestamp</code> : stores time without time zone</li> <li><code>timestamptz</code> : timestamp with time zone , stored using UTC format</li> <li>adding timestamp to timestamptz without mentioning the zone will result in server automatically assumes timezone to system's timezone</li> <li>Internally, PostgreSQL will store the timezoneaccurately but then OUTPUTting the data, will it be converted according to your timezone</li> </ul> <pre><code>SELECT name FROM pg_timezone_names \n    where name = 'posix/Asia/Calcutta';\n\nSET TIMEZONE='Asia/Calcutta';\n\nSELECT NOW()::TIMESTAMP;\n\n            now             \n----------------------------\n 2021-08-12 12:53:03.971433\n(1 row)\n\n\nCREATE TABLE table_time_tz (\n    ts timestamp,\n    tstz timestamptz\n);\n\nINSERT INTO table_time_tz (ts, tstz) \n    VALUES ('2020-12-22 10:10:10',\n            '2020-12-22 10:10:10.009+05:30');\n\nSELECT * FROM table_time_tz;\n\n         ts          |             tstz              \n---------------------+-------------------------------\n 2020-12-22 10:10:10 | 2020-12-22 10:10:10.009+05:30\n(1 row)\n\n\nSELECT CURRENT_TIMESTAMP;\n\n        current_timestamp        \n---------------------------------\n 2021-08-12 12:53:29.54762+05:30\n(1 row)\n\n\nSELECT timezone('Asia/Singapore','2020-01-01 00:00:00')\n\n      timezone       \n---------------------\n 2020-01-01 02:30:00\n(1 row)\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/#uuid","title":"UUID","text":"<ul> <li>UUID : Universal Unique Identifier</li> <li>PostgreSQL doesn't provide internal function to generate UUID's, use <code>uuid-ossp</code></li> </ul> <pre><code>CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";\n\nSELECT uuid_generate_v1();\n\n           uuid_generate_v1           \n--------------------------------------\n 4d459e0c-fb3e-11eb-a638-0242ac110002\n\n\n-- pure randomness\nSELECT uuid_generate_v4();\n\n           uuid_generate_v4           \n--------------------------------------\n 418f39e5-8a46-4da2-8cea-884904f45d6f\n\n\nCREATE TABLE products_uuid (\n    id uuid default uuid_generate_v1(),\n    product_name varchar(100) not null\n);\n\nINSERT INTO products_uuid (product_name) \n    VALUES ('ice cream'),('cake'),('candies');\n\nSELECT * FROM products_uuid;\n\n                  id                  | product_name \n--------------------------------------+--------------\n 5cf1dbe0-fb3e-11eb-a638-0242ac110002 | ice cream\n 5cf1df28-fb3e-11eb-a638-0242ac110002 | cake\n 5cf1df46-fb3e-11eb-a638-0242ac110002 | candies\n\nCREATE TABLE products_uuid_v4 (\n    id uuid default uuid_generate_v4(),\n    product_name varchar(100) not null\n);\n\nINSERT INTO products_uuid_v4 (product_name) \n    VALUES ('ice cream'),('cake'),('candies');\n\nSELECT * FROM products_uuid_v4;\n\nlearning=# SELECT * FROM products_uuid_v4;\n                  id                  | product_name \n--------------------------------------+--------------\n 83b74bed-2cf8-4e26-80b0-c7c7b2e5f3e7 | ice cream\n ac563251-7a95-408d-966b-ed5ecc1f228d | cake\n 1079f6d3-b0c3-40ef-bd2e-da4467b63432 | candies\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/#hstore","title":"HSTORE","text":"<ul> <li>stores data in key-value pairs</li> <li>key and VALUES are text string only</li> </ul> <pre><code>CREATE EXTENSION IF NOT EXISTS hstore;\n\nCREATE TABLE table_hstore (\n    id SERIAL PRIMARY KEY ,\n    title varchar(100) not null,\n    book_info hstore\n);\n\nINSERT INTO table_hstore (title, book_info) VALUES\n(\n    'Title 1', ' \"publisher\" =&gt; \"ABC publisher\" , \n    \"paper_cost\" =&gt; \"100\" , \"e_cost\" =&gt; \"5.85\" '\n);\n\nSELECT * FROM table_hstore;\n\n id |  title  |   book_info                              \n\n  1 | Title 1 | \"e_cost\"=&gt;\"5.85\", \"publisher\"=&gt;\"ABC publisher\", \"paper_cost\"=&gt;\"100\"\n\n\nSELECT book_info -&gt; 'publisher' as publisher \nFROM table_hstore;\n\n   publisher   \n---------------\n ABC publisher\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/#json","title":"Json","text":"<ul> <li>PostgreSQL supports both </li> <li>JSON</li> <li>BSON or JSONB ( Binary JSON )</li> <li>JSONB has full support for indexing</li> </ul> <pre><code>CREATE TABLE table_json (\n    id SERIAL PRIMARY KEY ,\n    docs json\n);\n\nINSERT INTO table_json (docs) \n    VALUES ('[1,2,3,4,5,6]'),('{\"key\":\"value\"}');\n\nINSERT INTO table_json (docs)\nVALUES ('[{\"key\":\"value\"},{\"key2\":\"value2\"}]');\n\nSELECT * FROM table_json;\n\n id |                docs                 \n----+-------------------------------------\n  1 | [1,2,3,4,5,6]\n  2 | {\"key\":\"value\"}\n  3 | [{\"key\":\"value\"},{\"key2\":\"value2\"}]\n\n\nALTER TABLE table_json alter column docs type jsonb;\n\nSELECT * FROM table_json where docs @&gt; '2';\n\n id |        docs        \n----+--------------------\n  1 | [1, 2, 3, 4, 5, 6]\n\n\nCREATE index on table_json USING GIN (docs jsonb_path_ops);\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/#network-address-data-types","title":"Network Address Data Types","text":"Name Storage Size Notes cidr 7 or 19 bytes IPv4 and IPv6 networks inet 7 or 19 bytes IPv4 and IPv6 hosts and networks macaddr 6 bytes MAC addresses macaddr8 8 bytes MAC addresses ( EUI 64-bit ) <ul> <li>It is better to use these types instead of plain text types of store network address, because  these types offer input error checking and specialised operators and functions</li> <li>Supports indexing and advance operations</li> </ul> <pre><code>CREATE TABLE table_netaddr (\n    id SERIAL PRIMARY KEY ,\n    ip inet\n);\n\nINSERT INTO table_netaddr (ip)\nVALUES ('148.77.50.74'),\n        ('110.158.172.66'),\n        ('176.103.251.175'),\n        ('84.84.14.58'),\n        ('141.122.225.161'),\n        ('78.44.113.33'),\n        ('81.236.254.9'),\n        ('82.116.85.21'),\n        ('54.64.79.223'),\n        ('162.240.78.253');\n\nSELECT * FROM table_netaddr LIMIT 5;\n\n id |       ip        \n----+-----------------\n  1 | 148.77.50.74\n  2 | 110.158.172.66\n  3 | 176.103.251.175\n  4 | 84.84.14.58\n  5 | 141.122.225.161\n\n\nSELECT \n       ip, \n       set_masklen(ip,24) as inet_24, \n       set_masklen(ip::cidr,24) as cidr_24 ,\n       set_masklen(ip::cidr,27) as cidr_27,\n       set_masklen(ip::cidr,28) as cidr_28 \nFROM \n     table_netaddr LIMIT 2;\n\n ip | inet_24 | cidr_24 | cidr_27 | cidr_28 \n\n 148.77.50.74   | 148.77.50.74/24   | 148.77.50.0/24   | 148.77.50.64/27   | 148.77.50.64/28\n 110.158.172.66 | 110.158.172.66/24 | 110.158.172.0/24 | 110.158.172.64/27 | 110.158.172.64/28\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/arrays/","title":"Arrays","text":""},{"location":"Databases/sql/2%20Data%20Types/arrays/#arrays_1","title":"Arrays","text":"<ul> <li>Original Documentation : here</li> </ul>"},{"location":"Databases/sql/2%20Data%20Types/arrays/#syntax","title":"Syntax","text":"<pre><code>column_name DATATYPE[] {CONSTRAINT}\n</code></pre> <pre><code>CREATE TABLE table_array\n(\n    id     SERIAL,\n    name   varchar(100),\n    grades text[]\n);\n\nINSERT INTO table_array (name, grades)\nVALUES ('person 1', array ['100','45']);\nINSERT INTO table_array (name, grades)\nVALUES ('person 2', array ['100','90']);\nINSERT INTO table_array (name, grades)\nVALUES ('person 3', array ['100','97']);\nINSERT INTO table_array (name, grades)\nVALUES ('person 4', array ['100','94']);\n\n\nSELECT name, grades[1]\nFROM table_array;\n\n   name   | grades \n----------+--------\n person 1 | 100\n person 2 | 100\n person 3 | 100\n person 4 | 100\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/arrays/#array-in-tables","title":"Array in Tables","text":""},{"location":"Databases/sql/2%20Data%20Types/arrays/#insert","title":"Insert","text":"<ul> <li>for non text data , use <code>{value1,value2}</code> or <code>array ['value1','value2']</code></li> <li>for text data , use <code>{\"value1\",\"value2\"}</code> or <code>array [value1,value2]</code></li> </ul> <pre><code>CREATE TABLE teachers\n(\n    id    serial primary key,\n    class text[]\n);\n\nCREATE TABLE IF NOT EXISTS teachers\n(\n    id    serial primary key,\n    class text array\n);\n\nINSERT INTO teachers (class)\nVALUES (array ['english','maths']);\n\n id |      class      \n----+-----------------\n  1 | {english,maths}\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/arrays/#query","title":"Query","text":"<pre><code>SELECT class[1]\nFROM teachers;\n\n  class  \n---------\n english\n\n\nSELECT *\nFROM teachers\nWHERE class[1] = 'english';\n\n id |      class      \n----+-----------------\n  1 | {english,maths}\n\n\nSELECT *\nFROM teachers\nWHERE 'english' = any (class);\n\n id |      class      \n----+-----------------\n  1 | {english,maths}\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/arrays/#update","title":"Update","text":"<pre><code>update teachers\nset class[1] = 'dutch'\nWHERE id = 1;\n\n id |     class     \n----+---------------\n  1 | {dutch,maths}\n\nUpdate teachers\nset class[3] = 'science'\nWHERE id = 1;\n\n id |         class         \n----+-----------------------\n  1 | {dutch,maths,science}\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/arrays/#dimensionless","title":"Dimensionless","text":"<pre><code>CREATE TABLE teacher2\n(\n    id    serial primary key,\n    class text array[1]\n);\n\nINSERT INTO teacher2 (class)\nVALUES (array ['english']);\n\n id |   class   \n----+-----------\n  1 | {english}\n\n-- dimensions doesnt matter\nINSERT INTO teacher2 (class)\nVALUES (array ['english','hindi']);\n\n id |      class      \n----+-----------------\n  1 | {english}\n  2 | {english,hindi}\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/arrays/#unnest","title":"Unnest","text":"<pre><code>SELECT id, class, unnest(class)\nFROM teacher2;\n\n id |      class      | unnest  \n----+-----------------+---------\n  1 | {english}       | english\n  2 | {english,hindi} | english\n  2 | {english,hindi} | hindi\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/arrays/#multi-dimensional-array","title":"Multi Dimensional Array","text":"<pre><code>CREATE TABLE students\n(\n    id    serial primary key,\n    name  varchar(50) not null,\n    grade integer[][]\n);\n\nINSERT INTO students (name, grade)\nVALUES ('s1', '{90,2020}'),\n       ('s1', '{70,2020}'),\n       ('s1', '{60,2020}');\n\nSELECT *\nFROM students;\n\n id | name |   grade   \n----+------+-----------\n  1 | s1   | {90,2020}\n  2 | s1   | {70,2020}\n  3 | s1   | {60,2020}\n\nSELECT *\nFROM students\nWHERE grade @&gt; '{90}';\n\n id | name |   grade   \n----+------+-----------\n  1 | s1   | {90,2020}\n\nSELECT *\nFROM students\nWHERE '2020' = any (grade);\n\n id | name |   grade   \n----+------+-----------\n  1 | s1   | {90,2020}\n  2 | s1   | {70,2020}\n  3 | s1   | {60,2020}\n\nSELECT *\nFROM students\nWHERE grade[1] &lt; 80;\n\n id | name |   grade   \n----+------+-----------\n  2 | s1   | {70,2020}\n  3 | s1   | {60,2020}\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/arrays/#array-vs-jsonb","title":"Array vs JSONB","text":""},{"location":"Databases/sql/2%20Data%20Types/arrays/#advantages-to-array","title":"Advantages to Array","text":"<ul> <li>It's pretty easy to setup</li> <li>Requires less storage than jsonb</li> <li>It has multi dimensional support </li> <li>Indexing through GIN, greatly speeds up query</li> <li>The PostgreSQL planner is likely to make better decisions with PostgreSQL array, as it collects statistics on its content, but not with JSONB.</li> </ul>"},{"location":"Databases/sql/2%20Data%20Types/arrays/#disadvantages-to-array","title":"Disadvantages to Array","text":"<ul> <li>Its main advantages is that you are limited to one data type</li> <li>Have to follow strict order of the array data input.</li> </ul>"},{"location":"Databases/sql/2%20Data%20Types/arrays/#advantages-to-jsonb","title":"Advantages to JSONB","text":"<ul> <li>Provides additional operators for querying</li> <li>Support for indexing</li> </ul>"},{"location":"Databases/sql/2%20Data%20Types/arrays/#disadvantages-to-jsonb","title":"Disadvantages to JSONB","text":"<ul> <li>Has to parse the json data to binary format</li> <li>slow in writing, but faster in reading</li> <li>Doesn't maintain order</li> </ul>"},{"location":"Databases/sql/2%20Data%20Types/arrays/#ranges","title":"Ranges","text":"<pre><code>SELECT INT4RANGE(1, 6)                                                   AS \"DEFAULT [(\",\n       NUMRANGE(1.432, 6.238, '[]')                                      AS \"[]\",\n       DATERANGE('20200101', '20201222', '()')                           AS \"DATES ()\",\n       TSRANGE(LOCALTIMESTAMP, LOCALTIMESTAMP + INTERVAL '8 DAYS', '(]') AS \"OPENED CLOSED\";\n\n DEFAULT [( |      []       |        DATES ()         |                       OPENED CLOSED                       \n------------+---------------+-------------------------+-----------------------------------------------------------\n [1,6)      | [1.432,6.238] | [2020-01-02,2020-12-22) | (\"2021-08-24 05:22:13.03625\",\"2021-09-01 05:22:13.03625\"]\n\n\nSELECT ARRAY [1,2,3]        AS \"INT ARRAYS\",\n       ARRAY [2.123::FLOAT] AS \"FLOATING NUMBERS\",\n       ARRAY [CURRENT_DATE, CURRENT_DATE + 5];\n\n INT ARRAYS | FLOATING NUMBERS |          array          \n------------+------------------+-------------------------\n {1,2,3}    | {2.123}          | {2021-08-24,2021-08-29}\n\n\nSELECT ARRAY [1,2,3,4] = ARRAY [1,2,3,4],\n       ARRAY [1,2,3,4] = ARRAY [1,1,3,4],\n       ARRAY [1,2,3,4] &lt;&gt; ARRAY [1,2,3,4],\n       ARRAY [1,2,3,4] &lt; ARRAY [1,5,3,4],\n       ARRAY [1,2,3,4] &lt;= ARRAY [1,3,3,4],\n       ARRAY [1,2,3,4] &gt; ARRAY [1,2,3,4];\n\n ?column? | ?column? | ?column? | ?column? | ?column? | ?column? \n----------+----------+----------+----------+----------+----------\n t        | f        | f        | t        | t        | f\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/arrays/#inclusion-operators","title":"Inclusion Operators","text":"<pre><code>SELECT ARRAY [1,2,3,4] @&gt; ARRAY [2,3,4]       AS \"CONTAINS\",\n       ARRAY ['A','B'] &lt;@ ARRAY ['A','B','C'] AS \"CONTAINED BY\",\n       ARRAY [1,2,3,4] &amp;&amp; ARRAY [2,3,4]       AS \"IS OVERLAP\";\n\n CONTAINS | CONTAINED BY | IS OVERLAP \n----------+--------------+------------\n t        | t            | t\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/arrays/#length-and-dimensions","title":"Length and Dimensions","text":"<pre><code>SELECT ARRAY [1,2,3] || ARRAY [4,5,6] AS \"COMBINED ARRAY\";\n\n COMBINED ARRAY \n----------------\n {1,2,3,4,5,6}\n\nSELECT ARRAY_CAT(ARRAY [1,2,3],\n                 ARRAY [4,5,6]) AS \"COMBINED ARRAY VIA CAT\";\n\n COMBINED ARRAY VIA CAT \n------------------------\n {1,2,3,4,5,6}\n\nSELECT 4 || ARRAY [1,2,3] AS \"ADDING TO ARRAY\";\n\n ADDING TO ARRAY \n-----------------\n {4,1,2,3}\n\nSELECT ARRAY [1,2,3] || 4 AS \"ADDING TO ARRAY\";\n\n ADDING TO ARRAY \n-----------------\n {1,2,3,4}\n\nSELECT ARRAY_APPEND(ARRAY [1,2,3], 4) AS \"USING APPEND\";\n\n USING APPEND \n--------------\n {1,2,3,4}\n\nSELECT ARRAY_PREPEND(4, ARRAY [1,2,3]) AS \"USING APPEND\";\n\n USING APPEND \n--------------\n {4,1,2,3}\n\nSELECT ARRAY_NDIMS(ARRAY [[1,2,3,4],[1,2,3,4],[1,2,3,4]]) AS \"DIMENSIONS\",\n       ARRAY_DIMS(ARRAY [1,2,3,4,2,3,4])                  AS \"DIMENSIONS\";\n\n DIMENSIONS | DIMENSIONS \n------------+------------\n          2 | [1:7]\n\nSELECT ARRAY_LENGTH(ARRAY [-111,2,3,4], 1);\n\n array_length \n--------------\n            4\n\nSELECT ARRAY_UPPER(ARRAY [1,2,3,4000], 1),\n       ARRAY_LOWER(ARRAY [-100,2,3,4], 1);\n\n array_upper | array_lower \n-------------+-------------\n           4 |           1\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/arrays/#positions","title":"Positions","text":"<pre><code>SELECT array_position(array ['jan','feb','mar'], 'feb');\n\n array_position \n----------------\n              2\n\nSELECT array_position(array [1,2,2,3,4], 2, 3);\n\n array_position \n----------------\n              3\n\nSELECT array_positions(array [1,2,2,3,4], 2);\n array_positions \n-----------------\n {2,3}\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/arrays/#search-replace-remove","title":"Search, Replace, Remove","text":"<pre><code>SELECT array_cat(array [1,2], array [3,4]);\n\n array_cat \n-----------\n {1,2,3,4}\n\nSELECT array_append(array [1,2,3], 4);\n\n array_append \n--------------\n {1,2,3,4}\n\nSELECT array_remove(array [1,2,3,4,4,4], 4);\n\n array_remove \n--------------\n {1,2,3}\n\nSELECT array_replace(array [1,2,3,4,4,4], 4, 5);\n\n array_replace \n---------------\n {1,2,3,5,5,5}\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/arrays/#in-not-in-any","title":"IN, NOT IN, ANY","text":"<pre><code>SELECT 20 in (1, 2, 3, 20) as \"result\";\n-- t\nSELECT 25 in (1, 2, 3, 20) as \"result\";\n-- f\nSELECT 25 not in (1, 2, 3, 20) as \"result\";\n-- t\nSELECT 20 = all (Array [20,22]), 20 = all (array [20,20]);\n-- f\nSELECT 20 = any (Array [1,2,25]) as \"result\";\n-- f\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/arrays/#string-to-array","title":"STRING TO Array","text":"<pre><code>SELECT string_to_array('1,2,3,4,5', ',');\n\n string_to_array \n-----------------\n {1,2,3,4,5}\n\nSELECT string_to_array('1,2,3,4,5,ABC', ',', 'ABC');\n\n string_to_array  \n------------------\n {1,2,3,4,5,NULL}\n\nSELECT string_to_array('1,2,3,4,,6', ',', '');\n\n string_to_array  \n------------------\n {1,2,3,4,NULL,6}\n\nSELECT array_to_string(ARRAY [1,2,3,4], '|');\n\n array_to_string \n-----------------\n 1|2|3|4\n\nSELECT array_to_string(ARRAY [1,2,3,4,NULL], '|', 'EMPTY');\n\n  array_to_string \n-----------------\n 1|2|3|4|EMPTY\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/date-time-stamps/","title":"Date/Time/Stamps","text":""},{"location":"Databases/sql/2%20Data%20Types/date-time-stamps/#datetimestamps_1","title":"Date/Time/Stamps","text":""},{"location":"Databases/sql/2%20Data%20Types/date-time-stamps/#set-date-time-style","title":"Set Date Time Style","text":"<pre><code>-- show system date style\nSHOW datestyle;\n\n-- set new datestyle\nSET datestyle = 'ISO, DMY';\nSET datestyle = 'ISO, MDY';\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/date-time-stamps/#make","title":"Make","text":"<pre><code>SELECT MAKE_DATE (2020,01,01);\n make_date  \n------------\n 2020-01-01\n\nSELECT MAKE_DATE (2020,01,01);\n make_date  \n------------\n 2020-01-01\n\nSELECT MAKE_TIME(2,3,14.65);\n  make_time  \n-------------\n 02:03:14.65\n\nSELECT MAKE_TIMESTAMP (2020,02,02,10,20,45.44);\n\n    make_timestamp     \n------------------------\n 2020-02-02 10:20:45.44\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/date-time-stamps/#make_interval","title":"Make_interval","text":"<pre><code>SELECT MAKE_INTERVAL (2020,01,02,10,20,33);\n\n          make_interval           \n-----------------------------------\n 2020 years 1 mon 24 days 20:33:00\n\nSELECT MAKE_INTERVAL (days =&gt; 10);\n\nmake_interval \n---------------\n 10 days\n\nSELECT MAKE_INTERVAL (months =&gt; 7, days =&gt; 10, mins=&gt;35);\n\n     make_interval      \n-------------------------\n 7 mons 10 days 00:35:00\n\nSELECT MAKE_INTERVAL (weeks =&gt; 10);\n\nmake_interval \n---------------\n 70 days\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/date-time-stamps/#make_timestamptz","title":"Make_timestamptz","text":"<pre><code>SELECT make_timestamptz(2020,02,02,10,30,45.55,'Asia/Calcutta');\n\n     make_timestamptz      \n---------------------------\n 2020-02-02 05:00:45.55+00\n\nSELECT pg_typeof(make_timestamptz(2020,02,02,10,30,45.55));\n\n        pg_typeof         \n--------------------------\n timestamp with time zone\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/date-time-stamps/#date-value-extractor","title":"Date Value Extractor","text":"<ul> <li>https://www.postgresql.org/docs/8.1/functions-datetime.html</li> <li>https://www.postgresqltutorial.com/postgresql-extract/</li> </ul>"},{"location":"Databases/sql/2%20Data%20Types/date-time-stamps/#extract","title":"Extract","text":"<pre><code>select extract ('day' FROM current_timestamp), extract ('month' FROM current_timestamp), extract ('year' FROM current_timestamp);\n\n date_part | date_part | date_part \n-----------+-----------+-----------\n        14 |         8 |      2021\n\nselect extract('epoch' FROM current_timestamp);\n\n     date_part     \n-------------------\n 1628923887.158532\n\n\nselect extract('century' FROM current_timestamp);\n\n date_part \n-----------\n        21\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/date-time-stamps/#maths-operations-on-date-time","title":"Maths Operations on Date Time","text":"<pre><code>select '2020-02-02'::date + 04;\n\n  ?column?  \n------------\n 2020-02-06\n\nselect '23:59:59' + INTERVAL '1 SECOND';\n\n ?column? \n----------\n 24:00:00\n\nselect '23:59:59' + INTERVAL '2 SECOND';\n\n ?column? \n----------\n 24:00:01\n\nSELECT CURRENT_TIMESTAMP + '01:01:01';\n\n           ?column?            \n-------------------------------\n 2021-08-14 07:53:05.444791+00\n\nSELECT DATE '20200101' + TIME '10:25:10';\n\n      ?column?       \n---------------------\n 2020-01-01 10:25:10\n\nSELECT '10:10:10' + TIME '10:25:10';\n\n ?column? \n----------\n 20:35:20\n\nSELECT DATE '20200101' - INTERVAL '1 HOUR';\n\n      ?column?       \n---------------------\n 2019-12-31 23:00:00\n\nSELECT INTERVAL '30 MINUTES' + '2 HOUR';\n\n ?column? \n----------\n 02:30:00\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/date-time-stamps/#overlap","title":"Overlap","text":"<pre><code>select\n    ( DATE '2020-01-01' , DATE '2020-12-31' )\n    OVERLAPS\n    ( DATE '2020-12-30', DATE '2020-12-01' );\n\n overlaps \n----------\n t\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/date-time-stamps/#current","title":"Current","text":"<pre><code>select\n    current_date,\n    current_time,\n    current_time(2),\n    current_timestamp;\n\n current_date |    current_time    |  current_time  |       current_timestamp       \n 2021-08-14   | 06:53:52.187847+00 | 06:53:52.19+00 | 2021-08-14 06:53:52.187847+00\n\n\nselect localtime,\n    localtimestamp,\n    localtimestamp(2);\n\n    localtime    |       localtimestamp       |     localtimestamp     \n-----------------+----------------------------+------------------------\n 06:54:07.540777 | 2021-08-14 06:54:07.540777 | 2021-08-14 06:54:07.54\n\n\nselect\n   now(),\n   transaction_timestamp(),\n   clock_timestamp();\n\nnow | transaction_timestamp | clock_timestamp        \n\n2021-08-14 06:54:31.371838+00 | 2021-08-14 06:54:31.371838+00 | 2021-08-14 06:54:31.371924+00\n\n\nselect statement_timestamp(),\n   timeofday();\n\nstatement_timestamp      |              timeofday              \n-------------------------------+-------------------------------------\n 2021-08-14 06:55:07.202782+00 | Sat Aug 14 06:55:07.202849 2021 UTC\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/date-time-stamps/#age","title":"Age","text":"<pre><code>select age('2020-01-01', '2019-10-01');\n\n  age   \n--------\n 3 mons\n\nselect age(timestamp '2020-01-01');\n\n          age          \n-----------------------\n 1 year 7 mons 13 days\n\nselect age(current_date, '2020-01-01');\n\n          age          \n-----------------------\n 1 year 7 mons 13 days\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/date-time-stamps/#epochs","title":"Epochs","text":"<pre><code>select age ( timestamp '2020-12-20', timestamp '2020-10-20' );\n\n  age   \n--------\n 2 mons\n\n\nSELECT \n    EXTRACT (EPOCH FROM TIMESTAMPTZ '2020-10-20')\n    - EXTRACT (EPOCH FROM TIMESTAMPTZ '2020-08-20') \n        AS \"DIFFERENCE IN SECONDS\";\n\n DIFFERENCE IN SECONDS \n-----------------------\n               5270400\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/date-time-stamps/#timezone","title":"Timezone","text":"<pre><code>SELECT * FROM pg_timezone_names;\n\nSELECT * FROM pg_timezone_abbrevs;\n\nSHOW TIME ZONE;\n\nSET TIME ZONE 'Asia/Calcutta';\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/date-time-stamps/#date_part-and-date_trunc","title":"date_part and date_trunc","text":"<pre><code>SELECT date_part ('day', date '2021-11-07');\n\n date_part \n-----------\n         7\n\n\nSELECT date_trunc('hour', \n    timestamptz '2021-07-16 23:38:40.775719 +05:30');\n\n       date_trunc       \n------------------------\n 2021-07-16 18:00:00+00\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/json/","title":"JSON","text":""},{"location":"Databases/sql/2%20Data%20Types/json/#json_1","title":"JSON","text":""},{"location":"Databases/sql/2%20Data%20Types/json/#json-vs-jsonb","title":"JSON vs JSONB","text":"JSON JSONB stores data in text format stores data in binary format stores data AS-is trims of white spaces slower in operations fASter in operations doesn't support full text indexing supports full text indexing <pre><code>SELECT '{\n    \"title\":\"book 1\"}\n'::json;\n\n         json          \n-----------------------\n {                    +\n     \"title\":\"book 1\"}+\n\n(1 row)\n\n\nSELECT '\n  {\"title\":\"book 1\"}\n  '::jsonb\n\n\n        jsonb        \n---------------------\n {\"title\": \"book 1\"}\n(1 row)\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/json/#operations","title":"Operations","text":"<pre><code>CREATE TABLE books_jsonb\n(\n    id        serial primary key,\n    book_info JSONB\n);\n\nINSERT INTO books_jsonb (book_info)\nVALUES ('{\n  \"title\": \"Book 1\"\n}'),\n('{\n  \"title\": \"Book 2\"\n}'),\n('{\n  \"title\": \"Book 3\"\n}');\n\n id | title  \n----+--------\n  1 | Book 1\n  2 | Book 2\n  3 | Book 3\n\n\nSELECT id, book_info -&gt;&gt; 'title' AS \"title\"\nFROM books_jsonb\nWHERE book_info -&gt;&gt; 'title' = 'Book 1';\n\n id | title  \n----+--------\n  1 | Book 1\n\n\nINSERT INTO books_jsonb (book_info)\nVALUES ('{ \"title\": \"Book 10\" }');\n\n\n id |      book_info       \n----+----------------------\n  1 | {\"title\": \"Book 1\"}\n  2 | {\"title\": \"Book 2\"}\n  3 | {\"title\": \"Book 3\"}\n  4 | {\"title\": \"Book 10\"}\n\n\nUPDATE books_jsonb\nSET book_info = book_info || '{\"title\": \"Book 4\" }'\nWHERE book_info -&gt;&gt; 'title' = 'Book 10';\n\n id |      book_info      \n----+---------------------\n  1 | {\"title\": \"Book 1\"}\n  2 | {\"title\": \"Book 2\"}\n  3 | {\"title\": \"Book 3\"}\n  4 | {\"title\": \"Book 4\"}\n\n\nUPDATE books_jsonb\nSET book_info = book_info || '{\"author\": \"author 1\" }'\nWHERE book_info -&gt;&gt; 'title' = 'Book 1';\n\n id |                 book_info                 \n----+-------------------------------------------\n  2 | {\"title\": \"Book 2\"}\n  3 | {\"title\": \"Book 3\"}\n  4 | {\"title\": \"Book 4\"}\n  1 | {\"title\": \"Book 1\", \"author\": \"author 1\"}\n\n\nUPDATE books_jsonb\nSET book_info = book_info - 'author'\nWHERE book_info -&gt;&gt; 'title' = 'Book 1';\n\n id |      book_info      \n----+---------------------\n  1 | {\"title\": \"Book 1\"}\n  2 | {\"title\": \"Book 2\"}\n  3 | {\"title\": \"Book 3\"}\n  4 | {\"title\": \"Book 4\"}\n\n\nUPDATE books_jsonb\nSET book_info = book_info || '{\"available\":[\"new delhi\",\"Tokyo\",\"sydney\"]}'\nWHERE book_info -&gt;&gt; 'title' = 'Book 1';\n\n id |  book_info                                         \n\n  2 | {\"title\": \"Book 2\"}\n  3 | {\"title\": \"Book 3\"}\n  4 | {\"title\": \"Book 4\"}\n  1 | {\"title\": \"Book 1\", \"author\": \"author 1\", \"available\": [\"new delhi\", \"Tokyo\", \"sydney\"]}\n\n\nUPDATE books_jsonb\nSET book_info = book_info #- '{available,1}'\nWHERE book_info -&gt;&gt; 'title' = 'Book 1';\n\n id |   Book_info                                    \n\n  2 | {\"title\": \"Book 2\"}\n  3 | {\"title\": \"Book 3\"}\n  4 | {\"title\": \"Book 4\"}\n  1 | {\"title\": \"Book 1\", \"author\": \"author 1\", \"available\": [\"new delhi\", \"sydney\"]}\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/json/#row_to_json","title":"ROW_TO_JSON()","text":"<pre><code>SELECT row_to_json(orders)\nFROM orders;\n\n {\"order_id\":10248,\"customer_id\":\"VINET\",\"employee_id\":5,\"order_date\":\"1996-07-04\",\"required_date\":\"1996-08-01\",\"shipped_date\":\"1996-07-16\",\"ship_via\":3,\"freight\":32.38,\"ship_name\":\"Vins et alcools Chevalier\",\"ship_address\":\"59 rue de l'Abbaye\",\"ship_city\":\"Reims\",\"ship_region\":null,\"ship_postal_code\":\"51100\",\"ship_country\":\"France\"}\n\nSELECT row_to_json(t)\nFROM \n(\n  SELECT *\n  FROM orders\n) AS t;\n\n {\"order_id\":10248,\"customer_id\":\"VINET\",\"employee_id\":5,\"order_date\":\"1996-07-04\",\"required_date\":\"1996-08-01\",\"shipped_date\":\"1996-07-16\",\"ship_via\":3,\"freight\":32.38,\"ship_name\":\"Vins et alcools Chevalier\",\"ship_address\":\"59 rue de l'Abbaye\",\"ship_city\":\"Reims\",\"ship_region\":null,\"ship_postal_code\":\"51100\",\"ship_country\":\"France\"}\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/json/#json_agg","title":"JSON_AGG()","text":"<pre><code>SELECT *\nFROM orders;\n\nSELECT director_id, first_name, lASt_name, \n(\n  SELECT json_agg(x)\n  FROM \n    (\n        SELECT movie_name\n        FROM movies mv\n        WHERE mv.director_id = directors.director_id\n    ) AS x\n) :: jsonb\nFROM directors;\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/json/#json_build","title":"JSON_BUILD","text":"<pre><code>SELECT json_build_array(1, 2, 3, 4, 5, 6);\n\n  json_build_array  \n--------------------\n [1, 2, 3, 4, 5, 6]\n\n\nSELECT json_build_array(1, 2, 3, 4, 5, 6, 'Hi');\n\n     json_build_array     \n--------------------------\n [1, 2, 3, 4, 5, 6, \"Hi\"]\n\n\n-- error : argument list must have even number of elements\nSELECT json_build_object(1, 2, 3, 4, 5);\n\nSELECT json_build_object(1, 2, 3, 4, 5, 6, 7, 'Hi');\n\n            json_build_object            \n-----------------------------------------\n {\"1\" : 2, \"3\" : 4, \"5\" : 6, \"7\" : \"Hi\"}\n\n\nSELECT json_object('{name,email}', '{\"adnan\",\"a@b.com\"}');\n\n               json_object               \n-----------------------------------------\n {\"name\" : \"adnan\", \"email\" : \"a@b.com\"}\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/json/#json-functions","title":"Json Functions","text":"<pre><code>CREATE TABLE directors_docs\n(\n    id   serial primary key,\n    body jsonb\n);\n\n\nSELECT director_id,\n       first_name,\n       last_name,\n       (\n           SELECT json_agg(x) AS all_movies\n           FROM (\n                    SELECT movie_name\n                    FROM movies mv\n                    WHERE mv.director_id = directors.director_id\n                ) x\n       ) :: jsonb\nFROM directors;\n\n\nINSERT INTO directors_docs (body)\nSELECT row_to_json(a)\nFROM (\n         SELECT director_id,\n                first_name,\n                last_name,\n                (\n                    SELECT json_agg(x) AS all_movies\n                    FROM (\n                             SELECT movie_name\n                             FROM movies mv\n                             WHERE mv.director_id = directors.director_id\n                         ) x\n                ) :: jsonb\n         FROM directors\n) AS a;\n\nSELECT *\nFROM directors_docs LIMIT 3;\n\n  1 | {\"last_name\": \"Alfredson\", \"all_movies\": [{\"movie_name\": \"Let the Right One In\"}], \"first_name\": \"Tomas\", \"director_id\": 1}\n  2 | {\"last_name\": \"Anderson\", \"all_movies\": [{\"movie_name\": \"There Will Be Blood\"}], \"first_name\": \"Paul\", \"director_id\": 2}\n  3 | {\"last_name\": \"Anderson\", \"all_movies\": [{\"movie_name\": \"Grand Budapest Hotel\"}, {\"movie_name\": \"Rushmore\"}, {\"movie_name\": \"The Darjeeling Limited\"}], \"first_name\": \"Wes\", \"director_id\": 3}\n\n\nSELECT *, jsonb_array_length(body -&gt; 'all_movies') AS total_movies\nFROM directors_docs\norder by jsonb_array_length(body-&gt;'all_movies') DESC;\n\n 13 | {\"last_name\": \"Kubrick\", \"all_movies\": [{\"movie_name\": \"A Clockwork Orange\"}, {\"movie_name\": \"Eyes Wide Shut\"}, {\"movie_name\": \"The Shining\"}], \"first_name\": \"Stanley\", \"director_id\": 13}                                   |            3\n  3 | {\"last_name\": \"Anderson\", \"all_movies\": [{\"movie_name\": \"Grand Budapest Hotel\"}, {\"movie_name\": \"Rushmore\"}, {\"movie_name\": \"The Darjeeling Limited\"}], \"first_name\": \"Wes\", \"director_id\": 3}                                |            3\n 17 | {\"last_name\": \"Lucas\", \"all_movies\": [{\"movie_name\": \"Star Wars: A New Hope\"}, {\"movie_name\": \"Star Wars: Empire Strikes Back\"}, {\"movie_name\": \"Star Wars: Return of the Jedi\"}], \"first_name\": \"George\", \"director_id\": 17} |            3\n\n\nSELECT *,jsonb_object_keys(body) FROM directors_docs;\n\n  1 | {\"last_name\": \"Alfredson\", \"all_movies\": [{\"movie_name\": \"Let the Right One In\"}], \"first_name\": \"Tomas\", \"director_id\": 1} | last_name\n  1 | {\"last_name\": \"Alfredson\", \"all_movies\": [{\"movie_name\": \"Let the Right One In\"}], \"first_name\": \"Tomas\", \"director_id\": 1} | all_movies\n  1 | {\"last_name\": \"Alfredson\", \"all_movies\": [{\"movie_name\": \"Let the Right One In\"}], \"first_name\": \"Tomas\", \"director_id\": 1} | first_name\n\n\nSELECT j.key, j.value\nFROM directors_docs,\n     jsonb_each(body) j;\n\n    key     |                  value                   \n------------+------------------------------------------\n last_name  | \"Alfredson\"\n all_movies | [{\"movie_name\": \"Let the Right One In\"}]\n first_name | \"Tomas\"\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/json/#existence-operators","title":"Existence Operators","text":"<pre><code>SELECT *\nFROM directors_docs\nWHERE body -&gt; 'first_name' ? 'John';\n\n 14 | {\"last_name\": \"Lasseter\", \"all_movies\": [{\"movie_name\": \"Toy Story\"}], \"first_name\": \"John\", \"director_id\": 14}\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/json/#searching-json","title":"Searching JSON","text":"<pre><code>SELECT *\nFROM directors_docs\nWHERE body @&gt; '{\"first_name\":\"John\"}';\n\n\nSELECT *\nFROM directors_docs\nWHERE body @&gt; '{\"director_id\":1}';\n\n-- error : No operator matches the given name and argument types. You might need to add explicit type casts.\nSELECT *\nFROM directors_docs\nWHERE body -&gt; 'first_name' LIKE 'J%';\n\n\nSELECT *\nFROM directors_docs\nWHERE body -&gt;&gt; 'first_name' LIKE 'J%';\n\nSELECT *\nFROM directors_docs\nWHERE (body -&gt;&gt; 'director_id')::integer in (1,2,3,4,5,10);\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/miscellaneous-1/","title":"Internal Functions","text":"<p>Order of execution of SQL statements</p> <ol> <li>FROM</li> <li>WHERE</li> <li>SELECT</li> <li>ORDER BY</li> </ol>"},{"location":"Databases/sql/2%20Data%20Types/miscellaneous-1/#concatenation-operator","title":"Concatenation Operator","text":"<pre><code>select concat(first_name,last_name) as full_name \n    from directors limit 10;\n\n   full_name    \n----------------\n TomasAlfredson\n PaulAnderson\n WesAnderson\n\nselect concat_ws(' ',first_name,last_name) as full_name \n    from directors limit 3;\n\n    full_name    \n-----------------\n Tomas Alfredson\n Paul Anderson\n Wes Anderson\n</code></pre> <ul> <li>if you can have a null value in column, always use <code>concat_ws</code> because it will place nothing in that and and also not place the spacer like | or a space</li> </ul>"},{"location":"Databases/sql/2%20Data%20Types/miscellaneous-1/#type-conversion","title":"Type Conversion","text":"Type of Conversion Notes Implicit data conversion is done AUTOMATICALLY Explicit data conversion is done via 'conversion functions' eg. <code>CAST</code> or <code>::</code> <pre><code>SELECT * FROM movies;\n\n-- exact datatype match : no conversion\nSELECT * FROM movies WHERE movie_id = 1;\n\n-- Implicit conversion : conversion\nSELECT * FROM movies WHERE movie_id = '1';\n\n-- Explicit conversion : conversion\nSELECT * FROM movies WHERE movie_id = integer '1';\n\n-- Output of all queries above\n movie_id |     movie_name     | movie_length | movie_lang | release_date | age_certificate | director_id \n----------+--------------------+--------------+------------+--------------+-----------------+-------------\n        1 | A Clockwork Orange |          112 | English    | 1972-02-02   | 18              |          13\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/miscellaneous-1/#casting","title":"Casting","text":"<pre><code>-- CAST function\n-- Syntax : CAST ( expression as target_data_type );\n\nSELECT CAST ( '10' AS INTEGER );\n\n int4 \n------\n   10\n\nSELECT \n    CAST ('2020-02-02' AS DATE), \n    CAST('01-FEB-2001' AS DATE);\n\n    date    |    date    \n------------+------------\n 2020-02-02 | 2001-02-01\n\nSELECT \n    CAST ( 'true' AS BOOLEAN ), \n    CAST ( '1' AS BOOLEAN ), \n    CAST ( '0' AS BOOLEAN );\n\n bool | bool | bool \n------+------+------\n t    | t    | f\n\nSELECT CAST ( '14.87789' AS DOUBLE PRECISION );\n\n  float8  \n----------\n 14.87789\n\nSELECT '2020-02-02'::DATE , '01-FEB-2001'::DATE;\n\n    date    |    date    \n------------+------------\n 2020-02-02 | 2001-02-01\n\nSELECT '2020-02-02 10:20:10.23'::TIMESTAMP;\n\n       timestamp        \n------------------------\n 2020-02-02 10:20:10.23\n\nSELECT '2020-02-02 10:20:10.23 +05:30'::TIMESTAMPTZ;\n\n        timestamptz        \n---------------------------\n 2020-02-02 04:50:10.23+00\n\nSELECT \n    '10 minute'::interval, \n    '10 hour'::interval, \n    '10 day'::interval, \n    '10 week'::interval, \n    '10 month'::interval;\n\n interval | interval | interval | interval | interval \n----------+----------+----------+----------+----------\n 00:10:00 | 10:00:00 | 10 days  | 70 days  | 10 mons\n\nSELECT \n    20! AS \"result 1\" , \n    CAST( 20 AS bigint ) ! AS \"result 2\";\n\n      result 1       |      result 2       \n---------------------+---------------------\n 2432902008176640000 | 2432902008176640000\n\nSELECT \n    ROUND(10,4) AS \"result 1\", \n    ROUND ( CAST (10 AS NUMERIC) ) AS \"result 2\", \n    ROUND ( CAST (10 AS NUMERIC) , 4 ) AS \"result 3\";\n\n result 1 | result 2 | result 3 \n----------+----------+----------\n  10.0000 |       10 |  10.0000\n\nSELECT \n    SUBSTR('12345',2) AS \"RESULT 1\", \n    SUBSTR( CAST('12345' AS TEXT) ,2) AS \"RESULT 2\";\n\n RESULT 1 | RESULT 2 \n----------+----------\n 2345     | 2345\n</code></pre> <pre><code>CREATE TABLE ratings (\n    rating_id SERIAL PRIMARY KEY,\n    rating VARCHAR(2) NOT NULL\n);\n\nINSERT INTO ratings ( rating ) \nVALUES ('A'), ('B'), ('C'), ('D'), (1), (2), (3), (4);\n\nSELECT \n    rating_id,\n    CASE \n        WHEN rating~E'^\\\\d+$' THEN\n            CAST ( rating as INTEGER )\n        ELSE\n            0\n        END AS rating\nFROM\n    ratings;\n\n rating_id | rating \n-----------+--------\n         1 |      0\n         2 |      0\n         3 |      0\n         4 |      0\n         5 |      1\n         6 |      2\n         7 |      3\n         8 |      4\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/miscellaneous-1/#formatting-functions","title":"Formatting Functions","text":"<p>https://www.postgresql.org/docs/12/functions-formatting.html</p>"},{"location":"Databases/sql/2%20Data%20Types/miscellaneous-1/#to_char","title":"to_char()","text":"<p>Refer to the documentation</p> <ul> <li>https://www.postgresqltutorial.com/postgresql-to_char/</li> </ul> <pre><code>SELECT TO_CHAR (\n    100870,\n    '9,999999'\n);    \n\n  to_char  \n-----------\n    100870\n\nSELECT \n    release_date, \n    TO_CHAR(release_date,'DD-MM-YYYY'),\n    TO_CHAR(release_date,'Dy, MM, YYYY') \nFROM \n    movies LIMIT 3;\n\n release_date |  to_char   |    to_char    \n--------------+------------+---------------\n 1972-02-02   | 02-02-1972 | Wed, 02, 1972\n 1979-08-15   | 15-08-1979 | Wed, 08, 1979\n 2001-01-04   | 04-01-2001 | Thu, 01, 2001\n\nSELECT \n    TO_CHAR ( \n        TIMESTAMP '2020-01-01 13:32:30', \n        'HH24:MI:SS' \n    );\n\n to_char  \n----------\n 13:32:30\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/miscellaneous-1/#to_number","title":"to_number()","text":"<ul> <li>https://www.postgresqltutorial.com/postgresql-to_number/</li> </ul> <pre><code>SELECT TO_NUMBER(\n    '1420.89', '9999.'\n);\n\n to_number \n-----------\n      1420\n\nSELECT TO_NUMBER(\n    '10,625.78-', '99G999D99S'\n);\n\n to_number \n-----------\n -10625.78\n\nSELECT TO_NUMBER(\n    '$1,625.78+', '99G999D99S'\n);\n\n to_number \n-----------\n   1625.78\n\nSELECT to_number(\n    '$1,420.65' , 'L9G999D99'\n);\n\n to_number \n-----------\n   1420.65\n\nSELECT to_number(\n    '21,420.65' , '99G999D99'\n);\n\n to_number \n-----------\n  21420.65\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/miscellaneous-1/#to_date","title":"to_date()","text":"<ul> <li>https://www.postgresqltutorial.com/postgresql-to_date/</li> </ul> <pre><code>SELECT TO_DATE( '2020/10/22' , 'YYYY/MM/DD' );\n\n  to_date   \n------------\n 2020-10-22\n\nSELECT to_date( '022199' , 'MMDDYY' );\n\n  to_date   \n------------\n 1999-02-21\n\nSELECT to_date( 'March 07, 2019' , 'Month DD, YYYY' );\n\n    to_date   \n------------\n 2019-03-07\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/miscellaneous-1/#to_timestamp","title":"to_timestamp()","text":"<ul> <li>https://www.postgresqltutorial.com/postgresql-to_timestamp/</li> </ul> <pre><code>SELECT TO_TIMESTAMP(\n    '2017-03-31 9:30:20',\n    'YYYY-MM-DD HH:MI:SS'\n);\n\n      to_timestamp      \n------------------------\n 2017-03-31 09:30:20+00\n\nSELECT\n    TO_TIMESTAMP('2017     Aug','YYYY MON');\n\n      to_timestamp      \n------------------------\n 2017-08-01 00:00:00+00\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/miscellaneous-1/#string-functions","title":"String Functions","text":"<ul> <li><code>Upper(string)</code> </li> <li><code>Lower(string)</code></li> <li><code>INITCAP(string)</code></li> <li><code>REVERSE(string)</code></li> <li><code>LPAD(string)</code></li> <li><code>RPAD(string)</code></li> <li><code>LENGTH(string)</code></li> <li><code>CHAR_LENGTH(string) : Same as Length</code></li> <li><code>POSITION( string in string )</code></li> <li><code>STRPOS ( , &lt; substring &gt; )</code></li> <li><code>SUBSTRING (string , length)</code></li> <li><code>REPLACE (string, from_string, to_string)</code></li> </ul> <pre><code>SELECT INITCAP(first_name) as FirstName,\n       INITCAP(last_name)  as LastName\nFROM directors\nLIMIT 3;\n\n firstname | lastname\n-----------+-----------\n Tomas     | Alfredson\n Paul      | Anderson\n Wes       | Anderson\n\nSELECT LEFT('Uday', 3), RIGHT('Uday', 3);\n\n left | right\n------+-------\n Uda  | day\n\nSELECT LEFT('Uday', -3), RIGHT('Uday', -3);\n\n left | right\n------+-------\n U    | y\n\nSELECT REVERSE('UDAY YADAV');\n\n  reverse\n------------\n VADAY YADU\n\nSELECT SPLIT_PART('1,2,3,4', ',', 1), \n       SPLIT_PART('1|2|3|4', '|', 2);\n\n split_part | split_part\n------------+------------\n 1          | 2\n\nSELECT TRIM(LEADING FROM '  Amazing PostgreSQL'),\n       TRIM(TRAILING FROM 'Amazing PostgreSQL  '),\n       TRIM('  Amazing PostgreSQL  ');\n\n       ltrim        |       rtrim        |       btrim\n--------------------+--------------------+--------------------\n Amazing PostgreSQL | Amazing PostgreSQL | Amazing PostgreSQL\n\n\nSELECT TRIM(LEADING '0' FROM CAST(0001245 AS TEXT));\n\n ltrim\n-------\n 1245\n\n\nSELECT LTRIM('yummy', 'y'),\n       RTRIM('yummy', 'y'),\n       BTRIM('yummy', 'y');\n\n ltrim | rtrim | btrim\n-------+-------+-------\n ummy  | yumm  | umm\n\n\nSELECT upper('uday yadav'),  initcap('uday yadav');\n\n   upper    |  initcap\n------------+------------\n UDAY YADAV | Uday Yadav\n\nSELECT INITCAP(first_name) as FirstName,\n       INITCAP(last_name)  as LastName\nFROM directors LIMIT 3;\n\n firstname | lastname\n-----------+-----------\n Tomas     | Alfredson\n Paul      | Anderson\n Wes       | Anderson\n\nSELECT LEFT('Uday', 3), RIGHT('Uday', 3);\n left | right\n------+-------\n Uda  | day\n\nSELECT LEFT('Uday', -3), RIGHT('Uday', -3);\n left | right\n------+-------\n U    | y\n\n\nSELECT LPAD('Database', 15, '*'),\n       RPAD('Database', 15, '*');\n\n      lpad       |      rpad\n-----------------+-----------------\n *******Database | Database*******\n\nSELECT LENGTH('Uday Yadav');\n\n length\n--------\n     10\n\nSELECT LENGTH(CAST(10013 AS TEXT));\n length\n--------\n      5\n\n\nSELECT char_length(''),\n       char_length('  '),\n       char_length(NULL);\n\n char_length | char_length | char_length\n-------------+-------------+-------------\n           0 |           2 |\n\nSELECT first_name || ' ' || last_name as FullName,\n       LENGTH(first_name || ' ' || last_name)\n                                      as FullNameLength\nFROM Directors\nORDER BY 2 DESC LIMIT 2;\n\n             fullname              | fullnamelength\n-----------------------------------+----------------\n Florian  Henckel von Donnersmarck |             33\n Francis Ford Coppola              |             20\n\nSELECT POSITION('Amazing' IN 'Amazing PostgreSQL'),\n       POSITION('is' IN 'This is a computer');\n position | position\n----------+----------\n        1 |        3\n\nSELECT STRPOS('World Bank', 'Bank');\n strpos\n--------\n      7\n\nSELECT first_name,\n       last_name\nFROM directors\nWHERE strpos(last_name, 'on') &gt; 0 LIMIT 3;\n\n first_name | last_name\n------------+-----------\n Tomas      | Alfredson\n Paul       | Anderson\n Wes        | Anderson\n\nSELECT substring('What a wonderful world' from 1 for 10);\n\n substring\n------------\n What a won\n\nSELECT repeat('A', 4), repeat(' ', 9), repeat('.', 8);\n repeat |  repeat   |  repeat\n--------+-----------+----------\n AAAA   |           | ........\n\nSELECT REPLACE('ABC XYZ', 'XY', 'Z');\n replace \n---------\n ABC ZZ\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/sequences/","title":"Sequences","text":""},{"location":"Databases/sql/2%20Data%20Types/sequences/#sequences_1","title":"Sequences","text":"<ul> <li>Specify datatype ( <code>SMALLINT | INT | BIGINT</code> )</li> <li>Default is <code>BIGINT</code></li> </ul>"},{"location":"Databases/sql/2%20Data%20Types/sequences/#list-all-sequence","title":"List all sequence","text":"<pre><code>SELECT relname AS seq_name \n    FROM pg_class WHERE relkind = 'S';\n</code></pre> <pre><code>CREATE SEQUENCE IF NOT EXISTS test_sequence AS bigint;\n\nSELECT NEXTVAL('test_sequence');\n\n nextval \n---------\n       1\n\n\nSELECT CURRVAL('test_sequence');\n\n currval \n---------\n       1\n\nSELECT SETVAL('test_sequence',3);\n\n setval \n--------\n      3\n\n\n-- set this value after the nextval is called, \n-- check using the currval cmd\nSELECT SETVAL('test_sequence',300,false);\n\n-- CHECKING CURRENT VALUE \nSELECT CURRVAL('test_sequence');\n currval \n---------\n       3\n\n\nALTER SEQUENCE test_sequence RESTART WITH 100;\n\nSELECT NEXTVAL('test_sequence');\n nextval \n---------\n     100\n\nCREATE SEQUENCE IF NOT EXISTS test_seq3\nINCREMENT 50\nMINVALUE 100\nMAXVALUE 1000\nSTART WITH 150;\n\nSELECT nextval('test_seq3');\n\n  nextval \n---------\n     150\n\nCREATE SEQUENCE IF NOT EXISTS seq_des\nINCREMENT -1\nMINVALUE 1\nMAXVALUE 999\nSTART 99\nNO CYCLE | CYCLE ;\n\nSELECT nextval('seq_des');\n\n nextval \n---------\n      99\n\n-- DROP SEQUENCE\n\nDROP SEQUENCE IF EXISTS seq_des;\n\nCREATE TABLE IF NOT EXISTS table_seq (\n    id INT primary key ,\n    name VARCHAR(10)\n);\n\nCREATE sequence IF NOT EXISTS table_seq_id_seq\nstart with 1 owned BY table_seq.id;\n\nALTER TABLE table_seq\nALTER COLUMN id SET DEFAULT nextval('table_seq_id_seq')\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/sequences/#alpha-numeric-sequence","title":"Alpha-Numeric Sequence","text":"<pre><code>CREATE sequence table_text_seq;\n\nCREATE TABLE contacts (\n    id text NOT null default ('ID' || nextval('table_text_seq')),\n    name VARCHAR(150) NOT null\n);\n\nINSERT INTO  contacts (name) VALUES ('uday 1'),('uday 2'),('uday 3');\n\nSELECT * FROM contacts;\n\n id  |  name  \n-----+--------\n ID1 | uday 1\n ID2 | uday 2\n ID3 | uday 3\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/user-defined-data-types/","title":"User Defined Data Types","text":""},{"location":"Databases/sql/2%20Data%20Types/user-defined-data-types/#create-domain","title":"CREATE DOMAIN","text":"<ul> <li>Create user defined data type with a range, optional, DEFAULT, NOT NULL and CHECK Constraint.</li> <li>They are unique within schema scope.</li> <li>Helps standardise your database types in one place.</li> <li>Composite Type : Only single value return</li> </ul> <pre><code>CREATE DOMAIN name datatype constraint\n\n-- ex 1\n-- 'addr' with domain VARCHAR(100)\nCREATE DOMAIN addr VARCHAR(100) NOT NULL;\n\nCREATE TABLE locations (\n    address addr\n);\n\n            Table \"public.locations\"\n Column  | Type | Collation | Nullable | Default \n---------+------+-----------+----------+---------\n address | addr |           |          | \n\n\n-- Dropping Constraints\n-- if domain isnt used anywhere\ndrop domain addr;    \n\n-- this will drop the column in the table it is present in\n-- use this with caution\ndrop domain addr CASCADE;\n\n-- List all domains inside a schema\nselect typname from pg_catalog.pg_type \n join pg_catalog.pg_namespace \n on pg_namespace.oid = pg_type.typnamespace\n where typtype = 'd' and nspname = 'public';\n\n      typname      \n------------------\n positive_numeric\n valid_color\n addr\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/user-defined-data-types/#number-based-components","title":"Number Based Components","text":"<pre><code>-- Example 2\n-- 'positive_numeric' : value &gt; 0\n\nCREATE DOMAIN positive_numeric \n    INT NOT NULL CHECK (VALUE &gt; 0);\n\nCREATE TABLE sample (\n    number positive_numeric\n);\n\nINSERT INTO sample (NUMBER) VALUES (10);\n\n-- error\nINSERT INTO sample (NUMBER) VALUES (-10);\n\n-- ERROR:  value for domain positive_numeric \n--    violates check constraint \"positive_numeric_check\"\n\nSELECT * FROM sample;\n\n number \n--------\n     10\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/user-defined-data-types/#text-based-domain","title":"Text Based Domain","text":"<pre><code>-- Example 3\n-- check email domain\n\nCREATE DOMAIN \n     proper_email VARCHAR(150) \nCHECK \n    ( VALUE ~* '^[A-Za-z0-9._%-]+@[A-Za-z0-9.-]+[.][A-Za-z]+$' );\n\n\nCREATE TABLE email_check (\n    client_email proper_email\n);\n\ninsert into email_check (client_email) \n    values ('a@b.com') ;\n\n-- error \ninsert into email_check (client_email) \n    values ('a@#.com') ;\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/user-defined-data-types/#enum-based-domain","title":"Enum Based Domain","text":"<pre><code>-- enum based domain\nCREATE DOMAIN valid_color VARCHAR(10)\nCHECK (VALUE IN ('red','green','blue'));\n\nCREATE TABLE color (\n    color valid_color\n);\n\nINSERT INTO color (color) \n    VALUES ('red'),('blue'),('green');\n\n-- error\nINSERT INTO color (color) \n    VALUES ('yellow');\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/user-defined-data-types/#composite-data-types","title":"Composite Data Types","text":"<p>Syntax : <code>(composite_column).city</code></p>"},{"location":"Databases/sql/2%20Data%20Types/user-defined-data-types/#example-1","title":"Example 1","text":"<pre><code>-- address type\n\nCREATE TYPE address AS (\n    city VARCHAR(50),\n    country VARCHAR(100)\n);\n\nCREATE TABLE person (\n    id SERIAL PRIMARY KEY,\n    address address\n);\n\nINSERT INTO person ( address ) \n    VALUES (ROW('London','UK')), (ROW('New York','USA'));\n\nselect * from person;\n\n id |     address      \n----+------------------\n  1 | (London,UK)\n  2 | (\"New York\",USA)\n\nselect (address).country from person;\n\n country \n---------\n UK\n USA\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/user-defined-data-types/#example-2","title":"Example 2","text":"<pre><code>CREATE TYPE currency AS ENUM(\n    'USD','EUR','GBP','CHF'\n);\n\nSELECT 'USD'::currency\n\n currency \n----------\n USD\n\nSELECT 'INR'::currency\n\n-- ERROR:  invalid input value for enum currency: \"INR\"\n-- LINE 1: SELECT 'INR'::currency\n\nALTER TYPE currency ADD VALUE 'CHF' AFTER 'EUR';\n\nCREATE TABLE stocks (\n    id SERIAL PRIMARY KEY,\n    symbol currency\n);\n\ninsert into stocks ( symbol ) VALUES ('CHF');\n\nselect * from stocks\n id | symbol \n----+--------\n  1 | CHF\n\n-- DROP TYPE currency;\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/user-defined-data-types/#alter","title":"Alter","text":""},{"location":"Databases/sql/2%20Data%20Types/user-defined-data-types/#alter-type","title":"Alter TYPE","text":"<pre><code>ALTER TYPE addr RENAME TO user_address\n\nALTER TYPE user_address OWNER TO uday\n\nALTER TYPE user_address SET SCHEMA test_scm\n\nALTER TYPE test_scm.user_address \n    ADD ATTRIBUTE street_address VARCHAR(150)    \n\nCREATE TYPE mycolors AS ENUM ('green','red','blue')\n\nALTER TYPE mycolors RENAME VALUE 'red' TO 'orange'\n\nSELECT enum_range(NULL::mycolors);\n\nALTER TYPE mycolors ADD VALUE 'red' BEFORE 'green'\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/user-defined-data-types/#alter-enum","title":"ALTER ENUM","text":"<pre><code>CREATE TYPE status_enum AS enum \n('queued','waiting','running','done');\n\nCREATE TABLE jobs (\n    id SERIAL PRIMARY KEY,\n    job_status status_enum\n);\n\nINSERT INTO jobs ( job_status ) VALUES \n    ('queued'),('waiting'),('running'),('done');\n\nSELECT * FROM jobs;\n\n id | job_status \n----+------------\n  1 | queued\n  2 | waiting\n  3 | running\n  4 | done\n\n-- UPDATING waiting to running\n\nUPDATE jobs SET job_status = 'running' \n    WHERE job_status = 'waiting';\n\n id | job_status \n----+------------\n  1 | queued\n  3 | running\n  4 | done\n  2 | running\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/user-defined-data-types/#updatingreplacing-enum-domain","title":"Updating/Replacing ENUM domain","text":"<pre><code>ALTER TYPE status_enum RENAME TO status_enum_old;\n\nCREATE TYPE status_enum as enum \n    ('queued','running','done');\n\nALTER TABLE jobs ALTER COLUMN job_status \n    TYPE status_enum USING job_status::text::status_enum;\n\nDROP TYPE status_enum_old;\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/user-defined-data-types/#default-value-enum","title":"Default value ENUM","text":"<pre><code>CREATE TYPE  status AS ENUM \n    ('PENDING','APPROVED','DECLINE')\n\nCREATE TABLE cron_jobs (\n    id SERIAL,\n    status status DEFAULT 'PENDING'\n);\n\nINSERT INTO cron_jobs ( status ) VALUES ('APPROVED');\n</code></pre>"},{"location":"Databases/sql/2%20Data%20Types/user-defined-data-types/#create-domain-if-not-exists","title":"CREATE DOMAIN IF NOT EXISTS","text":"<pre><code>DO\n$$\nBEGIN\n    IF NOT EXISTS ( \n            SELECT \n                * \n            FROM pg_type tp \n            INNER JOIN \n                pg_namespace nsp ON nsp.oid = typ.typnamespace\n                WHERE nsp.nspname = current_schema() \n                AND typ.typname = 'a' \n            ) \n        THEN\n            CREATE TYPE ai AS ( \n                a TEXT, i INT \n            );\n    END IF;\nEND;\n$$\nLANGUAGE plpgsql;\n</code></pre>"},{"location":"Databases/sql/3%20Databases/","title":"Database","text":""},{"location":"Databases/sql/3%20Databases/#creating-a-database","title":"Creating a database","text":""},{"location":"Databases/sql/3%20Databases/#dropping-database","title":"Dropping Database","text":""},{"location":"Databases/sql/3%20Databases/#commands","title":"Commands","text":"<pre><code>create database testdb;\n\ndrop database if exists testdb;\n</code></pre>"},{"location":"Databases/sql/3%20Databases/#connect-to-db","title":"Connect to DB","text":"<pre><code>-- using psql\n\\c test\n</code></pre>"},{"location":"Databases/sql/3%20Databases/#see-list-of-tables","title":"See list of tables","text":"<pre><code>-- using psql\n\\d                       #to see tables\n\\d &lt;name of table&gt;       #to see details about table\n</code></pre>"},{"location":"Databases/sql/3%20Databases/schema/","title":"Schema","text":"<ul> <li>PostgreSQL schema's should be unique and different from each other</li> <li>Allows you to organise database objects</li> <li>Schema allow multiple users to interact with one database without interfering with each other</li> <li>Allow access and limit database objects to be accessed by the user.</li> </ul> <pre><code>-- syntax to create new schema\nCREATE SCHEMA sales;\n\n-- syntax to create new schema\nCREATE SCHEMA hr;\n\n-- syntax to rename existing schema\nALTER SCHEMA sales RENAME TO marketing;\n\n-- drop schema, DO THIS CAREFULLY !!\nDROP SCHEMA hr;\n\n-- specify the schema for the table explicitly\nselect * from hr.public.jobs;\n\n-- creating a sample table\nCREATE TABLE temporders ( id SERIAL PRIMARY KEY );\n\n-- moving the table from one schema to another\nALTER TABLE public.temporders SET SCHEMA marketing;\n\n-- show current schema\nselect current_schema();\n\n-- get default search path where the query will start looking\nshow search_path;\n   search_path   \n-----------------\n \"$user\", public\n\n-- order to search path is important \nSET search_path to '$user', marketing, public;\n</code></pre>"},{"location":"Databases/sql/3%20Databases/schema/#pg_catalog","title":"PG_CATALOG","text":"<ul> <li>PostgreSQL stores the metadata information about the database and cluster in the schema <code>pg_catalog</code>. </li> <li>This information is partially used by PostgreSQL itself to keep track things itself, but it also presented so external people/processes can understand whats inside the database.</li> <li><code>pg_catalog</code> schema contains system tables and all the built-in types, functions and operators.</li> <li><code>pg_catalog</code> is effectively part of the search path. Although if it is not named explicitly in the path then it is implicitly searched before searching the path's schema.</li> </ul> <pre><code>select * from information_schema.schemata;\n\n catalog_name |    schema_name     | schema_owner \n--------------+--------------------+--------------\n learning     | information_schema | postgres\n learning     | public             | postgres\n learning     | pg_catalog         | postgres\n learning     | pg_toast           | postgres\n</code></pre>"},{"location":"Databases/sql/4%20Tables/","title":"Table","text":""},{"location":"Databases/sql/4%20Tables/#creating-table","title":"Creating Table","text":""},{"location":"Databases/sql/4%20Tables/#altering-table","title":"Altering Table","text":""},{"location":"Databases/sql/4%20Tables/#add-column","title":"Add Column","text":"<pre><code>-- adding column to table\nALTER TABLE [schema].[table name]\n    ADD COLUMN [column name] [data type];\n\n-- example\nALTER TABLE public.rainfalls\n    ADD COLUMN is_enable boolean [CONSTRAINT];\n\n-- renaming column in table\nALTER TABLE public.rainfalls\n    RENAME is_enable TO accurate;\n\n-- add mulitple columns\nALTER TABLE rainfals\nADD COLUMN city VARCHAR(20),\nADD COLUMN pincode VARCHAR(50);\n\n-- create dummy table\nCREATE TABLE IF NOT EXISTS wrong_table ( name text );\n-- rename table using ALTER\nALTER TABLE wrong_table RENAME TO name_table;\nDROP TABLE IF EXISTS name_table;\n\n-- drop column\nALTER TABLE public.rainfalls\nDROP COLUMN pincode;\n\n-- change data type of column\nALTER TABLE public.rainfalls\nALTER COLUMN accurate TYPE TEXT;\n\n     Table \"public.rainfalls\"\n  Column  |         Type          | \n----------+-----------------------+-\n location | text                  | \n year     | integer               | \n month    | integer               | \n raindays | integer               | \n accurate | TEXT                  | \n city     | character varying(20) | \n\n-- altering column datatype\nALTER TABLE rainfalls  \nALTER COLUMN accurate TYPE REAL \nUSING accurate::REAL;\n\n     Table \"public.rainfalls\"\n  Column  |         Type          | \n----------+-----------------------+-\n location | text                  | \n year     | integer               | \n month    | integer               | \n raindays | integer               | \n accurate | real                  | \n city     | character varying(20) | \n\n\n-- set default value of column\nALTER TABLE users\nALTER COLUMN is_enable SET DEFAULT 'Y';\n</code></pre>"},{"location":"Databases/sql/4%20Tables/#delete","title":"Delete","text":"<pre><code>-- delete table\nDROP TABLE IF EXISTS [table name];\n\n-- delete row\nDROP TABLE IF EXISTS [table name];\n\n-- delete column\nALTER TABLE [table name] \nDROP column [column name];\n\n-- delete constraints\nALTER TABLE [table name] \nDROP CONSTRAINT [constrain name];\n</code></pre>"},{"location":"Databases/sql/4%20Tables/#select","title":"Select","text":"<p>The <code>SELECT</code> statement has the following clauses:</p> <ul> <li>Select distinct rows using DISTINCT operator.</li> <li>Sort rows using ORDER BY clause.</li> <li>Filter rows using WHERE clause.</li> <li>Select a subset of rows from a table using LIMIT or FETCH clause.</li> <li>Group rows into groups using GROUP BY clause.</li> <li>Filter groups using HAVING clause.</li> <li>Join with other tables using joins such as INNER JOIN, LEFT JOIN, FULL OUTER JOIN, CROSS JOIN clauses.</li> <li>Perform set operations using UNION, INTERSECT, and EXCEPT.</li> </ul> <pre><code>SELECT first_name, last_name\nFROM directors\nLIMIT 5;\n</code></pre> Representation Function ASC Ascending DESC Descending <pre><code>SELECT * FROM directors LIMIT 3;\n\n director_id | first_name | last_name | date_of_birth | nationality \n-------------+------------+-----------+---------------+-------------\n           1 | Tomas      | Alfredson | 1965-04-01    | Swedish\n           2 | Paul       | Anderson  | 1970-06-26    | American\n           3 | Wes        | Anderson  | 1969-05-01    | American\n\nSELECT * FROM directors ORBER BY date_of_birth ASC|DESC;\n\nSELECT DISTINCT nationality FROM directors;\n\n-- to select one column with condtion\nSELECT * FROM directors WHERE nationality = 'Chinese'; \n\n-- Two or more conditions\nSELECT \n           * \nFROM \n           directors \nWHERE \n           nationality = 'Mexican' \n           AND \n           date_of_birth='1964-10-09';\n</code></pre>"},{"location":"Databases/sql/4%20Tables/#column-alias","title":"Column Alias","text":"<pre><code>-- normal column aliases\nSELECT \n   first_name || ' ' || last_name as full_name\nFROM \n   directors LIMIT 5;\n\n-- with spaces\nSELECT\n    first_name || ' ' || last_name \"full name\"\nFROM\n    customer;\n</code></pre>"},{"location":"Databases/sql/4%20Tables/#insert","title":"Insert","text":"<pre><code>CREATE TABLE IF NOT EXISTS temp_table (\n    col1 text,\n    col2 text,\n);\n\nINSERT INTO table (col1, col2) \n    VALUES ( 'VALUE 1' , 'VALUE 2');\n\n-- MULTIPLE\nINSERT INTO table (col1, col2) \n    VALUES ( 'VALUE 1' , 'VALUE 2','VALUE 3','VALUE 4');\n\n-- STRING WITH QUOTES , add another ' before '\nINSERT INTO table (col1) \n    VALUES ( 'VALUE''S 1' );\n\n-- RETURNING ROWS\nINSERT INTO temp_table (col1, col2) \n    VALUES ( 'VALUE 5' , 'VALUE 6')\n    RETURNING *;\n</code></pre>"},{"location":"Databases/sql/4%20Tables/#updating","title":"Updating","text":"<pre><code>UPDATE directors set last_name = 'Walker' \nwhere director_id = 2;\n\n-- returning updated row\nupdate actors set last_name = 'Anderson' \nwhere director_id = 150 returning *;\n\n-- update all records\nupdate [table name] set [column name] = [value];\n\nUPDATE [table name]\nSET email = 'not found'\nWHERE\n    email IS NULL;\n</code></pre>"},{"location":"Databases/sql/4%20Tables/#upsert","title":"Upsert","text":"<pre><code>-- syntax for upsert\nINSERT INTO tablename ( col_list ) VALUES \n    ( value_list ) ON CONFLICT (COL_NAME)\n    DO\n        NOTHING \n        -- OR\n        UPDATE SET col = val where condition;\n\n-- syntax for upsert\nINSERT INTO tablename ( COL_NAME ) VALUES \n    ( value_list ) ON CONFLICT (COL_NAME)\n    DO\n        UPDATE SET COL_NAME = EXCLUDED.COL_NAME;\n</code></pre>"},{"location":"Databases/sql/4%20Tables/aggregation/","title":"Aggregation","text":"<ul> <li><code>COUNT (column)</code></li> <li><code>SUM (column)</code></li> <li><code>MIN &amp; MAX</code></li> <li><code>LEAST &amp; GREATEST</code></li> <li><code>AVG</code></li> </ul>"},{"location":"Databases/sql/4%20Tables/aggregation/#count","title":"<code>count</code>","text":"<pre><code>SELECT COUNT(DISTINCT (movie_lang))\nfrom movies;\n\n count \n-------\n     8\n\n-- with where clause\n\nSELECT COUNT(movie_lang)\nFROM movies\nwhere movie_lang = 'English';\n\n count \n-------\n    38\n</code></pre>"},{"location":"Databases/sql/4%20Tables/aggregation/#sum","title":"Sum","text":"<pre><code>select sum(revenues_domestic)\nfrom movies_revenues;\n\n  sum   \n--------\n 5719.5\n\nselect SUM(revenues_domestic::numeric)\nfrom movies_revenues\nwhere revenues_domestic::numeric &gt; 200;\n\n  sum   \n--------\n 3425.6\n\nSELECT SUM(DISTINCT revenues_domestic)\nFROM movies_revenues;\n\n  sum   \n--------\n 5708.4\n</code></pre>"},{"location":"Databases/sql/4%20Tables/aggregation/#min-and-max","title":"Min and Max","text":"<pre><code>SELECT min(movie_length), MAX(movie_length)\nFROM movies\n\n min | max \n-----+-----\n  87 | 168\n</code></pre>"},{"location":"Databases/sql/4%20Tables/aggregation/#average-greatest-latest","title":"Average, Greatest, Latest","text":"<pre><code>SELECT GREATEST(10, 20, 30, 40), LEAST(10, 20, 30, 40);\n\n greatest | least\n----------+-------\n       40 |    10\n\nSELECT GREATEST('A', 'B', 'C', 'D'), LEAST('A', 'B', 'C', 'D');\n\n greatest | least\n----------+-------\n D        | A\n\nSELECT GREATEST('A', 'B', 'C', 1);\n\n-- ERROR:  invalid input syntax for type integer: \"A\"\n-- LINE 1: SELECT GREATEST('A', 'B', 'C', 1);\n\nSELECT AVG(movie_length)\nFROM movies;\n\n         avg          \n----------------------\n 126.1320754716981132\n</code></pre>"},{"location":"Databases/sql/4%20Tables/coalesce/","title":"Usefull Functions","text":""},{"location":"Databases/sql/4%20Tables/coalesce/#case","title":"Case","text":"<pre><code>SELECT movie_id,\n       movie_name,\n       CASE\n           WHEN movies.movie_length &lt; 100\n               AND movies.movie_length &lt;= 50 THEN 'Short'\n           WHEN movies.movie_length &gt; 100\n               AND movies.movie_length &lt;= 130 THEN 'Medium'\n           WHEN movies.movie_length &gt; 130 THEN 'Long'\n           END duration\nFROM movies\nORDER BY movie_name;\nLIMIT 10;\n\n movie_id |          movie_name           | duration \n----------+-------------------------------+----------\n        1 | A Clockwork Orange            | Medium\n        2 | Apocalypse Now                | Long\n        3 | Battle Royale                 | Medium\n        4 | Blade Runner                  | Medium\n        5 | Chungking Express             | Medium\n        6 | City of God                   | Long\n        7 | City of Men                   | Long\n        8 | Cold Fish                     | Medium\n        9 | Crouching Tiger Hidden Dragon | Long\n       10 | Eyes Wide Shut                | Medium\n(10 rows)\n\n\nSELECT\n       SUM(CASE age_certificate\n             WHEN '12' THEN 1 \n             ELSE 0 \n           END) \"Kids\",\n       SUM(CASE age_certificate\n             WHEN '15' THEN 1 \n             ELSE 0 \n           END) \"School\",\n       SUM(CASE age_certificate\n             WHEN '18' THEN 1 \n             ELSE 0 \n           END) \"Teens\",\n       SUM(CASE age_certificate\n             WHEN 'PG' THEN 1 \n             ELSE 0 \n           END) \"Restricted\",\n       SUM(CASE age_certificate\n             WHEN 'U' THEN 1 \n             ELSE 0 \n           END) \"Universal\"\nFROM movies;\n\n  Kids | School | Teens | Restricted | Universal \n------+--------+-------+------------+-----------\n   11 |     16 |     8 |         12 |         6\n</code></pre>"},{"location":"Databases/sql/4%20Tables/coalesce/#coalesce","title":"Coalesce","text":"<p><code>COALESCE</code> function that returns the first non-null argument.</p> <pre><code>SELECT\n    COALESCE (NULL, 2 , 1);\n</code></pre>"},{"location":"Databases/sql/4%20Tables/coalesce/#nullif","title":"NULLIF","text":"<p>The <code>NULLIF</code> function returns a null value if <code>argument_1</code> equals to <code>argument_2</code>, otherwise it returns <code>argument_1</code></p> <pre><code>SELECT\n    NULLIF (1, 1); -- return NULL\n\nSELECT\n    NULLIF (1, 0); -- return 1\n\nSELECT\n    NULLIF ('A', 'B'); -- return A\n</code></pre>"},{"location":"Databases/sql/4%20Tables/coalesce/#cube","title":"Cube","text":"<ul> <li>First, specify the <code>CUBE</code> subclause in the the <code>GROUP BY</code> clause of the <code>SELECT</code> statement.</li> <li>Second, in the select list, specify the columns (dimensions or dimension columns) which you want to analyze and aggregation function expressions.</li> <li>Third, in the <code>GROUP BY</code> clause, specify the dimension columns within the parentheses of the <code>CUBE</code> subclause.</li> </ul> <pre><code>select age_certificate, movie_length\nfrom movies\ngroup by age_certificate, cube (age_certificate, movie_length )\norder by age_certificate;\n</code></pre>"},{"location":"Databases/sql/4%20Tables/combining-tables/","title":"Combining Tables","text":""},{"location":"Databases/sql/4%20Tables/combining-tables/#union","title":"UNION","text":"<ul> <li>Combines result sets from two or more <code>SELECT</code> statements into a single result set.</li> <li>The order and number of the columns in the select list of all queries must be the same</li> </ul> <pre><code>-- load sample data\nDROP TABLE IF EXISTS top_rated_films;\nCREATE TABLE top_rated_films(\n    title VARCHAR NOT NULL,\n    release_year SMALLINT\n);\n\nDROP TABLE IF EXISTS most_popular_films;\nCREATE TABLE most_popular_films(\n    title VARCHAR NOT NULL,\n    release_year SMALLINT\n);\n\nINSERT INTO \n   top_rated_films(title,release_year)\nVALUES\n   ('The Shawshank Redemption',1994),\n   ('The Godfather',1972),\n   ('12 Angry Men',1957);\n\nINSERT INTO \n   most_popular_films(title,release_year)\nVALUES\n   ('An American Pickle',2020),\n   ('The Godfather',1972),\n   ('Greyhound',2020);\n</code></pre>"},{"location":"Databases/sql/4%20Tables/combining-tables/#union-query","title":"Union Query","text":"<pre><code>SELECT * FROM top_rated_films\nUNION\nSELECT * FROM most_popular_films;\n\n          title           | release_year \n--------------------------+--------------\n An American Pickle       |         2020\n Greyhound                |         2020\n The Shawshank Redemption |         1994\n The Godfather            |         1972\n 12 Angry Men             |         1957\n</code></pre>"},{"location":"Databases/sql/4%20Tables/combining-tables/#union-all-query","title":"Union All Query","text":"<p>This will not avoid duplicate values</p> <pre><code>SELECT * FROM top_rated_films\nUNION ALL\nSELECT * FROM most_popular_films;\n\n               title      | release_year \n--------------------------+--------------\n The Shawshank Redemption |         1994\n--  The Godfather         |         1972\n 12 Angry Men             |         1957\n An American Pickle       |         2020\n-- The Godfather          |         1972\n Greyhound                |         2020\n</code></pre>"},{"location":"Databases/sql/4%20Tables/combining-tables/#union-with-order-by","title":"UNION with ORDER BY","text":"<pre><code>SELECT * FROM top_rated_films\nUNION ALL\nSELECT * FROM most_popular_films\nORDER BY title;\n\n          title           | release_year \n--------------------------+--------------\n 12 Angry Men             |         1957\n An American Pickle       |         2020\n Greyhound                |         2020\n The Godfather            |         1972\n The Godfather            |         1972\n The Shawshank Redemption |         1994\n</code></pre>"},{"location":"Databases/sql/4%20Tables/combining-tables/#intersect","title":"Intersect","text":"<p>Like the UNION and EXCEPT operators, the PostgreSQL <code>INTERSECT</code> operator combines result sets of two or more SELECT statements into a single result set.</p> <p>The <code>INTERSECT</code> operator returns any rows that are available in both result sets.</p> <p></p> <pre><code>SELECT *\nFROM most_popular_films \nINTERSECT\nSELECT *\nFROM top_rated_films;\n\n     title     | release_year \n---------------+--------------\n The Godfather |         1972\n</code></pre>"},{"location":"Databases/sql/4%20Tables/combining-tables/#except","title":"Except","text":"<p>The <code>EXCEPT</code> operator returns distinct rows from the first (left) query that are not in the output of the second (right) query.\\</p> <pre><code>SELECT * FROM top_rated_films\nEXCEPT \nSELECT * FROM most_popular_films;\n\n          title           | release_year \n--------------------------+--------------\n The Shawshank Redemption |         1994\n 12 Angry Men             |         1957\n</code></pre>"},{"location":"Databases/sql/4%20Tables/constraints/","title":"Constraints","text":"<ul> <li>Constraints are like Gate Keepers. Control the type of data that goes into the table. These are used to prevent invalid data.</li> <li>Constraints can be added on</li> <li>Table </li> <li>Column</li> </ul>"},{"location":"Databases/sql/4%20Tables/constraints/#types-of-constraints","title":"Types of Constraints","text":"Type Note NOT NULL Field must have values UNIQUE Only unique value are allowed DEFAULT Ability to SET default PRIMARY KEY Uniquely identifies each row/record FOREIGN KEY Constraint based on Column in other table CHECK Check all values must meet specific criteria"},{"location":"Databases/sql/4%20Tables/constraints/#not-null","title":"NOT NULL","text":"<pre><code>CREATE TABLE table_nn (\n    id SERIAL PRIMARY KEY,\n    tag text NOT NULL\n);\n\nINSERT INTO table_nn ( tag ) \n    VALUES ('TAG 1'), ('TAG 2'), ('TAG 3'), ('');\n\n-- NULL value wont be accepted\nINSERT INTO table_nn ( tag ) VALUES (NULL);\n\n-- empty string aren't NULL values\nINSERT INTO table_nn ( tag ) VALUES ('');\n\n-- adding another column TEXT;\nALTER TABLE table_nn \nADD COLUMN is_enable TEXT;\n\n-- Updating values in new column to '' \n-- so that NULL constrain can be added\nUPDATE table_nn\nSET is_enable = '' WHERE is_enable IS NULL;\n\n-- adding null constraint on table\nALTER TABLE public.table_nn\n    ALTER COLUMN is_enable SET NOT NULL;\n</code></pre>"},{"location":"Databases/sql/4%20Tables/constraints/#unique","title":"UNIQUE","text":"<pre><code>CREATE TABLE table_unique (\n    id SERIAL PRIMARY KEY,\n    emails TEXT UNIQUE\n);\n\nINSERT INTO table_unique ( emailS ) \n    VALUES ('A@B.COM'),('C@D.COM');\n\nINSERT INTO table_unique ( emailS ) \n    VALUES ('A@B.COM');\n-- error\n</code></pre>"},{"location":"Databases/sql/4%20Tables/constraints/#adding-unique-constraint-on-column","title":"Adding Unique Constraint on column","text":"<pre><code>ALTER TABLE public.table_unique\n    ADD COLUMN is_enable text;\n\nALTER TABLE table_unique\n    ADD CONSTRAINT unique_is_enable UNIQUE (is_enable);\n\n\nALTER TABLE public.table_unique\n    ADD COLUMN code text,\n    ADD COLUMN code_key text;\n\nALTER TABLE table_unique\n    ADD CONSTRAINT unique_code_key \n        UNIQUE (code,code_key);\n</code></pre>"},{"location":"Databases/sql/4%20Tables/constraints/#default-constraint","title":"DEFAULT CONSTRAINT","text":"<pre><code>CREATE TABLE table_default (\n    id SERIAL PRIMARY KEY,\n    is_enable TEXT DEFAULT 'Y'\n);\n\nINSERT INTO table_default ( id ) VALUES (1),(2);\n\nselect * from table_default;\n\n id | is_enable \n----+-----------\n  1 | Y\n  2 | Y\n\nALTER TABLE public.table_default\n    ADD COLUMN tag text;\n\nALTER TABLE public.table_default\n    ALTER COLUMN tag SET DEFAULT 'N';\n\nINSERT INTO table_default ( id ) VALUES (5);\n\nselect * from table_default;\n\n id | is_enable | tag \n----+-----------+-----\n  1 | Y         | \n  2 | Y         | \n  5 | Y         | N\n\n\nALTER TABLE public.table_default\n    ALTER COLUMN tag DROP DEFAULT;\n\n                              Table \"public.table_default\"\n  Column   |  Type   | Collation | Nullable |                  Default                  \n-----------+---------+-----------+----------+-------------------------------------------\n id        | integer |           | not null | nextval('table_default_id_seq'::regclass)\n is_enable | text    |           |          | 'Y'::text\n tag       | text    |           |          | \nIndexes:\n    \"table_default_pkey\" PRIMARY KEY, btree (id)\n</code></pre>"},{"location":"Databases/sql/4%20Tables/constraints/#primary-key","title":"Primary Key","text":"<ol> <li>Uniquely identifies each record in a database table</li> <li>There can be more than one UNIQUE, but only one primary key</li> <li>A primary key is a field in a table, which uniquely identifies each row/column in a database table</li> <li>When multiple fields are used as a primary key, which may consist of single or multiple fields, also known</li> </ol> <p>as composite key.</p> <pre><code>CREATE TABLE table_item (\n    item_id INTEGER PRIMARY KEY,\n    item_name varchar(20) NOT NULL    \n);\n\nINSERT INTO table_item ( item_id,item_name ) \n    VALUES (1, 'pencil'), (2, 'pen' ), (3, 'box' );\n\nINSERT INTO table_item ( item_id,item_name ) \n    VALUES (1, 'mobile');\n\n-- ERROR:  duplicate key value violates unique constraint \"table_item_pkey\"\n-- DETAIL:  Key (item_id)=(1) already exists.\n-- SQL state: 23505\n\nALTER TABLE table_item\nDROP CONSTRAINT table_item_pkey;\n\nALTER TABLE public.table_item\n    ADD PRIMARY KEY (item_id);\n</code></pre>"},{"location":"Databases/sql/4%20Tables/constraints/#composite-primary-key","title":"Composite Primary Key","text":"<pre><code>CREATE TABLE table_cpk (\n    id VARCHAR(20) NOT NULL,\n    another_id VARCHAR(20) NOT NULL,\n    grade_id VARCHAR(20) NOT NULL\n);\n\nINSERT INTO table_cpk ( id, another_id, grade_id ) \n    VALUES \n        ('1','11','12'), \n        ('2','21','22'), \n        ('3','31','32');\n\nSELECT * FROM table_cpk;\n\n-- composite key = id + another_id;\n\nALTER TABLE public.table_cpk\n    ADD CONSTRAINT cpk_comp_pkey PRIMARY KEY (id, another_id)\n</code></pre>"},{"location":"Databases/sql/4%20Tables/constraints/#foreign-key","title":"Foreign Key","text":"<pre><code>CREATE TABLE t_suppliers (\n    s_id SERIAL PRIMARY KEY,\n    s_name VARCHAR(20) NOT NULL\n);\n\nCREATE TABLE t_products (\n    p_id SERIAL PRIMARY KEY,\n    p_name  VARCHAR(10) NOT NULL,\n    s_id INT NOT NULL,\n    FOREIGN KEY (s_id) REFERENCES t_suppliers (s_id)\n);\n\nINSERT INTO t_suppliers ( s_name ) \n    VALUES ('SUP 1'),('SUP 2'),('SUP 3'),('SUP 4');\n\nINSERT INTO t_products ( P_NAME, S_ID ) \n    VALUES ('PRO 1',1),('PRO 2',2);\n\nINSERT INTO t_products ( P_NAME, S_ID ) \n    VALUES ('PRO 1',9);\n\n-- ERROR:  insert or update on table \"t_products\" \n-- violates foreign key constraint \"t_products_s_id_fkey\"\n-- DETAIL:  Key (s_id)=(9) is not present in table \"t_suppliers\".\n-- SQL state: 23503\n</code></pre>"},{"location":"Databases/sql/4%20Tables/constraints/#check-constraint","title":"CHECK Constraint","text":"<pre><code>CREATE TABLE table_constr (\n    id SERIAL PRIMARY KEY,\n    birth_date DATE CHECK ( birth_date &gt; '1900-01-01' ),\n    joined_date DATE CHECK ( joined_date &gt; birth_date ),\n    salary NUMERIC CHECK ( salary &gt; 0 )\n);\n\nselect * from table_constr;\n\ninsert into table_constr (birth_date, joined_date, salary) \nvalues \n    ('2001-02-02','2019-01-10',100000);\n\ninsert into table_constr (birth_date, joined_date, salary) \nvalues \n    ('2001-02-02','2000-01-10',100000);\n\ninsert into table_constr (birth_date, joined_date, salary) \nvalues \n    ('2001-02-02','2000-01-10',-100000);\n\nALTER TABLE public.table_constr\n    ADD COLUMN prices int;\n\nALTER TABLE table_constr\nADD CONSTRAINT price_check \nCHECK (\n    prices &gt; 0\n    AND salary &gt; prices\n);\n</code></pre>"},{"location":"Databases/sql/4%20Tables/constraints/#example","title":"Example","text":"<pre><code>CREATE TABLE web_links (\n    link_id SERIAL PRIMARY KEY,\n    link_url VARCHAR(255) NOT NULL,\n    link_target VARCHAR(20)\n);\n\nSELECT * FROM web_links;\n\nALTER TABLE web_links\nADD CONSTRAINT unique_web_url UNIQUE (link_url);\n\nINSERT INTO web_links (link_url,link_target) \n    VALUES ('https://www.google.com/','_blank');\n\nALTER TABLE web_links\nADD COLUMN is_enable VARCHAR(2);\n\nINSERT INTO web_links (link_url,link_target,is_enable) \n    VALUES ('https://www.amazon.com/','_blank','Y');\n\nALTER TABLE web_links\nADD CHECK ( is_enable IN ('Y','N') );\n\nINSERT INTO web_links (link_url,link_target,is_enable) \n    VALUES ('https://www.NETFLIX.com/','_blank','N');\n\nSELECT * FROM web_links;\n\nUPDATE web_links\nSET is_enable = 'Y'\nWHERE link_id = 1\n</code></pre>"},{"location":"Databases/sql/4%20Tables/cte/","title":"Common Table Expression","text":"<ul> <li><code>CTE</code> is a temporary result take from a SQL statement</li> <li>A second approach to create temporary tables for query data instead of using sub queries in a <code>FROM</code> clause </li> <li>CTE's are a good alternative to sub queries.</li> <li><code>CTE</code> can be referenced multiple times in multiple places in query statement</li> <li>Lifetime of <code>CTE</code> is equal to the lifetime of a query.</li> <li>Types of <code>CTE</code></li> <li>Materialized</li> <li>Not materialized</li> </ul>"},{"location":"Databases/sql/4%20Tables/cte/#syntax","title":"Syntax","text":"<pre><code>with cte_table ( column_list ) as (\n    cte_query_definition\n)\n\nwith num as (\n    select *\n    from generate_series(1, 5) as id\n)\nselect *\nfrom num;\n\n id \n----\n  1\n  2\n  3\n  4\n  5\n</code></pre>"},{"location":"Databases/sql/4%20Tables/cte/#cte-with-join","title":"CTE with Join","text":"<pre><code>with cte_director as (\n    select movie_id, movie_name, d.director_id, first_name\n    from movies\n             inner join directors d on d.director_id = movies.director_id\n)\nselect *\nfrom cte_director\nlimit 5;\n\n movie_id |       movie_name       | director_id | first_name \n----------+------------------------+-------------+------------\n       20 | Let the Right One In   |           1 | Tomas\n       46 | There Will Be Blood    |           2 | Paul\n       40 | The Darjeeling Limited |           3 | Wes\n       30 | Rushmore               |           3 | Wes\n       15 | Grand Budapest Hotel   |           3 | Wes\n</code></pre>"},{"location":"Databases/sql/4%20Tables/cte/#cte-with-case","title":"CTE with CASE","text":"<pre><code>WITH cte_film AS (\n    SELECT movie_name,\n           movie_length\n                    title,\n           (CASE\n                WHEN movie_length &lt; 100 THEN 'Short'\n                WHEN movie_length &lt; 120 THEN 'Medium'\n                ELSE 'Long'\n               END) length\n    FROM movies\n)\nSELECT *\nFROM cte_film\nWHERE length = 'Long'\nORDER BY title\nlimit 5;\n\n     movie_name     | title | length \n--------------------+-------+--------\n The Wizard of Oz   |   120 | Long\n Spirited Away      |   120 | Long\n Top Gun            |   121 | Long\n Leon               |   123 | Long\n Gone with the Wind |   123 | Long\n</code></pre>"},{"location":"Databases/sql/4%20Tables/cte/#complex-query-example","title":"Complex query example","text":"<pre><code>WITH cte_movie_count AS\n (\n     SELECT d.director_id,\n            SUM(COALESCE(r.revenues_domestic, 0) \n            + COALESCE(r.revenues_international, 0)) \n                    AS total_revenues\n     FROM directors d\n              INNER JOIN movies mv \n                  ON mv.director_id = d.director_id\n              INNER JOIN movies_revenues r \n                  ON r.movie_id = mv.movie_id\n     GROUP BY d.director_id\n )\nSELECT d.director_id,\n       d.first_name,\n       d.last_name,\n       cte.total_revenues\nFROM cte_movie_count cte\n         INNER JOIN directors d \n         ON d.director_id = cte.director_id\nLIMIT 5;\n</code></pre>"},{"location":"Databases/sql/4%20Tables/cte/#cte-to-perform-dml","title":"CTE to perform DML","text":""},{"location":"Databases/sql/4%20Tables/cte/#loading-sample-data","title":"Loading Sample Data","text":"<pre><code>create table articles\n(\n    id      serial,\n    article text\n);\n\ncreate table deleted_articles\n(\n    id      serial,\n    article text\n);\n\ninsert into articles (article)\nvalues ('article 1'),\n       ('article 2'),\n       ('article 3'),\n       ('article 4'),\n       ('article 5');\n</code></pre>"},{"location":"Databases/sql/4%20Tables/cte/#query","title":"Query","text":"<pre><code>select * from articles;\n\n id |  article  \n----+-----------\n  1 | article 1\n  2 | article 2\n  3 | article 3\n  4 | article 4\n  5 | article 5\n\n\nselect *\nfrom deleted_articles;\n\n id | article \n----+---------\n(0 rows)\n\n-- deleting from one table\n-- returning from\n\nwith cte_delete_article as (\n    delete from articles\n        where id = 1\n        returning *\n)\ninsert\ninto deleted_articles\nselect *\nfrom cte_delete_article;\n\nselect *\nfrom deleted_articles;\n\n-- output \n\n  id |  article  \n----+-----------\n  1 | article 1\n(1 row)\n</code></pre>"},{"location":"Databases/sql/4%20Tables/group-by-and-having/","title":"GROUP BY and HAVING","text":""},{"location":"Databases/sql/4%20Tables/group-by-and-having/#group-by","title":"GROUP BY","text":"<ul> <li><code>GROUP BY</code> clause divide the rows returned from <code>SELECT</code> statement into groups</li> <li>For each group, you can apply aggregate functions like <code>COUNT</code>, <code>SUM</code>, <code>MIN</code>, <code>MAX</code> etc.</li> </ul>"},{"location":"Databases/sql/4%20Tables/group-by-and-having/#syntax","title":"Syntax","text":"<pre><code>SELECT column1,\n       AGGREGATE_FUNCTION(column2)\nFROM tablename\nGROUP BY column1;\n</code></pre>"},{"location":"Databases/sql/4%20Tables/group-by-and-having/#group-by_1","title":"Group BY","text":"<p>Group the data in column and pass it to aggregate function</p>"},{"location":"Databases/sql/4%20Tables/group-by-and-having/#group-by-with-count","title":"Group BY with count","text":"<pre><code>SELECT \n    movie_lang,\n    COUNT(movie_lang) as count\nFROM\n    movies\nGROUP BY\n    movie_lang\nORDER BY \n    count ASC;\n\n movie_lang | count \n------------+-------\n Swedish    |     1\n German     |     1\n Korean     |     1\n Spanish    |     1\n Portuguese |     2\n Japanese   |     4\n Chinese    |     5\n English    |    38\n</code></pre>"},{"location":"Databases/sql/4%20Tables/group-by-and-having/#group-by-with-sum","title":"Group BY with SUM","text":"<pre><code>SELECT \n    age_certificate,\n    SUM(movie_length) \nFROM \n    movies \nGROUP BY \n    age_certificate;\n\n age_certificate | sum  \n-----------------+------\n PG              | 1462\n 15              | 2184\n 12              | 1425\n 18              |  994\n U               |  620\n</code></pre>"},{"location":"Databases/sql/4%20Tables/group-by-and-having/#group-by-with-min-max","title":"Group BY with MIN, MAX","text":"<pre><code>SELECT movie_lang,\n       MIN(movie_length),\n       MAX(movie_length)\nFROM movies\nGROUP BY movie_lang\n\n movie_lang | min | max \n------------+-----+-----\n Portuguese | 140 | 145\n German     | 165 | 165\n Chinese    |  99 | 139\n English    |  87 | 168\n Swedish    | 128 | 128\n Spanish    |  98 |  98\n Korean     | 130 | 130\n Japanese   | 107 | 120\n\nSELECT \n    movie_lang,\n    MIN(movie_length),\n    MAX(movie_length)\nFROM\n    movies\nGROUP BY\n    movie_lang\nORDER BY MAX(movie_length) DESC;\n\n movie_lang | min | max \n------------+-----+-----\n English    |  87 | 168\n German     | 165 | 165\n Portuguese | 140 | 145\n Chinese    |  99 | 139\n Korean     | 130 | 130\n Swedish    | 128 | 128\n Japanese   | 107 | 120\n Spanish    |  98 |  98\n</code></pre>"},{"location":"Databases/sql/4%20Tables/group-by-and-having/#having","title":"HAVING","text":"<ul> <li>We use <code>HAVING</code> clause to specify a search condition for a group or an aggregate</li> <li>The <code>HAVING</code> clause is often used with the <code>GROUP BY</code> clause to filter rows based on filter condition</li> <li>cannot use column alias with having clause because it is evaluated before the <code>SELECT</code> statement</li> </ul> <pre><code>SELECT \n    column1,\n    AGGREGATE_FUNCTION(column2),\nFROM tablename\nGROUP BY column1\nHAVING \n    condition;\n</code></pre> <ul> <li><code>HAVING AGGREGATE_FUNCTION(column2) = value</code></li> <li><code>HAVING AGGREGATE_FUNCTION(column2) &gt;= value</code></li> </ul> <pre><code>SELECT \n    movie_lang, \n    SUM(movie_length) \nFROM \n    movies \nGROUP BY \n    movie_lang \nHAVING SUM(movie_length) &gt; 200 \nORDER BY SUM(movie_length);\n\n movie_lang | sum  \n------------+------\n Portuguese |  285\n Japanese   |  446\n Chinese    |  609\n English    | 4824\n</code></pre>"},{"location":"Databases/sql/4%20Tables/group-by-and-having/#having-vs-where","title":"HAVING vs WHERE","text":"<ul> <li><code>HAVING</code> works on result group</li> <li><code>WHERE</code> works on <code>SELECT</code> columns and not on the result group</li> </ul> <pre><code>SELECT\n    movie_lang,\n    SUM(movie_length)\nFROM \n    movies\nGROUP BY movie_lang\nORDER BY 2 DESC;\n\n movie_lang | sum  \n------------+------\n English    | 4824\n Chinese    |  609\n Japanese   |  446\n Portuguese |  285\n German     |  165\n Korean     |  130\n Swedish    |  128\n Spanish    |   98\n</code></pre>"},{"location":"Databases/sql/4%20Tables/miscellaneous/","title":"OPERATORS","text":""},{"location":"Databases/sql/4%20Tables/miscellaneous/#logical","title":"Logical","text":"<pre><code>SELECT 1=1, 1&lt;1, 1&gt;1, 1&lt;=1, 1&gt;=1;\n\n ?column? | ?column? | ?column? | ?column? | ?column? \n----------+----------+----------+----------+----------\n t        | f        | f        | t        | t\n\nSELECT 1&lt;&gt;1; \n ?column? \n----------\n f\n\nSELECT 1 = 1 or 1 = 2 ;\n ?column? \n----------\n t\n\nselect 1 / 0;\n-- ERROR:  division by zero\n</code></pre>"},{"location":"Databases/sql/4%20Tables/miscellaneous/#pattern-matching","title":"Pattern Matching","text":"<pre><code>SELECT * FROM actors WHERE last_name LIKE '%son%';\n\n  first_name |  last_name  | gender |    date_of_birth    \n------------+-------------+--------+---------------------\n Woody      | Harrelson   | M      | 1961-07-23\n Samuel     | Jackson     | M      | 1948-12-21\n Lina       | Leandersson | F      | 1995-09-27\n Jack       | Nicholson   | M      | 1937-04-22\n Mykelti    | Williamson  | M      | 1957-03-04\n Luke       | Wilson      | M      | 1971-09-21\n Owen       | Wilson      | M      | 1968-11-18\n Patrick    | Wilson      | M      | 1973-07-03\n\n-- ILIKE\n-- to ignore the case\nSELECT * FROM actors WHERE last_name ILIKE '__i%'; \n\n first_name | last_name | gender |    date_of_birth    \n------------+-----------+--------+---------------------\n Hiroki     | Doi       | M      | 1999-08-10 \n Alec       | Guiness   | M      | 1914-04-02 \n Rumi       | Hiiragi   | F      | 1987-08-01 \n Miyu       | Irino     | M      | 1988-02-19 \n Keira      | Knightley | F      | 1985-03-26 \n Vivien     | Leigh     | F      | 1913-11-05 \n Yasmin     | Paige     | F      | 1991-06-24 \n Tilda      | Swinton   | F      | 1960-11-05 \n Robin      | Wright    | F      | 1966-04-08 \n\nselect 'hello' like 'hello';\nselect 'hello' like 'h%';\nselect 'hello' like '%e%';\nselect 'hello' like '%lo';\nselect 'hello' like '_ello';\nselect 'hello' like '__llo';\nselect 'hello' like '%ll_';\n\n ?column? | ?column? | ?column? | ?column? | ?column? | ?column? | ?column? \n----------+----------+----------+----------+----------+----------+----------\n t        | t        | t        | t        | t        | t        | t\n\n\n-- like with length of characters\nselect * from table where name like '____';\n</code></pre>"},{"location":"Databases/sql/4%20Tables/miscellaneous/#tips","title":"Tips","text":"<ul> <li>When using <code>AND</code> and <code>OR</code> in sample <code>SQL</code> statement, use brackets to differentiate between statements</li> <li><code>AND</code>  operator is processed before OR operator</li> <li><code>SQL</code> treats <code>AND</code> operator like multiplication and <code>OR</code> like divide</li> </ul>"},{"location":"Databases/sql/4%20Tables/miscellaneous/#fetch","title":"Fetch","text":"<pre><code>-- OFFSET start { ROW | ROWS }\n-- FETCH { FIRST | NEXT } { ROW_COUNT } { ROWS|ROW } ONLY\n\nSELECT\n    *\nFROM movies\nfetch first row only ;\n\n movie_id |     movie_name     | movie_length | movie_lang | release_date | age_certificate | director_id \n----------+--------------------+--------------+------------+--------------+-----------------+-------------\n        1 | A Clockwork Orange |          112 | English    | 1972-02-02   | 18              |          13\n\n\nSELECT\n    *\nFROM movies\noffset 3\nfetch next 10 row only ;\n\n movie_id |          movie_name           | movie_length | movie_lang | release_date | age_certificate | director_id \n----------+-------------------------------+--------------+------------+--------------+-----------------+-------------\n        4 | Blade Runner                  |          121 | English    | 1982-06-25   | 15              |          27\n        5 | Chungking Express             |          113 | Chinese    | 1996-08-03   | 15              |          35\n        6 | City of God                   |          145 | Portuguese | 2003-01-17   | 18              |          20\n        7 | City of Men                   |          140 | Portuguese | 2008-02-29   | 15              |          22\n        8 | Cold Fish                     |          108 | Japanese   | 2010-09-12   | 18              |          30\n        9 | Crouching Tiger Hidden Dragon |          139 | Chinese    | 2000-07-06   | 12              |          15\n       10 | Eyes Wide Shut                |          130 | English    | 1999-07-16   | 18              |          13\n       11 | Forrest Gump                  |          119 | English    | 1994-07-06   | PG              |          36\n       12 | Gladiator                     |          165 | English    | 2000-05-05   | 15              |          27\n       13 | Gone with the Wind            |          123 | English    | 1939-12-15   | PG              |           8\n</code></pre>"},{"location":"Databases/sql/4%20Tables/miscellaneous/#is-null-or-is-not-null","title":"IS NULL or IS NOT NULL","text":"<pre><code>select * from actors where date_of_birth is null;\n\n first_name | last_name | gender | date_of_birth \n------------+-----------+--------+---------------\n Xian       | Gao       | M      | \n</code></pre>"},{"location":"Databases/sql/4%20Tables/order-by/","title":"ORDER BY and DISTINCT","text":""},{"location":"Databases/sql/4%20Tables/order-by/#order-by","title":"Order BY","text":"<pre><code>SELECT\n    company_name,\n    contact_name\nFROM\n    customers\nORDER BY\n    company_name DESC,\n    contact_name\nLIMIT 10;\n\n           company_name            |      contact_name       \n-----------------------------------+-------------------------\n Wolski  Zajazd                    | Zbyszek Piestrzeniewicz\n Wilman Kala                       | Matti Karttunen\n White Clover Markets              | Karl Jablonski\n Wellington Importadora            | Paula Parente\n Wartian Herkku                    | Pirkko Koskitalo\n Vins et alcools Chevalier         | Paul Henriot\n Victuailles en stock              | Mary Saveley\n Vaffeljernet                      | Palle Ibsen\n Trails Head Gourmet Provisioners  | Helvetius Nagy\n Tradi\u00e7\u00e3o Hipermercados            | Anabela Domingues\n\n\nSELECT \n    orderNumber, \n    orderlinenumber, \n    quantityOrdered * priceEach as final_price\nFROM\n    orderdetails\nORDER BY \n   final_price DESC LIMIT 10;\n\n-- orders\n    order_id | product_id |    total_price     \n----------+------------+--------------------\n    10981 |         38 |              15810\n    10865 |         38 |              15810\n    10353 |         38 |              10540\n    10417 |         38 |              10540\n    10889 |         38 |              10540\n    10424 |         38 |              10329\n    10897 |         29 |               9903\n    10372 |         38 |               8432\n    10816 |         38 |               7905\n    10540 |         38 |               7905\n(10 rows)\n\n\n-- using alias in order by\nSELECT\n    first_name,\n    last_name as surname\nFROM\n    actors\nORDER BY\n    surname DESC ;\n\n first_name | surname \n------------+---------\n Ziyi       | Zhang\n Billy      | Zane\n Sean       | Young\n Jin-seo    | Yoon\n Ji-tae     | Yoo\n\n-- NULLS FIRST AND LAST\nselect *\nfrom actors\norder by gender NULLS LAST\nLIMIT 5;\n\n first_name | last_name | gender |    date_of_birth    \n------------+-----------+--------+---------------------\n Malin      | Akerman   | F      | 1978-05-12 00:00:00\n Julie      | Andrews   | F      | 1935-10-01 00:00:00\n Ivana      | Baquero   | F      | 1994-06-11 00:00:00\n Lorraine   | Bracco    | F      | 1954-10-02 00:00:00\n Alice      | Braga     | F      | 1983-04-15 00:00:00\n</code></pre>"},{"location":"Databases/sql/4%20Tables/order-by/#order-by-on-multiple-columns","title":"ORDER BY on multiple columns","text":"<p>The following statement selects the first name and last name from the customer table and sorts the rows by the first name in ascending order and last name in descending order:</p> <pre><code>SELECT\n    first_name,\n    last_name\nFROM\n    customer\nORDER BY\n    first_name ASC,\n    last_name DESC;\n</code></pre> First Name Last Name Kelly Torres Kelly Knott <p>In this example, the ORDER BY clause sorts rows by values in the first name column first. And then it sorts the sorted rows by values in the last name column.</p> <p>As you can see clearly from the output, two customers with the same first name Kelly have the last name sorted in descending order.</p>"},{"location":"Databases/sql/4%20Tables/order-by/#distinct","title":"DISTINCT","text":"<pre><code>SELECT DISTINCT region\nFROM customers\nWHERE country = 'USA'\nLIMIT 10;\n\n region \n--------\n NM\n CA\n AK\n WY\n OR\n MT\n ID\n WA\n(8 rows)\n</code></pre>"},{"location":"Databases/sql/4%20Tables/order-by/#distinct-count","title":"Distinct COUNT","text":"<pre><code>SELECT COUNT(DISTINCT region)\nFROM customers\nWHERE country = 'USA';\n\n count \n-------\n     8\n</code></pre>"},{"location":"Databases/sql/4%20Tables/views/","title":"Views","text":""},{"location":"Databases/sql/4%20Tables/views/#plain-view","title":"Plain View","text":"<p>A view is a database object that is a stored query. A view is a virtual table you can create dynamically using a saved query acting as a <code>virtual table.</code></p> <ul> <li>You can join a view to another table or view</li> <li>You can query a view</li> <li>Regular views don't store any data, but materialised view does</li> </ul>"},{"location":"Databases/sql/4%20Tables/views/#syntax","title":"Syntax","text":"<pre><code>CREATE OR REPLACE VIEW view_name AS query\n</code></pre>"},{"location":"Databases/sql/4%20Tables/views/#example-1","title":"Example 1","text":"<pre><code>CREATE OR REPLACE VIEW v_movie_quick AS\nSELECT movie_name,\n       movie_length,\n       release_date\nfrom movies mv;\n\n-- use just like a normal table\nselect * from v_movie_quick limit 5;\n\n     movie_name     | movie_length | release_date \n--------------------+--------------+--------------\n A Clockwork Orange |          112 | 1972-02-02\n Apocalypse Now     |          168 | 1979-08-15\n Battle Royale      |          111 | 2001-01-04\n Blade Runner       |          121 | 1982-06-25\n Chungking Express  |          113 | 1996-08-03\n</code></pre>"},{"location":"Databases/sql/4%20Tables/views/#example-with-join","title":"Example with JOIN","text":"<pre><code>CREATE OR REPLACE VIEW v_movie_d_name as\nSELECT movie_name,\n       movie_length,\n       release_date,\n       d.first_name || ' ' || d.last_name as \"full name\"\nfrom movies mv\n         inner join directors d \n          on mv.director_id = d.director_id;\n\nselect * from v_movie_d_name limit 5;\n\n       movie_name       | movie_length | release_date |    full name    \n------------------------+--------------+--------------+-----------------\n Let the Right One In   |          128 | 2008-10-24   | Tomas Alfredson\n There Will Be Blood    |          168 | 2007-12-26   | Paul Anderson\n The Darjeeling Limited |          119 | 2007-09-29   | Wes Anderson\n Rushmore               |          104 | 1998-11-12   | Wes Anderson\n Grand Budapest Hotel   |          117 | 2014-07-03   | Wes Anderson\n</code></pre>"},{"location":"Databases/sql/4%20Tables/views/#managing-view","title":"Managing VIEW","text":"<pre><code>ALTER VIEW v_movie_d_name RENAME TO v_movie_with_names;\n-- ALTER VIEW\n\nselect * from v_movie_with_names limit 5;\n-- SAME OUTPUT AS ABOVE\n\n-- dropping view\nDROP VIEW if exists v_movie_quick;\n</code></pre>"},{"location":"Databases/sql/4%20Tables/views/#view-containing-condition","title":"View containing condition","text":"<pre><code>CREATE OR REPLACE VIEW v_movie_after_1997 as\nselect *\nfrom movies\nwhere release_date &gt;= '1997-12-31'\n  and movie_lang = 'English'\norder by release_date desc limit 5;\n\n movie_id |                 movie_name                 | movie_length | movie_lang | release_date | age_certificate | director_id \n----------+--------------------------------------------+--------------+------------+--------------+-----------------+-------------\n       47 | Three Billboards Outside Ebbing, Missouri  |          134 | English    | 2017-11-10   | 15              |          18\n       15 | Grand Budapest Hotel                       |          117 | English    | 2014-07-03   | PG              |           3\n       22 | Life of Pi                                 |          129 | English    | 2012-11-21   | PG              |          15\n       38 | Submarine                                  |          115 | English    | 2011-06-03   | 15              |           4\n       24 | Never Let Me Go                            |          117 | English    | 2010-09-15   | 15              |          25\n</code></pre> <p>You cannot add, update, delete columns from a view once created for that create a new view, drop the old view and rename the new view back to original</p>"},{"location":"Databases/sql/4%20Tables/views/#deleting-from-view-also-deletes-from-the-main-table","title":"Deleting from view also deletes from the main table","text":"<pre><code>CREATE OR REPLACE VIEW v_movie as\nselect *\nfrom movies;\n\nselect *\nfrom v_movie limit 5;\n\nselect *\nfrom movies limit 5;\n\n-- deleting from v_movie also deletes from main table\ndelete\nfrom v_movie\nwhere movie_id = 4;\n</code></pre>"},{"location":"Databases/sql/4%20Tables/views/#materialized-view","title":"Materialized View","text":""},{"location":"Databases/sql/4%20Tables/views/#allows-you-to","title":"Allows you to","text":"<ul> <li>store result of a query</li> <li>update data periodically :: manual</li> <li>used to cache result of heavy data</li> </ul> <p>syntax</p> <pre><code>CREATE MATERIALIZED VIEW IF NOT EXISTS view_name \nAS query WITH [ NO ] DATA;\n</code></pre> <p>If you want to load data into the materialised view at the creation time, you will use <code>WITH DATA</code></p> <pre><code>create materialized view if not exists mv_dir as\nselect first_name, last_name\nfrom directors\nwith no data;\n\n-- this will give error becuz no data\n-- is loaded at time of creation\nselect * from mv_dir limit 5;\n\n-- refreshing view to load data in view\nrefresh materialized view  mv_dir;\n\nselect * from mv_dir limit 5;\n\n-- dropping materialized view\ndrop materialized view mv_dir;\n</code></pre> <ul> <li>cannot change data in <code>materialised view</code> : insert, update and delete </li> <li>advantage of using<code>materialised view</code> : access and update <code>materialised view</code> without locking everyone else out </li> <li>disadvantage of using <code>materialised view</code> if you alter the base table , </li> </ul> <p><code>materialised view</code> must also me alter : delete the old <code>materialised view</code> and create a new one</p>"},{"location":"Databases/sql/4%20Tables/where/","title":"WHERE Clause","text":"<p>Operators Available</p> Operator Description = Equal &gt; Greater than &lt; Less than &gt;= Greater than or equal &lt;= Less than or equal &lt;&gt; or != Not equal AND Logical operator AND OR Logical operator OR IN Return true if a value matches any value in a list BETWEEN Return true if a value is between a range of values LIKE Return true if a value matches a pattern IS NULL Return true if a value is NULL NOT Negate the result of other operators"},{"location":"Databases/sql/4%20Tables/where/#where","title":"Where","text":"<ul> <li>Cannot use column alias with where clause</li> </ul> <pre><code>SELECT select_list\nFROM table_name\nWHERE condition\nORDER BY sort_expression\n</code></pre>"},{"location":"Databases/sql/4%20Tables/where/#where-with-or","title":"Where with OR","text":"<pre><code>select first_name, last_name, date_of_birth\nfrom actors\nwhere date_of_birth &lt; '1990-01-01' \n   or date_of_birth &gt; '1980-01-01'\nLIMIT 10;\n\n first_name | last_name |    date_of_birth    \n------------+-----------+---------------------\n Malin      | Akerman   | 1978-05-12\n Tim        | Allen     | 1953-06-13\n Julie      | Andrews   | 1935-10-01\n Ivana      | Baquero   | 1994-06-11\n Lorraine   | Bracco    | 1954-10-02\n Alice      | Braga     | 1983-04-15\n Marlon     | Brando    | 1924-04-03\n Adrien     | Brody     | 1973-04-14\n Peter      | Carlberg  | 1950-12-08\n Gemma      | Chan      | 1982-11-29\n\nselect first_name, last_name, date_of_birth\nfrom actors\nwhere date_of_birth &lt; '1990-01-01'\n   or date_of_birth &gt; '1980-01-01'\nORDER BY date_of_birth\nLIMIT 10;\n\n-- with order by clause\n first_name | last_name |    date_of_birth    \n------------+-----------+---------------------\n Clark      | Gable     | 1901-02-01 00:00:00\n Scatman    | Crothers  | 1910-05-23 00:00:00\n Vivien     | Leigh     | 1913-11-05 00:00:00\n Alec       | Guiness   | 1914-04-02 00:00:00\n Judy       | Garland   | 1922-06-10 00:00:00\n Marlon     | Brando    | 1924-04-03 00:00:00\n Dick       | Van Dyke  | 1925-12-13 00:00:00\n Sihung     | Lung      | 1930-01-01 00:00:00\n Ian        | Holm      | 1931-09-12 00:00:00\n Rebecca    | Pan       | 1931-12-29 00:00:00\n</code></pre>"},{"location":"Databases/sql/4%20Tables/where/#where-with-and","title":"Where with AND","text":"<pre><code>select *\nfrom directors\nwhere date_of_birth &gt; '1970-01-01'\n  AND nationality = 'British';\n\n director_id | first_name | last_name | date_of_birth | nationality \n-------------+------------+-----------+---------------+-------------\n           4 | Richard    | Ayoade    | 1977-06-12    | British\n          18 | Martin     | McDonagh  | 1970-03-26    | British\n</code></pre>"},{"location":"Databases/sql/4%20Tables/where/#where-with-between-not-between","title":"Where with BETWEEN (NOT BETWEEN)","text":"<pre><code>select *\nfrom directors\nwhere date_of_birth between '1970-01-01' and '1990-01-01';\n\n director_id | first_name |        last_name         | date_of_birth | nationality \n-------------+------------+--------------------------+---------------+-------------\n           2 | Paul       | Anderson                 | 1970-06-26    | American\n           4 | Richard    | Ayoade                   | 1977-06-12    | British\n          11 | Florian    | Henckel von Donnersmarck | 1973-05-02    | German\n          18 | Martin     | McDonagh                 | 1970-03-26    | British\n</code></pre>"},{"location":"Databases/sql/4%20Tables/where/#where-with-like","title":"Where with LIKE","text":"<pre><code>SELECT first_name,\n       last_name\nFROM actors\nWHERE actors.last_name LIKE '%son'\nORDER BY first_name;\n\n first_name |  last_name  \n------------+-------------\n Jack       | Nicholson\n Lina       | Leandersson\n Luke       | Wilson\n Mykelti    | Williamson\n Owen       | Wilson\n Patrick    | Wilson\n Samuel     | Jackson\n Woody      | Harrelson\n</code></pre>"},{"location":"Databases/sql/4%20Tables/where/#where-with-in-and-not-in","title":"Where with IN and NOT IN","text":"<pre><code>select *\nfrom directors\nwhere nationality in ('American', 'Japenese');\n\n director_id | first_name |        last_name         | date_of_birth | nationality  \n-------------+------------+--------------------------+---------------+--------------\n           1 | Tomas      | Alfredson                | 1965-04-01    | Swedish\n           2 | Paul       | Anderson                 | 1970-06-26    | American\n           3 | Wes        | Anderson                 | 1969-05-01    | American\n           4 | Richard    | Ayoade                   | 1977-06-12    | British\n           5 | Luc        | Besson                   | 1959-03-18    | French\n           6 | James      | Cameron                  | 1954-08-16    | American\n           7 | Guillermo  | del Toro                 | 1964-10-09    | Mexican\n\nselect *\nfrom directors\nwhere nationality not in ('American', 'Japenese');\n\n director_id | first_name |        last_name         | date_of_birth | nationality  \n-------------+------------+--------------------------+---------------+--------------\n           1 | Tomas      | Alfredson                | 1965-04-01    | Swedish\n           4 | Richard    | Ayoade                   | 1977-06-12    | British\n           5 | Luc        | Besson                   | 1959-03-18    | French\n           7 | Guillermo  | del Toro                 | 1964-10-09    | Mexican\n          10 | Kinji      | Fukasaku                 | 1930-07-03    | Japanese\n          11 | Florian    | Henckel von Donnersmarck | 1973-05-02    | German\n          12 | Terry      | Jones                    | 1942-02-01    | British\n</code></pre>"},{"location":"Databases/sql/6-Advance-tables/internals/","title":"Advance Tables","text":""},{"location":"Databases/sql/6-Advance-tables/internals/#generated-columns","title":"Generated Columns","text":"<ul> <li>faster than triggers</li> </ul> <pre><code>create table if not exists area (\n    w real,\n    h real,\n    area real GENERATED ALWAYS AS ( w*h ) STORED\n);\n\nINSERT INTO area (w, h) values (2,3),(4,7);\n\nselect * from area;\n\nupdate area\nset w = 10\nwhere w = 4\n\nselect * from area;\n</code></pre>"},{"location":"Databases/sql/6-Advance-tables/internals/#internals","title":"Internals","text":"<pre><code>-- size of database \nSELECT \n    datname as db_name,\n    pg_size_pretty(pg_database_size(datname)) \n        as database_size\nFROM\n    pg_database\nORDER BY\n    pg_database_size(datname) DESC;\n\n-- list all databases and schema\nSELECT \n    catalog_name as \"Database Name\"\nFROM \n    information_schema.information_schema_catalog_name;\n\n-- list all schemas\nSELECT \n    catalog_name, schema_name, schema_owner\nFROM \n    information_schema.schemata;\n\n-- list all schema starting with pg_...\nSELECT *\nFROM information_schema.schemata\nWHERE schema_name LIKE 'pg%';\n\n-- list all tables\nSELECT * \nFROM  information_schema.tables\nWHERE table_schema = 'public'\n\n-- list all views\nSELECT * \nFROM  information_schema.views\nWHERE table_schema = 'public'\n\n-- views from information_schema\nSELECT * \nFROM  information_schema.views\nWHERE table_schema = 'information_schema'\n\n-- list all columns\nSELECT *\nFROM information_schema.columns\nWHERE table_name = 'orders'\n\n-- look at system metadata\nSELECT \n    CURRENT_CATALOG,\n    CURRENT_DATABASE(),\n    CURRENT_SCHEMA,\n    CURRENT_USER,\n    SESSION_USER;\n\n-- LOOK AT DATABASE VERSION\nSELECT VERSION();\n\nSELECT\n    has_database_privilege('learning','CREATE')\n    has_schema_privilege('public','USAGE'),\n    has_table_privilege('orders','INSERT'),\n    has_any_column_privilege('orders','SELECT');\n\nSELECT  current_setting('timezone');\n\n-- show all running queries\nSELECT  \n    pid,\n    age(clock_timestamp(),query_start),\n    usename as run_by_user_name,\n    query as running\nFROM pg_stat_activity\nWHERE query != '&lt;IDLE&gt;'\n    AND query NOT ILIKE '%pg_stat_activity%'\nORDER BY query_start DESC;\n\n-- show all idle query\nSELECT  \n    pid,\n    age(clock_timestamp(),query_start),\n    usename as run_by_user_name,\n    query as running\nFROM pg_stat_activity\nWHERE query = '&lt;IDLE&gt;'\nORDER BY query_start DESC;\n\n-- KILL running query\nSELECT pg_cancel_backend(pid);\n\n-- get live and dead rows in table\nSELECT\n    relname,\n    n_live_tup,\n    n_dead_tup\nFROM pg_stat_user_tables;\n\n-- show location of postgres data directory\nshow data_directory;\n\n-- show files of a table is located\nSELECT pg_relation_filepath('orders');\n</code></pre>"},{"location":"Databases/sql/6-Advance-tables/managing-tables/","title":"Managing Tables","text":""},{"location":"Databases/sql/6-Advance-tables/managing-tables/#creating-new-tables-with-into","title":"Creating new tables with INTO","text":"<pre><code>SELECT * \nINTO emp3\nFROM orders\nWHERE employee_id = 3;\n\nSELECT * FROM emp3;\n\nselect employee_id, ship_name from emp3 limit 5;\n\n employee_id |       ship_name        \n-------------+------------------------\n           3 | Victuailles en stock\n           3 | Hanari Carnes\n           3 | Wellington Importadora\n           3 | Wartian Herkku\n           3 | QUICK-Stop\n</code></pre>"},{"location":"Databases/sql/6-Advance-tables/managing-tables/#create-table-with-not-data","title":"Create table with NOT DATA","text":"<p>Just copying the table structure</p> <pre><code>CREATE TABLE emp1 as \n    (SELECT * FROM orders where employee_id = 1) \n    WITH NO DATA;\n\nSELECT * FROM emp1;\n</code></pre>"},{"location":"Databases/sql/6-Advance-tables/managing-tables/#tables-are-fraud","title":"Tables are Fraud","text":"<ul> <li>When you UPDATE value in database, it doesnt update the original row</li> <li>When you DELETE a row in table, it doesnt show you the new row, but it still remains in db</li> </ul> <pre><code>drop table table_vacuum;\n\nCREATE TABLE table_vacuum\n(\n    id integer\n);\n\nselect pg_total_relation_size('table_vacuum'),\n       pg_size_pretty(pg_total_relation_size('table_vacuum'));\n-- output : 0 bytes\n\nINSERT INTO table_vacuum\nSELECT *\nFROM generate_series(1, 400000);\n\nselect pg_total_relation_size('table_vacuum'),\n       pg_size_pretty(pg_total_relation_size('table_vacuum'));\n-- output : 14MB\n\nSELECT *\nFROM table_vacuum\nlimit 5;\n\nupdate table_vacuum\nset id = id + 2;\n\nselect pg_total_relation_size('table_vacuum'),\n       pg_size_pretty(pg_total_relation_size('table_vacuum'));\n-- output : 28MB\n\nSELECT *\nFROM table_vacuum\nlimit 5;\n\n-- look at autovacuum process\n\n    SELECT relname,\n           last_vacuum,\n           last_autovacuum,\n           last_analyze,\n           vacuum_count,\n           autovacuum_count\n    FROM pg_stat_all_tables\n        WHERE relname = 'table_vacuum';\n\nVACUUM FULL VERBOSE table_vacuum;\n\nselect pg_total_relation_size('table_vacuum'),\n       pg_size_pretty(pg_total_relation_size('table_vacuum'));\n-- output : 14MB\n</code></pre>"},{"location":"Databases/sql/6-Advance-tables/paritioning-tables/","title":"Partitioning Tables","text":"<p>It is splitting table into</p> <ul> <li>logical division</li> <li>multiple smaller pieces</li> <li>more manageable pieces table</li> </ul> <p>Partition leads to a huge performance boost.</p> <p>Types of ranges</p> <ul> <li><code>Range</code> : The table is partitioned into \"range\" defined by a key column or set of columns, with no overlap between the ranges of values assigned to different partitions</li> <li><code>List</code> : According to a key in table, ex : country, sales</li> <li><code>Hash</code> : The partition specifying a modulus and a reminder for each partition. </li> </ul>"},{"location":"Databases/sql/6-Advance-tables/paritioning-tables/#table-inheritance","title":"Table Inheritance","text":"<pre><code>create table master (\n    pk INTEGER primary key ,\n    tag text,\n    parent integer\n);\n\ncreate table master_child() inherits (master);\n\nALTER TABLE public.master_child\n    ADD PRIMARY KEY (pk);\n\n\nselect * from master;\nselect * from master_child;\n\ninsert into master (pk, tag, parent) values (1,'pencil',0);\ninsert into master_child (pk, tag, parent) values (2,'pen',0);\n\nupdate master set tag = 'monitor' where pk = 2;\n\nselect * from only master;\nselect * from only master_child;\n\n-- error\ndrop table master;\ndrop table master cascade ;\n</code></pre>"},{"location":"Databases/sql/6-Advance-tables/paritioning-tables/#range-partitioning","title":"Range Partitioning","text":"<pre><code>create table employees_range (\n    id bigserial,\n    birth_date DATE NOT NULL ,\n    country_code VARCHAR(2) NOT NULL\n) PARTITION BY RANGE (birth_date);\n\nCREATE TABLE employee_range_y2000 PARTITION of\n    employees_range for values from ('2000-01-01') to ('2001-01-01');\n\nCREATE TABLE employee_range_y2001 PARTITION of\n    employees_range for values from ('2001-01-01') to ('2002-01-01');\n\n\ninsert into employees_range (birth_date, country_code) values\n('2000-01-01','US'),\n('2000-01-02','US'),\n('2000-12-31','US'),\n('2000-01-01','US'),\n('2001-01-01','US'),\n('2001-01-02','US'),\n('2001-12-31','US'),\n('2001-01-01','US');\n\nselect * from employees_range;\nselect * from only employees_range; -- you will get nothing\nselect * from employee_range_y2000;\nselect * from employee_range_y2001;\n\nexplain analyze select * from employees_range \nwhere birth_date = '2001-01-01';\n</code></pre>"},{"location":"Databases/sql/6-Advance-tables/paritioning-tables/#list-partitioning","title":"List Partitioning","text":"<pre><code>create table employee_list (\n    id bigserial,\n    birth_date DATE NOT NULL ,\n    country_code VARCHAR(2) not null\n) PARTITION BY LIST ( country_code );\n\ncreate table employee_list_eu PARTITION of employee_list\n    for values in ('UK','DE');\n\ncreate table employee_list_us PARTITION of employee_list\n    for values in ('US','BZ');\n</code></pre>"},{"location":"Databases/sql/6-Advance-tables/paritioning-tables/#hash-partitioning","title":"Hash Partitioning","text":"<pre><code>create table employee_hash (\n    id bigserial,\n    birth_date DATE NOT NULL,\n    country_code varchar(2) not null\n) PARTITION BY HASH (id);\n\ncreate table employee_hash_0 partition of employee_hash\nfor values with (modulus 3, remainder 0);\n\ncreate table employee_hash_1 partition of employee_hash\nfor values with (modulus 3, remainder 1);\n\ncreate table employee_hash_2 partition of employee_hash\nfor values with (modulus 3, remainder 2);\n</code></pre>"},{"location":"Databases/sql/6-Advance-tables/paritioning-tables/#default-partitioning","title":"Default Partitioning","text":"<pre><code>create table employee_default_part partition of employee_list default;\n</code></pre>"},{"location":"Databases/sql/6-Advance-tables/paritioning-tables/#multilevel-partitioning","title":"Multilevel Partitioning","text":"<pre><code>create table employee_list_master (\n    id bigserial,\n    birth_date DATE NOT NULL ,\n    country_code VARCHAR(2) not null\n) PARTITION BY LIST ( country_code );\n\ncreate table employee_list_eu_m PARTITION of employee_list_master\n    for values in ('UK','DE')\n    partition by hash (id);\n\ncreate table employee_list_us_m PARTITION of employee_list_master\n    for values in ('US','BZ');\n\ncreate table employee_hash_0_m partition of employee_list_eu_m\nfor values with (modulus 3, remainder 0);\n\ncreate table employee_hash_1_m partition of employee_list_eu_m\nfor values with (modulus 3, remainder 1);\n\ncreate table employee_hash_2_m partition of employee_list_eu_m\nfor values with (modulus 3, remainder 2);\n</code></pre>"},{"location":"Databases/sql/6-Advance-tables/paritioning-tables/#attach-and-deattach-partitions","title":"Attach and DeAttach Partitions","text":"<pre><code>create table employees_list_sp partition of employee_list\n for values in ('SP');\n\ninsert into employee_list (birth_date, country_code) values ('2001-01-01','SP');\n\ncreate table employees_list_in partition of employee_list\n for values in ('IN');\n\ncreate table employees_list_default partition of employee_list default\n\nALTER TABLE employee_list detach partition employees_list_in;\n</code></pre>"},{"location":"Databases/sql/6-Advance-tables/paritioning-tables/#altering-partitions","title":"Altering Partitions","text":"<pre><code>create table t1\n(\n    a int,\n    b int\n) partition by range (a);\n\ncreate table t1p1 partition of t1 for values from (0) to (1000);\ncreate table t1p2 partition of t1 for values from (2000) to (3000);\n\ninsert into t1 (a, b)\nvalues (1, 1);\n\n-- detach\n-- alter\n-- attach\n\n\nBEGIN TRANSACTION ;\nALTER TABLE T1 DETACH PARTITION t1p1;\nALTER TABLE t1 attach partition t1p1 for values FROM (0) to (200);\ncommit TRANSACTION;\n</code></pre>"},{"location":"Databases/sql/6-Advance-tables/paritioning-tables/#partition-indexes","title":"Partition Indexes","text":"<ul> <li>Creating an index on the master/parent table will automatically create same indexes to every attached table partition</li> <li>PostgreSQL doesnt allow a way to create a single index covering every partition of the parent table. You have to create indexes for each table</li> </ul> <pre><code>create unique index idx_u_employee_list_id_country_code on employee_list\n    (id); -- error\n\ncreate unique index idx_u_employee_list_id_country_code on employee_list\n    (id,country_code); -- with paritition key\n</code></pre>"},{"location":"Databases/sql/6-Advance-tables/pivotal-or-crosstab-tables/","title":"Pivotal or Crosstab Tables","text":"<p>something about cross tab</p> <pre><code>CREATE EXTENSION IF NOT EXISTS tablefunc;\n\nCREATE TABLE ct(id SERIAL, rowid TEXT, attribute TEXT, value TEXT);\nINSERT INTO ct(rowid, attribute, value) VALUES('test1','att1','val1');\nINSERT INTO ct(rowid, attribute, value) VALUES('test1','att2','val2');\nINSERT INTO ct(rowid, attribute, value) VALUES('test1','att3','val3');\nINSERT INTO ct(rowid, attribute, value) VALUES('test1','att4','val4');\nINSERT INTO ct(rowid, attribute, value) VALUES('test2','att1','val5');\nINSERT INTO ct(rowid, attribute, value) VALUES('test2','att2','val6');\nINSERT INTO ct(rowid, attribute, value) VALUES('test2','att3','val7');\nINSERT INTO ct(rowid, attribute, value) VALUES('test2','att4','val8');\n\nSELECT * FROM ct;\n\n\nSELECT *\nFROM crosstab(\n  'select rowid, attribute, value\n   from ct\n   where attribute = ''att2'' or attribute = ''att3''\n   order by 1,2')\nAS ct(row_name text, category_1 text, category_2 text);\n\n\nSELECT * FROM crosstab (\n    '\n        SELECT location, year, SUM(raindays)::int\n        FROM rainfalls\n        GROUP BY \n            location,\n            year\n        ORDER BY\n            location,\n            year\n    '\n) AS ct \n(\n    \"LOCATION\" TEXT,\n    \"2012\" INT,\n    \"2013\" INT,\n    \"2014\" INT,\n    \"2015\" INT,\n    \"2016\" INT,\n    \"2017\" INT\n)\n</code></pre>"},{"location":"Databases/sql/7%20Joins/","title":"Joins","text":"<ul> <li>Cross &amp; Natural Joins</li> <li>Full, Multiple &amp; Self Join</li> <li>Inner Join</li> <li>Left &amp; Right Join</li> </ul>"},{"location":"Databases/sql/7%20Joins/cross-join/","title":"Cross &amp; Natural Joins","text":""},{"location":"Databases/sql/7%20Joins/cross-join/#cross-join","title":"Cross Join","text":"<pre><code>-- every row in the left table will match with every row \n-- in the right table in cross join\n\nselect *\nfrom left_product\n         cross join right_product;\n\n product_id | product_name | product_id | product_name \n------------+--------------+------------+--------------\n          1 | a            |          1 | a\n          1 | a            |          2 | B\n          1 | a            |          3 | C\n          1 | a            |          4 | d\n          1 | a            |          7 | E1\n          2 | B            |          1 | a\n          2 | B            |          2 | B\n          2 | B            |          3 | C\n          2 | B            |          4 | d\n          2 | B            |          7 | E1\n          3 | C            |          1 | a\n          3 | C            |          2 | B\n          3 | C            |          3 | C\n          3 | C            |          4 | d\n          3 | C            |          7 | E1\n          5 | E            |          1 | a\n          5 | E            |          2 | B\n          5 | E            |          3 | C\n          5 | E            |          4 | d\n          5 | E            |          7 | E1\n\n\n-- short hand notation for cross join\nselect * from left_product , right_product;\n-- same output as above\n\n-- using inner join to preform query like cross join\n-- using ON TRUE at the end\nselect *\nfrom left_product\n         inner join right_product on true;\n-- same output as above\n</code></pre>"},{"location":"Databases/sql/7%20Joins/cross-join/#natural-join","title":"Natural Join","text":""},{"location":"Databases/sql/7%20Joins/cross-join/#loading-sample-data","title":"Loading sample data","text":"<pre><code>DROP TABLE IF EXISTS c1;\nCREATE TABLE c1\n(\n    category_id   serial PRIMARY KEY,\n    category_name VARCHAR(255) NOT NULL\n);\n\nDROP TABLE IF EXISTS p1;\nCREATE TABLE p1\n(\n    product_id   serial PRIMARY KEY,\n    product_name VARCHAR(255) NOT NULL,\n    category_id  INT          NOT NULL,\n    FOREIGN KEY (category_id) REFERENCES c1 (category_id)\n);\n\nINSERT INTO c1 (category_name)\nVALUES ('Smart Phone'),\n       ('Laptop'),\n       ('Tablet');\n\nINSERT INTO p1 (product_name, category_id)\nVALUES ('iPhone', 1),\n       ('Samsung Galaxy', 1),\n       ('HP Elite', 2),\n       ('Lenovo Think pad', 2),\n       ('iPad', 3),\n       ('Kindle Fire', 3);\n</code></pre>"},{"location":"Databases/sql/7%20Joins/cross-join/#queries","title":"Queries","text":"<pre><code>SELECT *\nFROM p1\n         NATURAL JOIN c1;\n\n category_id | product_id |   product_name   | category_name \n-------------+------------+------------------+---------------\n           1 |          1 | iPhone           | Smart Phone\n           1 |          2 | Samsung Galaxy   | Smart Phone\n           2 |          3 | HP Elite         | Laptop\n           2 |          4 | Lenovo Think pad | Laptop\n           3 |          5 | iPad             | Tablet\n           3 |          6 | Kindle Fire      | Tablet\n\n--same query using inner join\nSELECT *\nFROM p1\n         INNER JOIN c1 USING (category_id);\n-- same output as above\n</code></pre>"},{"location":"Databases/sql/7%20Joins/full-multiple-and-self-joins/","title":"Full, Multiple &amp; Self Joins","text":""},{"location":"Databases/sql/7%20Joins/full-multiple-and-self-joins/#full-join","title":"Full Join","text":"<pre><code>select *\nfrom right_product\n  full join \n    left_product\n  on \n    right_product.product_id = left_poduct.product_id;\n\n-- gets result from tables o both side\n-- those entries that match the each other are in one row\n-- else they are present in seperate row in table\n\n product_id | product_name | product_id | product_name \n------------+--------------+------------+--------------\n          1 | a            |          1 | a\n          2 | B            |          2 | B\n          3 | C            |          3 | C\n          4 | d            |            | \n          7 | E1           |            | \n            |              |          5 | E\n\n\nselect dir.first_name || ' ' || dir.last_name\n           as \"name\",\n       mv.movie_name\nfrom directors dir\n         full join movies mv \n         on mv.director_id = dir.director_id\n\n      name       |       movie_name       \n-----------------+------------------------\n Tomas Alfredson | Let the Right One In\n Paul Anderson   | There Will Be Blood\n Wes Anderson    | The Darjeeling Limited\n Wes Anderson    | Rushmore\n Wes Anderson    | Grand Budapest Hotel\n</code></pre>"},{"location":"Databases/sql/7%20Joins/full-multiple-and-self-joins/#multiple-join","title":"Multiple Join","text":"<pre><code>select r.movie_id, movie_name,\n       dir.first_name || ' ' || dir.last_name \n         as \"dir name\"\nfrom movies mv\n         JOIN movies_revenues r\n             on r.movie_id = mv.movie_id\n         JOIN directors dir\n             on dir.director_id = mv.director_id\nlimit 5;\n\n movie_id |     movie_name     |     dir name     \n----------+--------------------+------------------\n       45 | The Wizard of Oz   | Victor Fleming\n       13 | Gone with the Wind | Victor Fleming\n       23 | Mary Poppins       | Robert Stevenson\n       44 | The Sound of Music | Robert Wise\n\n-- same result even after re-arranging\nselect r.movie_id,\n       movie_name,\n       dir.first_name || ' ' || dir.last_name \n         as \"dir name\"\nfrom movies mv\n         JOIN directors dir\n              on dir.director_id = mv.director_id\n         JOIN movies_revenues r\n              on r.movie_id = mv.movie_id\nlimit 5;\n</code></pre>"},{"location":"Databases/sql/7%20Joins/full-multiple-and-self-joins/#self-join","title":"Self Join","text":"<pre><code>select * \nfrom left_product t1 \nINNER JOIN left_product t2 \nON t1.product_id = t2.product_id;\n\n product_id | product_name | product_id | product_name \n------------+--------------+------------+--------------\n          1 | a            |          1 | a\n          2 | B            |          2 | B\n          3 | C            |          3 | C\n          5 | E            |          5 | E\n</code></pre>"},{"location":"Databases/sql/7%20Joins/inner-join/","title":"Inner Join","text":""},{"location":"Databases/sql/7%20Joins/inner-join/#basic","title":"Basic","text":"<p>Remember : All common column defined at ON must match values on both tables</p> <pre><code>-- syntax\n\nSELECT\n table_a.column1\n table_b.column2\nFROM\n table_a\nINNER JOIN table_b ON table1.column1 = table2.column2\n\n\nSELECT mv.*,\n       dir.*\nFROM movies as mv\n         INNER JOIN directors as dir\n                    ON mv.director_id = dir.director_id\nLIMIT 5;\n\n-- director_id column will be repeated\n\n movie_id |       movie_name       | movie_length | movie_lang | release_date | age_certificate | director_id | director_id | first_name | last_name | date_of_birth | nationality \n----------+------------------------+--------------+------------+--------------+-----------------+-------------+-------------+------------+-----------+---------------+-------------\n       20 | Let the Right One In   |          128 | Swedish    | 2008-10-24   | 15              |           1 |           1 | Tomas      | Alfredson | 1965-04-01    | Swedish\n       46 | There Will Be Blood    |          168 | English    | 2007-12-26   | 15              |           2 |           2 | Paul       | Anderson  | 1970-06-26    | American\n       40 | The Darjeeling Limited |          119 | English    | 2007-09-29   | PG              |           3 |           3 | Wes        | Anderson  | 1969-05-01    | American\n       30 | Rushmore               |          104 | English    | 1998-11-12   | 12              |           3 |           3 | Wes        | Anderson  | 1969-05-01    | American\n       15 | Grand Budapest Hotel   |          117 | English    | 2014-07-03   | PG              |           3 |           3 | Wes        | Anderson  | 1969-05-01    | American\n</code></pre>"},{"location":"Databases/sql/7%20Joins/inner-join/#selecting-only-required-columns","title":"Selecting only required columns","text":"<pre><code>SELECT movie_name,\n       dir.first_name || ' ' || dir.last_name \n           as \"Director Name\",\n       mv.movie_id,\n       dir.director_id\nFROM movies as mv\n         INNER JOIN directors as dir \n             ON mv.director_id = dir.director_id\nLIMIT 5;\n\n       movie_name       |  Director Name  | movie_id | director_id \n------------------------+-----------------+----------+-------------\n Let the Right One In   | Tomas Alfredson |       20 |           1\n There Will Be Blood    | Paul Anderson   |       46 |           2\n The Darjeeling Limited | Wes Anderson    |       40 |           3\n Rushmore               | Wes Anderson    |       30 |           3\n Grand Budapest Hotel   | Wes Anderson    |       15 |           3\n</code></pre>"},{"location":"Databases/sql/7%20Joins/inner-join/#with-where-clause","title":"with WHERE clause","text":"<pre><code>SELECT movie_name,\n       mv.movie_lang,\n       dir.first_name || ' ' || dir.last_name \n          as \"Director Name\",\n       mv.movie_id,\n       dir.director_id\nFROM movies as mv\n         INNER JOIN directors as dir\n            ON mv.director_id = dir.director_id\nWHERE mv.movie_lang = 'English' limit 5;\n\n       movie_name       | movie_lang | Director Name  | movie_id | director_id \n------------------------+------------+----------------+----------+-------------\n There Will Be Blood    | English    | Paul Anderson  |       46 |           2\n The Darjeeling Limited | English    | Wes Anderson   |       40 |           3\n Rushmore               | English    | Wes Anderson   |       30 |           3\n Grand Budapest Hotel   | English    | Wes Anderson   |       15 |           3\n Submarine              | English    | Richard Ayoade |       38 |           4\n</code></pre>"},{"location":"Databases/sql/7%20Joins/inner-join/#inner-join-with-using","title":"Inner Join with USING","text":"<ul> <li>we use USING  only when joining tables have the SAME column name, rather then ON !</li> </ul> <pre><code>select\n    table1.column1,\n    table2.column1\nfrom\n    table1\nINNER JOIN \n    table2 USING (column1);\n\n\nselect *\nfrom movies\n         INNER JOIN directors USING (director_id)\nLIMIT 10;\n\n director_id | movie_id |     movie_name     | movie_length | movie_lang | release_date | age_certificate | first_name |  last_name   | date_of_birth | nationality \n-------------+----------+--------------------+--------------+------------+--------------+-----------------+------------+--------------+---------------+-------------\n          13 |        1 | A Clockwork Orange |          112 | English    | 1972-02-02   | 18              | Stanley    | Kubrick      | 1928-07-26    | American\n           9 |        2 | Apocalypse Now     |          168 | English    | 1979-08-15   | 15              | Francis    | Ford Coppola | 1939-04-07    | American\n</code></pre>"},{"location":"Databases/sql/7%20Joins/left-and-right-join/","title":"Left and Right JOIN","text":""},{"location":"Databases/sql/7%20Joins/left-and-right-join/#left-join","title":"Left Join","text":""},{"location":"Databases/sql/7%20Joins/left-and-right-join/#basic","title":"Basic","text":"<ul> <li>Returns every row from LEFT table plus rows that match values in the joined column from the RIGHT table</li> </ul>"},{"location":"Databases/sql/7%20Joins/left-and-right-join/#syntax","title":"Syntax","text":"<pre><code>SELECT table1.column1,\n       table2.column1\nFROM table1\n         LEFT JOIN\n     table2\n     ON\n         table1.column1 = table2.column2\n</code></pre>"},{"location":"Databases/sql/7%20Joins/left-and-right-join/#loading-sample-data","title":"Loading Sample Data","text":"<pre><code>create table left_product\n(\n    product_id   INT PRIMARY KEY,\n    product_name VARCHAR(100)\n);\n\nCREATE TABLE right_product\n(\n    product_id   INT PRIMARY KEY,\n    product_name VARCHAR(100)\n);\n\nINSERT INTO left_product ( PRODUCT_ID, PRODUCT_NAME )\n    VALUES (1,'a'),('2','B'),('3','C'),('5','E');\n\nINSERT INTO right_product ( PRODUCT_ID, PRODUCT_NAME )\n    VALUES (1,'a'),('2','B'),('3','C'),('4','d'),(7,'E1');\n</code></pre> <pre><code>-- left join selects all data in the left data and\n-- picks all data in the right table matching the condition\n\nSELECT\n    *\nfrom left_product\nleft join\n    right_product \n    on left_product.product_id = right_product.product_id;\n\n product_id | product_name | product_id | product_name \n------------+--------------+------------+--------------\n          1 | a            |          1 | a\n          2 | B            |          2 | B\n          3 | C            |          3 | C\n          5 | E            |-- no match | -- no match\n\n-- select name from director table\n-- and select movie from another table\n-- where director.id matches director id in movies table\n\nselect\n    dir.first_name,\n    dir.last_name,\n    mv.movie_name\nfrom\n     directors dir\nleft join movies mv\n    ON mv.director_id = dir.director_id\nlimit 5;\n\n first_name | last_name |       movie_name       \n------------+-----------+------------------------\n Tomas      | Alfredson | Let the Right One In\n Paul       | Anderson  | There Will Be Blood\n Wes        | Anderson  | The Darjeeling Limited\n Wes        | Anderson  | Rushmore\n Wes        | Anderson  | Grand Budapest Hotel\n\n-- same above query with where clause\n\nselect dir.first_name || ' ' || dir.last_name\n           as \"Directors Name\",\n       mv.movie_name,\n       mv.movie_lang\nfrom directors dir\n         left join movies mv\n                   ON mv.director_id = dir.director_id\nwhere mv.movie_lang in ('English', 'Chinese');\n</code></pre>"},{"location":"Databases/sql/7%20Joins/left-and-right-join/#right-join","title":"Right Join","text":""},{"location":"Databases/sql/7%20Joins/left-and-right-join/#syntax_1","title":"Syntax","text":"<pre><code>SELECT \n    table1.column1,\n    table2.column2\nFROM\n    table1\nRIGHT JOIN\n    table2 \n    ON table1.column1 = table2.column2;\n</code></pre>"},{"location":"Databases/sql/7%20Joins/left-and-right-join/#load-sample-data","title":"Load Sample Data","text":"<pre><code>CREATE TABLE films\n(\n    film_id SERIAL PRIMARY KEY,\n    title   varchar(255) NOT NULL\n);\n\nINSERT INTO films(title)\nVALUES ('Joker'),\n       ('Avengers: Endgame'),\n       ('Parasite');\n\nCREATE TABLE film_reviews\n(\n    review_id SERIAL PRIMARY KEY,\n    film_id   INT,\n    review    VARCHAR(255) NOT NULL\n);\n\nINSERT INTO film_reviews(film_id, review)\nVALUES (1, 'Excellent'),\n       (1, 'Awesome'),\n       (2, 'Cool'),\n       (NULL, 'Beautiful');\n</code></pre> <pre><code>-- contains products from left table \n-- that are in right table\n-- and all rows from right table that do not\n-- match anything from left\n\nSELECT\n    *\nfrom left_product\nright join\n    right_product\n    on left_product.product_id = right_product.product_id;\n\n product_id | product_name | product_id | product_name \n------------+--------------+------------+--------------\n          1 | a            |          1 | a\n          2 | B            |          2 | B\n          3 | C            |          3 | C\n            |              |          4 | d\n            |              |          7 | E1\n\n\nSELECT review, title\nFROM films\nRIGHT JOIN film_reviews using (film_id)\nWHERE title IS NULL;\n\n  review   | title \n-----------+-------\n Beautiful | \n</code></pre>"},{"location":"Databases/sql/8%20Functions/","title":"Functions","text":""},{"location":"Databases/sql/8%20Functions/#basic-functions","title":"Basic Functions","text":""},{"location":"Databases/sql/8%20Functions/#syntax","title":"Syntax","text":"<pre><code>CREATE OR REPLACE FUNCTION function_name() RETURNS return_type as \n'\n    -- SQL COMMAND \n' LANGUAGE SQL;\n</code></pre>"},{"location":"Databases/sql/8%20Functions/#some-examples","title":"Some Examples","text":"<pre><code>-- Function to Add\n\nCREATE OR REPLACE FUNCTION fn_my_sum( int, int ) RETURNS int as \n'\n    SELECT $1 + $2;\n' LANGUAGE SQL;\n\n-- function call\n\nSELECT fn_my_sum(1,2);\n\n-- output\n\n fn_my_sum \n-----------\n         3\n(1 row)\n\n--------------------------------\n\n-- Function to printer\n\nCREATE OR REPLACE FUNCTION fn_printer( text ) RETURNS text as \n$$\n    SELECT 'Hello ' || $1 ;\n$$ LANGUAGE SQL;\n\n--function call\n\nSELECT fn_printer( 'Uday' );\n\n-- output\n\n fn_printer \n------------\n Hello Uday\n(1 row)\n\n-- Another syntax\n\nCREATE OR REPLACE FUNCTION fn_printer( text ) RETURNS text as \n$body$\n    SELECT 'Hello ' || $1 ;\n$body$ \nLANGUAGE SQL;\n\n-- function call\n\nSELECT fn_printer( 'Uday' );\n\n-- output\n fn_printer \n------------\n Hello Uday\n(1 row)\n</code></pre>"},{"location":"Databases/sql/8%20Functions/#functions-with-dml","title":"Functions with DML","text":"<pre><code>-- function\n\nCREATE OR REPLACE FUNCTION fn_employee_update_country () returns void AS\n$$\n\n    update employees \n    set country = 'n/a'\n    where country is NULL\n$$ \nLANGUAGE SQL\n\nSELECT fn_employee_update_country();\n</code></pre>"},{"location":"Databases/sql/8%20Functions/#function-with-dql","title":"Function with DQL","text":"<pre><code>CREATE OR REPLACE FUNCTION fn_api_order_latest() RETURNS orders AS\n$$\n\n    select *\n    from orders\n    order by order_date DESC\n    limit 1\n\n$$\nLANGUAGE SQL;\n\nselect (fn_api_order_latest()).*;\n\nselect (fn_api_order_latest()).order_date;\n\nselect order_date(fn_api_order_latest())\n\n-----------------------------\n\nCREATE OR REPLACE FUNCTION fn_employee_hire_bydate( p_year integer ) returns setof employees as \n$$\n\n    select * from employees\n    where extract('YEAR' from hire_date) = p_year\n\n$$\nLANGUAGE SQL\n\nSELECT (fn_employee_hire_bydate('1992')).*;\n</code></pre>"},{"location":"Databases/sql/8%20Functions/#returning-table-from-function","title":"Returning table from function","text":"<pre><code>CREATE OR REPLACE FUNCTION fn_orders()\n    returns table\n            (\n                order_id SMALLINT,\n                employee_id SMALLINT\n            )\n    as\n    $$\n\n    select order_id, employee_id\n    from orders;\n\n    $$\nLANGUAGE SQL;\n\nSELECT (fn_orders()).*;\n</code></pre>"},{"location":"Databases/sql/8%20Functions/#function-with-defualt-parameters","title":"Function with Defualt parameters","text":"<pre><code>CREATE OR REPLACE FUNCTION function_name \n    ( x int default 0, y int DEFAULT 10 ) returns int as\n    $$ \n\n    select x+y;\n\n    $$\nLANGUAGE SQL;\n\nSELECT function_name();\n</code></pre>"},{"location":"Databases/sql/8%20Functions/#dropping-function","title":"Dropping Function","text":"<pre><code>DROP FUNCTION [ IF EXISTS ] function_name \n    ( argument_list ) ( cascade | restrict );\n</code></pre>"},{"location":"Databases/sql/8%20Functions/cursors/","title":"Cursors","text":"<ul> <li>Rows returned by the SQL query are those which match the condition. It can either be zero or more at a time.</li> <li>Sometimes you need to traverse through the rows one by one, forward or backwards</li> <li>Life Cycle of Cursor</li> <li>DECLARE </li> <li>OPEN</li> <li>FETCH</li> <li>CLOSE</li> <li>Cursor enable SQL to retrieve ( or update, or delete ) a single row at a time.</li> <li>Cursor needs to be created in </li> </ul> <pre><code>DECLARE cur_al_movie refcursor;\n\n-- or\n\ncursor-name [cursor-scrollability] cursor [(name datatype ...)]\nFOR\n    query-expression\n</code></pre> <ul> <li><code>cursor-scrollability</code> :  <code>SCROLL</code> OR <code>NO SCROLL</code>, <code>NO SCROLL</code> mean the cursor cannot scrol backward.</li> <li><code>query-expression</code> : You can use any legal SELECT statement as a query expression. The result set rows are considered as scope of the cursor.</li> </ul>"},{"location":"Databases/sql/8%20Functions/cursors/#example","title":"Example","text":"<pre><code>DECLARE cur_all_movies CURSOR\n    FOR\n        SELECT movie_name, movie_length FROM movies;\n</code></pre>"},{"location":"Databases/sql/8%20Functions/cursors/#cursor-with-parameters","title":"Cursor with Parameters","text":"<pre><code>DECLARE cur_all_movies_by_year CURSOR ( custom_year integer )\nFOR\n    SELECT  \n        *\n    FROM movies\n    WHERE EXTRACT ('YEAR' FROM release_date ) = custom_year\n</code></pre>"},{"location":"Databases/sql/8%20Functions/cursors/#opening-a-cursor","title":"Opening a cursor","text":"<ul> <li>Opening an unbound cursor</li> </ul> <pre><code>OPEN unbound_cursor_variable [[NO] SCROLL] FOR query;\n</code></pre>"},{"location":"Databases/sql/8%20Functions/cursors/#opening-un-bound-cursor","title":"Opening un bound cursor","text":"<pre><code>OPEN cur_directors_us\nFOR\n    SELECT\n        first_name,\n        last_name,\n        date_of_birth\n    FROM\n        directors\n    WHERE\n        nationality = 'American'\n</code></pre>"},{"location":"Databases/sql/8%20Functions/cursors/#opening-an-un-bound-cursor-with-dynamic-query","title":"Opening an un bound cursor with dynamic query","text":"<pre><code>OPEN unbound_cursor_variable [[NO] SCROLL]\nFOR EXECUTE\n    query-expression [using expression [,...]];\n</code></pre> <pre><code>select * from movies order by movie_name;\n\nDO\n$$\n\n    DECLARE\n        output_text text default '';\n        rec_movie record;\n\n        cur_all_movies CURSOR\n        FOR\n            SELECT * FROM movies;\n\n    BEGIN\n\n        OPEN cur_all_movies;\n\n        LOOP\n\n            FETCH cur_all_movies into rec_movie;\n                EXIT WHEN NOT FOUND;\n\n            output_text := output_text || ',' || rec_movie.movie_name;\n\n        END LOOP;\n\n        RAISE NOTICE 'ALL MOVIES NAME %' , output_text;\n\n    END;\n\n$$\n</code></pre>"},{"location":"Databases/sql/8%20Functions/pl-pgsql/","title":"PL/pgSQL","text":""},{"location":"Databases/sql/8%20Functions/pl-pgsql/#declaring-variables","title":"Declaring Variables","text":"<pre><code>DO\n$$\n    DECLARE\n        mynum      integer     := 89;\n        first_name varchar(20) := 'Uday';\n        hire_date  date        := '2020-01-01';\n        start_time timestamp   := NOW();\n        emptyvar   integer;\n\n    BEGIN\n        RAISE NOTICE 'My variable % % % % %',\n        mynum, first_name, hire_date,\n        start_time, emptyvar ;\n    END;\n$$\n</code></pre>"},{"location":"Databases/sql/8%20Functions/pl-pgsql/#parameters-to-function","title":"Parameters to Function","text":"<pre><code>CREATE OR REPLACE FUNCTION function_name \n    (INT, INT) RETURNS INT as\n$$\n    DECLARE\n        x alias for $1;\n        y alias for $2;\n\n    BEGIN\n        --\n    END;\n$$\n</code></pre>"},{"location":"Databases/sql/8%20Functions/pl-pgsql/#assigning-value-from-result-into-variable","title":"Assigning value from result into variable","text":"<pre><code>DO\n$$\n    DECLARE \n        product_title products.product_name%TYPE;\n    BEGIN\n        SELECT product_name FROM products \n        INTO product_title\n        where product_id = 1 limit 1;\n\n        RAISE NOTICE 'Your product name is %', product_title;\n    END;\n$$        \n</code></pre>"},{"location":"Databases/sql/8%20Functions/pl-pgsql/#function-parameter-wt-in-and-out","title":"Function Parameter w/t IN and OUT","text":"<pre><code>CREATE OR REPLACE FUNCTION fn_sum_using_inout \n    ( IN x integer, IN y integer, OUT Z integer ) as \n$$\n\n    BEGIN\n        z := x+y;\n    END;\n\n$$\nLANGUAGE PLPGSQL;\n\nselect fn_sum_using_inout(2,3);\n\n-- another example\n\nCREATE OR REPLACE FUNCTION fn_sum_using_inouts \n    ( IN x integer, IN y integer, OUT Z integer, OUT w integer ) as \n$$\n\n    BEGIN\n        z := x+y;\n        w := x*y;\n    END;\n\n$$\nLANGUAGE PLPGSQL;\n\nselect * from fn_sum_using_inouts(2,3);\n</code></pre>"},{"location":"Databases/sql/8%20Functions/pl-pgsql/#nested-functions","title":"Nested functions","text":"<pre><code>DO\n$$\n    &lt;&lt; Parent &gt;&gt;\n\n    DECLARE\n        counter integer := 0;\n    BEGIN\n        counter := counter+1;\n        RAISE NOTICE 'the current value of counter (IN PARENT) is %', counter;\n\n            DECLARE \n                counter integer := 0;\n            BEGIN\n                counter := counter + 5;\n                RAISE NOTICE 'The current value of counter at subblocks is %', counter;\n                RAISE NOTICE 'The parent value of counter at subblocks is %', PARENT.counter;\n            END;\n\n        counter := counter + 5;\n        RAISE NOTICE 'the current value of counter (IN PARENT) is %', counter;\n\n    END;\n$$\nLANGUAGE PLPGSQL;\n</code></pre>"},{"location":"Databases/sql/8%20Functions/pl-pgsql/#returning-resultset-from-function","title":"Returning ResultSet from function","text":"<pre><code>CREATE OR REPLACE FUNCTION fn_order_by_date_pro() RETURNS SETOF orders AS\n$$\n    BEGIN\n        RETURN QUERY SELECT * FROM orders limit 10;\n    END;\n$$\nLANGUAGE PLPGSQL;\n\nSELECT * FROM fn_order_by_date_pro();\n</code></pre>"},{"location":"Databases/sql/8%20Functions/pl-pgsql/#conditional-statement-inside-functions","title":"Conditional Statement inside functions","text":""},{"location":"Databases/sql/8%20Functions/pl-pgsql/#default-parameters","title":"Default Parameters","text":"<pre><code>CREATE OR REPLACE FUNCTION fn_which_is_greater\n    ( x integer default 0, y integer default 0 ) RETURNS text AS\n$$\n    BEGIN\n        IF x &gt; y then \n            return ' x &gt; y ';\n        else \n            return ' x &lt; y ';\n        end if ;\n    END;\n$$ LANGUAGE PLPGSQL;\n\nSELECT  fn_which_is_greater(4,3);\n</code></pre>"},{"location":"Databases/sql/8%20Functions/pl-pgsql/#switch-case-example","title":"Switch Case Example","text":"<pre><code>CREATE OR REPLACE FUNCTION fn_checker \n    ( x integer default 0 ) RETURNS text AS\n$$\n    BEGIN\n        CASE x\n            when 10 then\n                return 'value = 10';\n            when 20 then\n                return 'value = 20';\n            else\n                RETURN 'MORE';\n        END CASE;\n    END;\n$$ LANGUAGE PLPGSQL;\n\nSELECT fn_checker(30);\n</code></pre>"},{"location":"Databases/sql/8%20Functions/pl-pgsql/#loops-in-plpgsql","title":"Loops in PLPGSQL","text":"<pre><code>DO\n$$\n    DECLARE \n        i_counter integer = 0;\n    BEGIN\n        LOOP\n            RAISE NOTICE '%', i_counter;\n            i_counter := i_counter+1;\n            EXIT WHEN\n                i_counter = 5;\n        END LOOP;\n    END;\n$$ LANGUAGE PLPGSQL;\n</code></pre>"},{"location":"Databases/sql/8%20Functions/pl-pgsql/#loops-in-range-exaple","title":"Loops in range exaple","text":"<pre><code>DO\n$$\n    BEGIN\n        FOR counter IN 1..5 BY 1\n        LOOP\n\n            RAISE NOTICE 'COUNTER : %', counter;\n        END LOOP;        \n    END;\n$$\nLANGUAGE PLPGSQL;\n</code></pre>"},{"location":"Databases/sql/8%20Functions/pl-pgsql/#reverse-loops","title":"Reverse Loops","text":"<pre><code>DO\n$$\n    BEGIN\n        FOR counter IN REVERSE 5..1 BY 1\n        LOOP\n            RAISE NOTICE 'COUNTER : %', counter;\n        END LOOP;        \n    END;\n$$\nLANGUAGE PLPGSQL;\n</code></pre>"},{"location":"Databases/sql/8%20Functions/pl-pgsql/#iterating-over-result-set","title":"Iterating over result set","text":"<pre><code>DO\n$$\n    DECLARE \n        rec record ;\n    BEGIN\n        FOR rec in \n            select order_id, customer_id from orders LIMIT 10\n        LOOP\n            RAISE NOTICE '% %', rec.order_id, rec.customer_id;\n        END LOOP; \n    END;\n$$ LANGUAGE PLPGSQL\n</code></pre>"},{"location":"Databases/sql/8%20Functions/pl-pgsql/#loop-with-exit-condition","title":"Loop with Exit condition","text":"<pre><code>DO\n$$\n    DECLARE \n        i_counter int = 0;\n    BEGIN\n        LOOP\n            i_counter = i_counter + 1;\n        EXIT WHEN\n            i_counter &gt; 20;\n        CONTINUE\n            WHEN MOD (i_counter,2) = 0;\n\n        RAISE NOTICE 'COUNTER : %', i_counter;\n        END LOOP;\n    END;\n$$ LANGUAGE PLPGSQL;\n</code></pre>"},{"location":"Databases/sql/8%20Functions/pl-pgsql/#declaring-arrays-in-plpgsql","title":"Declaring arrays in PLPGSQL","text":"<pre><code>DO\n$$\n    DECLARE \n        arr1 int[] := array[1,2,3];\n        arr2 int[] := array[4,5,6,7,8];\n        var int;\n    BEGIN\n        FOREACH var IN ARRAY arr1||ARR2\n        LOOP\n            RAISE NOTICE '%', var;\n        END LOOP;\n    END;\n$$ LANGUAGE PLPGSQL;\n</code></pre>"},{"location":"Databases/sql/8%20Functions/pl-pgsql/#while-loop-in-plpgsql","title":"While Loop in PLPGSQL","text":"<pre><code>CREATE OR REPLACE FUNCTION fn_while_loop_sum_all(x integer) \n    returns numeric as\n$$\n    DECLARE \n        counter integer := 1;\n        sum_all integer := 0;\n    BEGIN\n        WHILE counter &lt;= x\n        LOOP\n            sum_all := sum_all + counter;\n            counter := counter + 1;\n        END LOOP;\n        return sum_all;\n    END;\n$$ language plpgsql\n\nselect fn_while_loop_sum_all(4);\n</code></pre>"},{"location":"Databases/sql/8%20Functions/pl-pgsql/#returning-specific-query-from-column","title":"Returning specific Query from column","text":"<pre><code>CREATE OR REPLACE FUNCTION fn_api_products_by_names(p_pattern varchar)\nRETURNS TABLE ( productname varchar, unitprice real )\nAS\n$$\n    BEGIN\n        RETURN QUERY\n            SELECT product_name,unit_price from products \n            where product_name like p_pattern;\n\n    END;\n$$ LANGUAGE PLPGSQL;\n\nSELECT * FROM fn_api_products_by_names('A%');\n</code></pre> <pre><code>CREATE OR REPLACE FUNCTION fn_all_orders_greater() RETURNS SETOF order_details as \n$$\n    DECLARE\n        r record;\n    BEGIN\n        for r in\n            select * from order_details where unit_price &gt; 100\n        loop\n            return next r;\n        end loop;\n        return;\n    end;\n$$ language plpgsql;\n\nselect * from fn_all_orders_greater();\n</code></pre>"},{"location":"Databases/sql/8%20Functions/pl-pgsql/#if-data-not-found-condition","title":"If data not found condition","text":"<pre><code>DO\n$$\n    DECLARE \n        rec record;\n        orderid smallint = 1;\n    BEGIN\n        SELECT customer_id, order_date\n        FROM orders \n        INTO STRICT rec\n        WHERE order_id = orderid;\n\n        EXCEPTION\n            WHEN NO_DATA_FOUND THEN\n                RAISE EXCEPTION 'No order id was found';\n\n    END;\n$$ LANGUAGE PLPGSQL;    \n</code></pre>"},{"location":"Databases/sql/8%20Functions/pl-pgsql/#throwing-execption-on-condition","title":"Throwing execption on condition","text":"<pre><code>DO\n$$\n\n    DECLARE \n        rec record;\n        orderid smallint = 1;\n    BEGIN\n        SELECT customer_id, order_date\n        FROM orders \n        INTO STRICT rec\n        WHERE order_id &gt; 1000;\n\n        EXCEPTION\n            WHEN TOO_MANY_ROWS THEN\n                RAISE EXCEPTION 'Too many rows were found';\n\n    END;\n$$ LANGUAGE PLPGSQL;  \n</code></pre>"},{"location":"Databases/sql/8%20Functions/pl-pgsql/#throwing-execption-example","title":"Throwing execption example","text":"<pre><code>CREATE OR REPLACE FUNCTION fn_div_exception (x real, y real) RETURNS real as \n$$\n\n    DECLARE \n        ret real;\n    BEGIN\n        ret := x / y;\n        return ret;\n    EXCEPTION\n        WHEN division_by_zero then\n            RAISE INFO 'division by zero error';\n            RAISE INFO 'ERROR % %', SQLSTATE, SQLERRM;\n    END;\n$$ LANGUAGE PLPGSQL;\n\nSELECT fn_div_exception(5,0);\n</code></pre>"},{"location":"Databases/sql/8%20Functions/stored-procedures/","title":"Stored Procedures","text":"<ul> <li> <p>They are compiled objects</p> </li> <li> <p>The procedure allows SELECT as well as DML(INSERT/UPDATE/DELETE) statement in it whereas Function allows only SELECT statement in it.</p> </li> <li> <p>Procedures cannot be utilized in a SELECT statement whereas Function can be embedded in a SELECT statement.</p> </li> <li> <p>Stored Procedures cannot be used in the SQL statements anywhere in the WHERE/HAVING/SELECT section whereas Function can be.</p> </li> <li> <p>Functions that return tables can be treated as another rowset. This can be used in JOINs with other tables.</p> </li> <li> <p>Inline Function can be though of as views that take parameters and can be used in JOINs and other Rowset operations.</p> </li> <li> <p>An exception can be handled by try-catch block in a Procedure whereas try-catch block cannot be used in a Function.</p> </li> <li> <p>We can use Transactions in Procedure whereas we can't use Transactions in Function.</p> </li> </ul>"},{"location":"Databases/sql/8%20Functions/stored-procedures/#sample-data","title":"Sample Data","text":"<pre><code>CREATE TABLE t_accounts (\n    recid SERIAL PRIMARY KEY,\n    name VARCHAR NOT NULL,\n    balance dec(15,2) NOT NULL\n);\n\ndrop table t_accounts;\n\nINSERT INTO t_accounts (name,balance) values ('Adam',100),('Linda',100);\n\nselect * from t_accounts;\n</code></pre>"},{"location":"Databases/sql/8%20Functions/stored-procedures/#creating-procedure","title":"Creating Procedure","text":"<pre><code>CREATE OR REPLACE PROCEDURE pr_money_transfer \n    (sender int, receiver int, amount dec) \nAS\n    $$\n\n        BEGIN\n\n            UPDATE t_accounts\n            SET balance = balance - amount\n            WHERE recid = sender;\n\n            UPDATE t_accounts\n            SET balance = balance + amount\n            WHERE recid = receiver;\n\n            COMMIT;\n\n        END;\n    $$\nLANGUAGE PLPGSQL;\n\nCALL pr_money_transfer(1,2,30);\n\nselect * from t_accounts;\n</code></pre>"},{"location":"Databases/sql/8%20Functions/stored-procedures/#another-example","title":"Another Example","text":"<pre><code>CREATE OR REPLACE PROCEDURE pr_orders_count(INOUT total_count INTEGER DEFAULT 0 ) AS\n$$\n    BEGIN\n\n        SELECT COUNT(*) INTO total_count FROM orders;\n\n    END;\n$$ LANGUAGE PLPGSQL;\n\nCALL pr_orders_count();\n</code></pre>"},{"location":"Databases/sql/8%20Functions/triggers/","title":"Triggers","text":"<ul> <li> <p>A postgresql trigger is a functoin invoked automatically whenever 'an event' associated with a table occurs.</p> </li> <li> <p>An event could be any of the following;</p> <ul> <li>INSERT</li> <li>UDPATE</li> <li>DELETE</li> <li>TRUNCATE</li> </ul> </li> <li> <p>You can associate a trigger with a</p> <ul> <li>Table</li> <li>View</li> <li>Foreign Table</li> </ul> </li> <li> <p>A trigger is a special 'user-defined function'</p> </li> <li>A trigger is automatically invoked</li> <li> <p>We can create a triger</p> <ul> <li>BEFORE<ul> <li>Trigger is fired before an event is about to happen</li> </ul> </li> <li>AFTER<ul> <li>Trigger is fired after the event is completed</li> </ul> </li> <li>INSTEAD<ul> <li>In case the event fails, trigger is fired</li> </ul> </li> </ul> </li> <li> <p>Cannot be fired manually</p> </li> <li>Fired in alphbetically order</li> <li>DO Not change in primary key, foriegn key or unique key column</li> <li>DO Not update records in the table that you normally read during the transaction</li> <li>DO Not read data from a table that is updating during the same transaction</li> <li>DO Not aggregate/summarized over the table that you are updating</li> </ul>"},{"location":"Databases/sql/8%20Functions/triggers/#types-of-triggers","title":"Types of Triggers","text":"<ul> <li> <p>Row level</p> <ul> <li>If row is marked for <code>FOR EACH ROW</code>, then trigger will be called for each row that is getting modfied by the event</li> </ul> </li> <li> <p>Statement level</p> <ul> <li>The <code>FOR EACH STATEMENT</code> will call the trigger function only ONCE for each statement, regardless of the number of rows getting modified.</li> </ul> </li> </ul> When Event Row-level Statement-level INSERT/UDPATE/DELETE Tables Tables and view before Truncate ---------- -------------------- --------- --------------- INSERT/UDPATE/DELETE Tables Tables and view AFTER Truncate ---------- -------------------- --------- --------------- INSERT/UDPATE/DELETE Views INSTEAD OF Truncate"},{"location":"Databases/sql/8%20Functions/triggers/#create-your-own-trigger-in-postgresql","title":"Create your own Trigger in PostgreSQL","text":"<pre><code>CREATE FUNCTION trigger_function( ) \n    RETURNS TRIGGER LANGUAGE PLPGSQL\nAS $$\nBEGIN\n    -- TRIGGER LOGIC\nEND;\n$$\n</code></pre>"},{"location":"Databases/sql/8%20Functions/triggers/#syntax","title":"Syntax","text":"<pre><code>CREATE TRIGGER trigger_name {BEFORE|AFTER} {EVENT} \nON table_name \n    [FOR [EACH] {ROW | STATEMENT}]\n    EXECUTE PROCEDURE trigger_function\n\n-- FOR EACH [ROW|STATEMENT]\n</code></pre>"},{"location":"Databases/sql/8%20Functions/triggers/#data-auditing-with-triggers","title":"Data Auditing with Triggers","text":""},{"location":"Databases/sql/8%20Functions/triggers/#setup-example-tables","title":"Setup Example tables","text":"<pre><code>CREATE TABLE players (\n    player_id SERIAL PRIMARY KEY,\n    name VARCHAR(100)\n);\n\nCREATE TABLE player_audits (\n    player_audit_id SERIAL PRIMARY KEY,\n    player_id INT NOT NULL,\n    name VARCHAR(100) NOT NULL,\n    new_name VARCHAR(100) NOT NULL,\n    edit_date TIMESTAMP NOT NULL\n);\n</code></pre>"},{"location":"Databases/sql/8%20Functions/triggers/#setup-function-to-trigger","title":"Setup Function to TRIGGER","text":"<pre><code>-- Function executed by TRIGGER\n\nCREATE OR REPLACE FUNCTION fn_players_name_changes_log()\n    RETURNS TRIGGER\n    LANGUAGE PLPGSQL\n    AS\n$$\nBEGIN\n\n    IF NEW.name &lt;&gt; OLD.name THEN\n        INSERT INTO player_audits\n        ( player_id, name, new_name, edit_date ) \n        values ( OLD.player_id, OLD.name, NEW.name ,NOW() );\n    END IF;\n\n    RETURN NEW;\nEND;\n$$\n\n-- TRIGGER Definition\n\nCREATE TRIGGER trg_players_name_changes\n    BEFORE UPDATE \n    ON players\n    FOR EACH ROW\n    EXECUTE PROCEDURE fn_players_name_changes_log();\n</code></pre>"},{"location":"Databases/sql/8%20Functions/triggers/#dml-to-fire-above-trigger","title":"DML to fire above trigger","text":"<pre><code>INSERT INTO players (name) VALUES ('UDAY'),('YADAV');\n\nSELECT * FROM players;\nselect * from player_audits;\n\nUPDATE players\nSET name = 'UDAY 2'\nWHERE player_id = 2;\n</code></pre>"},{"location":"Databases/sql/8%20Functions/triggers/#another-trigger-example","title":"Another Trigger Example","text":"<pre><code>-- create table for example\n\nCREATE TABLE t_temperature_log (\n    id_temperature SERIAL PRIMARY KEY,\n    add_date TIMESTAMP,\n    temperature NUMERIC\n);\n\n-- create function for trigger to execute\n\nCREATE OR REPLACE FUNCTION fn_temperature_value_check_at_insert()\nRETURNS TRIGGER\nLANGUAGE PLPGSQL\nAS\n$$\nBEGIN\n\n    IF NEW.temperature &lt; -30 then\n        NEW.temperature = 0;\n    END IF;\n\n    RETURN NEW;\n\nEND;\n$$\n\n-- creating trigger\n\nCREATE TRIGGER trg_temperature_value_check_at_insert\nBEFORE INSERT \nON t_temperature_log\nFOR EACH ROW\nEXECUTE PROCEDURE fn_temperature_value_check_at_insert();\n\n-- Queries\n\nINSERT INTO t_temperature_log ( add_date, temperature )\nvalues ( '2020-10-01' , 10 );\n\nselect * from t_temperature_log;\n\nINSERT INTO t_temperature_log ( add_date, temperature )\nvalues ( '2020-10-01' , -33 );\n\nselect * from t_temperature_log;\n</code></pre>"},{"location":"Databases/sql/8%20Functions/triggers/more-on-triggers/","title":"More on Triggers","text":""},{"location":"Databases/sql/8%20Functions/triggers/more-on-triggers/#getting-internal-information","title":"Getting Internal Information","text":"<pre><code>CREATE OR REPLACE FUNCTION fn_trigger_variables_display()\nRETURNS TRIGGER\nLANGUAGE PLPGSQL\nAS\n$$\n\n    BEGIN\n\n        RAISE NOTICE 'TG_NAME: %', TG_NAME;\n        RAISE NOTICE 'TG_RELNAME: %', TG_RELNAME;\n        RAISE NOTICE 'TG_TABLE_SCHEMA: %', TG_TABLE_SCHEMA;\n        RAISE NOTICE 'TG_WHEN: %', TG_WHEN;\n        RAISE NOTICE 'TG_LEVEL: %', TG_LEVEL;\n        RAISE NOTICE 'TG_OP: %', TG_OP;\n        RAISE NOTICE 'TG_NARGS: %', TG_NARGS;\n        RAISE NOTICE 'TG_ARGV: %', TG_NAME;\n\n        RETURN NEW;\n    END;\n$$\n\nCREATE TRIGGER trg_trigger_variables_display\n    AFTER INSERT\n    ON t_temperature_log\n    FOR EACH ROW\n    EXECUTE PROCEDURE fn_trigger_variables_display();\n\nINSERT INTO t_temperature_log  ( add_date, temperature ) values ('2020-02-02', -40);\n</code></pre>"},{"location":"Databases/sql/8%20Functions/triggers/more-on-triggers/#disallow-delete-on-table","title":"Disallow DELETE on table","text":"<pre><code>-- create sample table\n\ncreate table test_delete (\n    id int\n);\n\ninsert into test_delete (id ) values (1),(2),(3);\n\nselect * from test_delete;\n\n-- creating function\n\ncreate or replace function fn_generic_cancel_op()\nreturns trigger\nlanguage plpgsql\nas\n$$\n    begin\n\n    if TG_WHEN = 'AFTER' THEN\n        raise exception 'you are not allowed to % rows in %.%', tg_op,tg_table_schema,tg_table_name;\n    end if;\n\n    raise notice '% on rows in %.% wont happen', tg_op, tg_table_schema, tg_table_name;\n    return null; \n\n    end;\n$$\n\n-- creating trigger : AFTER\n\nCREATE TRIGGER trg_disallow_delete\nAFTER DELETE\nON test_delete\nFOR EACH ROW\nEXECUTE PROCEDURE fn_generic_cancel_op();\n\ndelete from test_delete where id = 1;\n\n-- creating trigger : BEFORE\n\nCREATE TRIGGER trg_disallow_delete_before\nBEFORE DELETE\nON test_delete\nFOR EACH ROW\nEXECUTE PROCEDURE fn_generic_cancel_op();\n\ndelete from test_delete where id = 1;\n\n-- checking\n\nselect * from test_delete;\n</code></pre>"},{"location":"Databases/sql/8%20Functions/triggers/more-on-triggers/#disallow-truncating","title":"Disallow truncating","text":"<pre><code>CREATE TRIGGER trg_disallow_truncate_after\nAFTER TRUNCATE\nON test_delete\nFOR EACH STATEMENT\nEXECUTE PROCEDURE fn_generic_cancel_op();\n\n\nCREATE TRIGGER trg_disallow_truncate_beforE\nBEFORE TRUNCATE\nON test_delete\nFOR EACH STATEMENT\nEXECUTE PROCEDURE fn_generic_cancel_op();\n</code></pre>"},{"location":"Databases/sql/8%20Functions/triggers/more-on-triggers/#creating-audit-trigger","title":"Creating Audit Trigger","text":"<ul> <li>To log data changes to tables in a consistent and transparent manner.</li> </ul> <pre><code>CREATE TABLE  audit (\n    id INT\n);\n\nCREATE TABLE audit_log (\n    username TEXT,\n    add_time TIMESTAMP,\n    table_name TEXT,\n    operation TEXT,\n    row_before JSON,\n    row_after JSON\n);\n</code></pre> <ul> <li>Please not that new OLD are not null for DELETE and INSERT triggers.</li> </ul> <pre><code>CREATE OR REPLACE FUNCTION fn_audit_trigger()\nRETURNS TRIGGER \nLANGUAGE PLPGSQL\nAS\n$$\nBEGIN\n\n    DECLARE\n        old_row json = NULL;\n        new_row json = NULL;\n\n    BEGIN\n\n        IF TG_OP IN ('UPDATE','DELETE') THEN\n\n            old_row = row_to_json(OLD);\n\n\n        END IF;\n\n        IF TG_OP IN ('INSERT','UPDATE') THEN\n\n            new_row = row_to_json(NEW);\n\n        END IF;\n\n        INSERT INTO audit_log \n            ( username, add_time, table_name, operation, row_before, row_after )\n        values\n            (\n                session_user,\n                NOW(),\n                TG_TABLE_SCHEMA || '.' || TG_TABLE_NAME,\n                TG_OP,\n                old_row,\n                new_row\n            );\n\n        RETURN NEW;\n    END;    \n\nEND;\n$$\n\n-- bind trigger\n\nCREATE TRIGGER trg_audit_trigger\nAFTER INSERT OR UPDATE OR DELETE\nON audit\nFOR EACH ROW\nEXECUTE PROCEDURE fn_audit_trigger();\n\n-- Queries\n\ninsert into audit(id) values (1),(2),(3);\n\nupdate audit\nset id = '100'\nwhere id = 1;\n\ndelete from audit\nwhere id = 2;\n\nselect * from audit;\nselect * from audit_log;\n</code></pre>"},{"location":"Databases/sql/8%20Functions/triggers/more-on-triggers/#creating-conditional-triggers","title":"Creating Conditional Triggers","text":"<ul> <li>Created by using generic WHEN clause.</li> <li>With a WHEN clause, you can write some conditions except a subquery</li> </ul> <pre><code>-- sample table\n\nCREATE TABLE mytask (\n    task_id SERIAL PRIMARY KEY,\n    task text\n);\n\n-- trigger function\n\nCREATE OR REPLACE FUNCTION fn_cancel_with_message()\nRETURNS TRIGGER \nLANGUAGE PLPGSQL\nAS\n$$\nBEGIN\n\n    RAISE EXCEPTION '%', TG_ARGV[0];\n\n    RETURN NULL;\n\nEND;\n$$\n\n-- function binding to trigger\n\nCREATE TRIGGER trg_no_update \nBEFORE INSERT OR UPDATE OR DELETE OR TRUNCATE\nON mytask\nFOR EACH STATEMENT\nWHEN \n(\n    EXTRACT ('DOW' FROM CURRENT_TIMESTAMP) = 5\n    -- 5 means friday\n    AND CURRENT_TIME &gt; '12:00'\n)\nEXECUTE PROCEDURE fn_cancel_with_message('NO UPDATE ARE ALLOWED');\n\n-- Queries\n\nINSERT INTO mytask (task) values ('task 1'), ('task 2'), ('task 3');\n</code></pre>"},{"location":"Databases/sql/8%20Functions/triggers/more-on-triggers/#disallow-updating-primary-key-of-table","title":"Disallow updating Primary Key of table","text":"<pre><code>create table pg_table(\n    id serial primary key,\n    t text\n);\n\ninsert into pg_table(t) values ('t1'),('t2');\n\ncreate trigger disallow_pk_change\nafter update of id\non pg_table\nfor each row\nexecute procedure fn_generic_cancel_op();\n\nupdate pg_table \nset id = 10\nwhere id = 1;\n</code></pre>"},{"location":"Databases/sql/8%20Functions/triggers/more-on-triggers/#event-triggers","title":"Event Triggers","text":"<ul> <li>Event triggers are data-specific and not bind or attached to a table</li> <li>Unlike regular triggers they capture system level DLL events</li> <li>Event triggers can be BEFORE or AFTER triggers</li> <li>Trigger function can be written in any language except SQL</li> <li>Event triggers are disabled in the single user mode and can only be created by a superuser</li> <li> <p>Syntax : <code>CREATE EVENT TRIGGER trg_name</code></p> </li> <li> <p>Before creating an event trigger, we must have a function that the trigger will execute</p> </li> <li>The function must return a specifi type called <code>EVENT_TRIGGER</code></li> <li> <p>This function need not (and may not) return a valuel the return type serivces merely as s signal that the function is to be invoked as an event trigger.</p> </li> <li> <p>Can we create conditional event trigger ? Yes, using the when clause</p> </li> <li>Event trigger cannot be executed in an aborted transaction</li> </ul>"},{"location":"Databases/sql/8%20Functions/triggers/more-on-triggers/#event-trigger-events","title":"Event trigger events","text":"when explaination ddl_command_start This event occurs jsut BEFORE a CREATE, ALTER, or DROP DLL command is executed ddl_command_end This event occurs just AFTER a create, alter, or drop command has finished executing table_rewrite This event occurs just before a table is re written by some action of the commands <code>ALTER TABLE</code> and <code>ALTER TYPE</code>. sql_drop This evetn occurs just before the ddl_command_end eevent for the commands that frop database objects"},{"location":"Databases/sql/8%20Functions/triggers/more-on-triggers/#event-trigger-variables","title":"Event trigger variables","text":"<ul> <li><code>TG_TAG</code> :  this variable contains the 'TAG' or the command for which the trigger is executed.</li> <li><code>TG_EVENT</code> : This variable contains the event name, which can be ddl_command_start, ddl_comman_end, and sql_drop.</li> </ul>"},{"location":"Databases/sql/8%20Functions/triggers/more-on-triggers/#creating-an-auditing-event-trigger","title":"Creating an auditing event trigger","text":"<pre><code>CREATE TABLE audit_dll (\n    audit_ddl_id SERIAL PRIMARY KEY,\n    username TEXT,\n    ddl_event TEXT,\n    ddl_command TEXT,\n    ddl_add_time TIMESTAMPTZ\n);\n\nCREATE OR REPLACE FUNCTION fn_event_audit_ddl()\nRETURNS EVENT_TRIGGER\nLANGUAGE PLPGSQL\nSECURITY DEFINER \nAS\n$$\n    BEGIN\n\n        INSERT INTO public.audit_dll\n        (username, ddl_event, ddl_command, ddl_add_time)\n        VALUES \n        (session_user, TG_EVENT, TG_TAG, NOW());        \n\n        RAISE NOTICE 'DDL activity is created';\n\n    END;\n$$\n\n-- without condition\ncreate event trigger trg_event_audit_ddl_no_cond\non ddl_command_start\nexecute procedure fn_event_audit_ddl();\n\n-- with condition\n\ncreate event trigger trg_event_audit_ddl\non ddl_command_start\nwhen\n TAG IN ('CREATE TABLE')\nexecute procedure fn_event_audit_ddl();\n</code></pre>"},{"location":"Databases/sql/8%20Functions/triggers/more-on-triggers/#dont-allow-anyone-to-create-table-between-time","title":"Dont allow anyone to create table between time","text":"<pre><code>CREATE OR REPLACE FUNCTION fn_event_abort_create_table_func()\nRETURNS EVENT_TRIGGER\nLANGUAGE PLPGSQL\nSECURITY DEFINER\nAS\n$$\n    DECLARE\n        current_hour int = EXTRACT ('HOUR' FROM NOW());\n    BEGIN\n        IF current_hour between 4 and 16 then\n            RAISE EXCEPTION 'tables are not allowed to be created during 9-4';\n        END IF;\n    END;\n$$\n\nCREATE EVENT TRIGGER trg_event_create_table_function\nON ddl_command_start \nWHEN\n    TAG IN ('CREATE TABLE')\nEXECUTE PROCEDURE fn_event_abort_create_table_func();\n</code></pre>"},{"location":"Databases/sql/8%20Functions/triggers/more-on-triggers/#dropping-event-trigger","title":"Dropping event trigger","text":"<pre><code>drop event trigger trg_name;\n</code></pre>"},{"location":"Databases/sql/9%20Indexing/","title":"Indexing","text":""},{"location":"Databases/sql/9%20Indexing/#what-is-a-index-in-database","title":"What is a Index in Database","text":"<ul> <li>An index help improve the access of data in our database </li> <li>Indexed tuple point to the table page where the tuple is stored on disk.</li> <li>An Index is a data structure that allows faster access to the underlying table so that specific tuples can be found quickly . Here \"quickly\" means much faster than scanning the entire table and analysing every single tuple.</li> <li>They add a cost to running a query, consume more memory to maintain the data structure.</li> </ul> <pre><code>INDEX        : idx_table_name_column_name \nUNIQUE INDEX : idx_u_table_name_column_name\n</code></pre>"},{"location":"Databases/sql/9%20Indexing/#syntax","title":"Syntax","text":"<pre><code>create index index_name on table_name (col1,col2,.....)\n\n-- create index for only unique values in column\ncreate unique index index_name on table_name (col1,col2,.....)\n\n-- more descriptive method\ncreate index index_name on table_name [USING method]\n(\n    column_name [ASC|DESC] [NULLS {FIRST | LAST}],\n    ...\n);\n</code></pre> <pre><code>create index idx_orders_order_date \n on orders (order_date);\n\ncreate index idx_orders_ship_city \n on orders (ship_city);\n\ncreate index idx_orders_customer_id_order_id \n on orders (customer_id,order_id);\n\nCREATE INDEX idx_shippers_company_name\n    ON public.shippers USING btree\n    (company_name ASC NULLS LAST);\n\n                          Table \"public.orders\"\n      Column      |         Type          | Collation | Nullable | Default \n------------------+-----------------------+-----------+----------+---------\n order_id         | smallint              |           | not null | \n customer_id      | bpchar                |           |          | \n employee_id      | smallint              |           |          | \n order_date       | date                  |           |          | \n required_date    | date                  |           |          | \n shipped_date     | date                  |           |          | \n ship_via         | smallint              |           |          | \n freight          | real                  |           |          | \n ship_name        | character varying(40) |           |          | \n ship_address     | character varying(60) |           |          | \n ship_city        | character varying(15) |           |          | \n ship_region      | character varying(15) |           |          | \n ship_postal_code | character varying(10) |           |          | \n ship_country     | character varying(15) |           |          | \n\n-- indexes present on table\n\nIndexes:\n    \"pk_orders\" PRIMARY KEY, btree (order_id)\n    \"idx_orders_customer_id_order_id\" btree (customer_id, order_id)\n    \"idx_orders_order_date\" btree (order_date)\n    \"idx_orders_ship_city\" btree (ship_city)\nForeign-key constraints:\n    \"fk_orders_customers\" FOREIGN KEY (customer_id) \n       REFERENCES customers(customer_id)\n    \"fk_orders_employees\" FOREIGN KEY (employee_id) \n       REFERENCES employees(employee_id)\n    \"fk_orders_shippers\" FOREIGN KEY (ship_via) \n       REFERENCES shippers(shipper_id)\nReferenced by:\n    TABLE \"order_details\" CONSTRAINT \"fk_order_details_orders\" \n    FOREIGN KEY (order_id) REFERENCES orders(order_id)\n</code></pre>"},{"location":"Databases/sql/9%20Indexing/#get-all-indexes","title":"Get All Indexes","text":"<pre><code>select * from pg_indexes limit 3;\n\n schemaname |  tablename   |            indexname             | tablespace |                                                           indexdef                                                            \n------------+--------------+----------------------------------+------------+-------------------------------------------------------------------------------------------------------------------------------\n pg_catalog | pg_statistic | pg_statistic_relid_att_inh_index |            | CREATE UNIQUE INDEX pg_statistic_relid_att_inh_index ON pg_catalog.pg_statistic USING btree (starelid, staattnum, stainherit)\n pg_catalog | pg_type      | pg_type_oid_index                |            | CREATE UNIQUE INDEX pg_type_oid_index ON pg_catalog.pg_type USING btree (oid)\n pg_catalog | pg_type      | pg_type_typname_nsp_index        |            | CREATE UNIQUE INDEX pg_type_typname_nsp_index ON pg_catalog.pg_type USING btree (typname, typnamespace)\n\n\nselect * from pg_indexes where schemaname = 'public' limti 3;\n\n  schemaname | tablename |  indexname   | tablespace |                                    indexdef                                    \n------------+-----------+--------------+------------+--------------------------------------------------------------------------------\n public     | us_states | pk_usstates  |            | CREATE UNIQUE INDEX pk_usstates ON public.us_states USING btree (state_id)\n public     | customers | pk_customers |            | CREATE UNIQUE INDEX pk_customers ON public.customers USING btree (customer_id)\n public     | orders    | pk_orders    |            | CREATE UNIQUE INDEX pk_orders ON public.orders USING btree (order_id)\n</code></pre>"},{"location":"Databases/sql/9%20Indexing/#size-of-indexes","title":"Size of Indexes","text":"<pre><code>select pg_size_pretty(pg_indexes_size('tablename'));\n\n pg_size_pretty \n----------------\n 128 kB\n</code></pre>"},{"location":"Databases/sql/9%20Indexing/#stats-about-indexes","title":"Stats about indexes","text":"<pre><code>select * from postgres.pg_catalog.pg_stat_all_indexes;\n\n relid | indexrelid | schemaname |   relname    |           indexrelname           | idx_scan | idx_tup_read | idx_tup_fetch \n-------+------------+------------+--------------+----------------------------------+----------+--------------+---------------\n  2619 |       2696 | pg_catalog | pg_statistic | pg_statistic_relid_att_inh_index |     1592 |         1155 |          1155\n  1247 |       2703 | pg_catalog | pg_type      | pg_type_oid_index                |      924 |          924 |           911\n  1247 |       2704 | pg_catalog | pg_type      | pg_type_typname_nsp_index        |      162 |          120 |           120\n(3 rows)\n</code></pre>"},{"location":"Databases/sql/9%20Indexing/#drop-index","title":"Drop Index","text":"<pre><code>DROP INDEX [ concurrently ] \n[ IF EXISTS  ] INDEX_NAME  [ CASCADE | RESTRICT ];\n</code></pre> <ul> <li><code>CASCADE</code>: If object has dependent objects, you will also drop the dependent ones after dropping it.</li> <li><code>RESTRICT</code> : It denies the user to drop the index if a dependency exists</li> <li><code>CONCURRENTLY</code> : PostgreSQL will require exclusive lock over the whole table and block access until index is removed</li> </ul>"},{"location":"Databases/sql/9%20Indexing/#vacuum-analyze","title":"Vacuum analyze","text":"<p>\\ When a vacuum process runs, the space occupied by these dead tuples is marked reusable by other tuples. </p> <p>An \u201canalyze\u201d operation does what its name says \u2013 it analyzes the contents of a database's tables and collects statistics about the distribution of values in each column of every table.</p> <pre><code>vacuum analyze table_name;\n</code></pre>"},{"location":"Databases/sql/9%20Indexing/#rebuilding-indexes","title":"Rebuilding Indexes","text":"<pre><code>REINDEX ( VERBOSE ) INDEX concurrently idx_orders_ship_city;\n</code></pre>"},{"location":"Databases/sql/9%20Indexing/custom-indexes/","title":"Custom Indexes","text":"<p>Example : </p> <p>CREATE a index for social security number , which is the the format of 1111-222-nnnn, you have to index the last 4 characters</p> <pre><code>CREATE TABLE if not exists ssn\n(\n    ssn text\n);\n\nINSERT INTO ssn (ssn)\nvalues ('111-11-0100'),\n       ('222-22-0120'),\n       ('333-33-0140'),\n       ('444-44-0160');\n\nselect *\nfrom ssn;\n\nexplain\nselect *\nfrom ssn;\n\nCREATE OR REPLACE FUNCTION FN_FIX_SSN(TEXT) RETURNS text AS\n$$\nBEGIN\n    return substring($1, 8) || replace(substring($1, 1, 7), '-', '');\n\nend ;\n$$ LANGUAGE plpgsql IMMUTABLE;\n\nSELECT ssn, FN_FIX_SSN(ssn)\nfrom ssn;\n\ndrop function fn_ssn_compare cascade;\n\nCREATE OR REPLACE FUNCTION fn_ssn_compare(text,text) returns int as\n$$\n    BEGIN\n        if FN_FIX_SSN($1) &lt; FN_FIX_SSN($2) then return -1;\n        elseif FN_FIX_SSN($1) &gt; FN_FIX_SSN($2) then return 1;\n        else\n            return 0;\n        end if;\n\n    end;\n$$ language plpgsql immutable;\n\nCREATE OPERATOR CLASS op_class_ssn_ops1\nFOR TYPE text USING btree\nas\n        OPERATOR 1 &lt;,\n        OPERATOR 2 &lt;=,\n        OPERATOR 3 =,\n        OPERATOR 4 &gt;=,\n        operator 5 &gt;,\n        function 1 fn_ssn_compare(text,text);\n\nCREATE INDEX idx_ssn on ssn(ssn op_class_ssn_ops1);\n\nexplain\nselect *\nfrom ssn where ssn.ssn = '222-22-0120';\n\nset enable_seqscan = 'off';\nshow enable_seqscan;\n</code></pre>"},{"location":"Databases/sql/9%20Indexing/gin-index/","title":"GIN Index","text":"<ul> <li>GIN index stands for Generalised Invert Index.</li> <li>Speeds up full text searches</li> <li>A GIN index stores a set of (key, posting list) pairs, where a posting list is a set of row IDs in which the key occurs. The same row ID can appear in multiple posting lists, since an item can contain more than one key. </li> <li>Each key value is stored only once, so a GIN index is very compact for cases where the same key appears many times.</li> </ul>"},{"location":"Databases/sql/9%20Indexing/gin-index/#query","title":"Query","text":"<p><pre><code>select * from contacts_docs\nwhere body @&gt; '{\"first_name\":\"John\"}';\n\n\nexplain select * from contacts_docs\nwhere body @&gt; '{\"first_name\":\"John\"}';\n</code></pre> </p>"},{"location":"Databases/sql/9%20Indexing/gin-index/#creating-a-gin-index","title":"Creating a GIN Index","text":"<pre><code>create index idx_gin_contacts_docs_body on contacts_docs USING GIN(body);\n</code></pre>"},{"location":"Databases/sql/9%20Indexing/gin-index/#get-size-of-index","title":"Get Size of Index","text":"<pre><code>select pg_size_pretty((pg_relation_size('idx_gin_contacts_docs_body'::regclass))) \n    as index_name;\n</code></pre>"},{"location":"Databases/sql/9%20Indexing/gin-index/#using-jsonb_path_ops-better","title":"Using JSONB_PATH_OPS ( better )","text":"<pre><code>create index idx_gin_contacts_docs_body_cool\n    on contacts_docs USING GIN(body jsonb_path_ops);\n</code></pre>"},{"location":"Databases/sql/9%20Indexing/gin-index/#size-with-jsonb_path_ops","title":"Size with jsonb_path_ops","text":"<pre><code>select pg_size_pretty((pg_relation_size('idx_gin_contacts_docs_body_cool'::regclass))) \n    as index_name;\n</code></pre>"},{"location":"Databases/sql/9%20Indexing/gin-index/#on-specific-column-for-smaller-size-not-working","title":"On Specific column for smaller size ( not working )","text":"<pre><code>select pg_size_pretty((pg_relation_size('idx_gin_contacts_docs_body_fname'::regclass))) \n    as index_name;\n</code></pre>"},{"location":"Databases/sql/9%20Indexing/indexes/","title":"Indexes","text":""},{"location":"Databases/sql/9%20Indexing/indexes/#data-structure","title":"Data structure","text":"<ul> <li>Data structure used : B-Tree</li> <li>Self Balancing Index</li> <li>SELECT, INSERT, DELETE and sequential access  in logarithmic time</li> <li>Can be used for most of operations and column type</li> <li>supports unique condition</li> <li>Used in Primary Key</li> <li>Used with operators </li> <li>Used when pattern matching</li> </ul>"},{"location":"Databases/sql/9%20Indexing/indexes/#hash-index","title":"Hash Index","text":"<p>https://codingsight.com/hash-index-understanding-hash-indexes/</p> <ul> <li>for equality operators</li> <li>not for range </li> <li>Larger than btree in size</li> </ul>"},{"location":"Databases/sql/9%20Indexing/indexes/#brin-index","title":"Brin Index","text":"<ul> <li>block range index</li> <li>block data -&gt; min to max value</li> <li>smaller index</li> <li>less costly to maintain than btree index</li> <li>Can be used on very large table</li> </ul> <pre><code>create table t_big\n(\n    id   serial,\n    name text\n);\n\ndrop table t_big;\n\ninsert into t_big (name)\nselect 'adam'\nfrom generate_series(1, 2000000);\n\n\nCREATE INDEX CONCURRENTLY brin_index\n    ON public.t_big USING brin\n    (id);\n\ncreate index btree_index on t_big(id);\n\nselect pg_size_pretty(pg_total_relation_size('t_big'));\n\nselect pg_size_pretty(pg_indexes_size('t_big'));\n\ndrop index brin_index;\n\ndrop index btree_index;\n\nexplain analyse\nselect *\nfrom t_big\nwhere id = 9999;\n\nexplain analyse\nselect id\nfrom t_big\norder by id desc limit 100;\n</code></pre>"},{"location":"Databases/sql/9%20Indexing/indexes/#partial-index","title":"Partial Index","text":"<ul> <li>To performance of the query while reducing the index size.</li> </ul> <pre><code>create index if not exists partial_inx on t_big(id) where id &gt; 1000000;\n\n\nexplain analyse\nselect *\nfrom t_big\nwhere id = 10000033;\n\n\nexplain analyse\nselect *\nfrom t_big\nwhere id = 99999;\n</code></pre>"},{"location":"Databases/sql/9%20Indexing/indexes/#expression-index","title":"Expression Index","text":"<ul> <li>PostgreSQL will use this index when WHERE clause or ORDER BY clause in statement</li> <li>Very Expensive to use</li> </ul> <pre><code>CREATE TABLE IF NOT EXISTS t_dates AS\nSELECT d, repeat(md5(d::text),10) as padding\n    FROM generate_series\n        (timestamp '1800-01-01', timestamp '2100-01-01', interval '1 day') s(d);\n\nselect * from t_dates limit 10;\n\nvacuum analyse t_dates;\n\nEXPLAIN ANALYSE\nSELECT *\nFROM t_dates\nWHERE d BETWEEN '2001-01-01' AND '2001-01-31';\n\n\nCREATE INDEX IF NOT EXISTS idx_t_dates_d on t_dates (d);\n\nANALYSE t_dates;\n\nEXPLAIN ANALYSE\nSELECT *\nFROM t_dates\nWHERE d BETWEEN '2001-01-01' AND '2001-01-31';\n\nEXPLAIN ANALYSE\nSELECT *\nFROM t_dates\nWHERE EXTRACT(DAY FROM d) = 1;\n\nCREATE INDEX idx_expr_t_dates on t_dates (extract(day from d));\n\nANALYSE t_dates;\n\nEXPLAIN ANALYSE\nSELECT *\nFROM t_dates\nWHERE EXTRACT(DAY FROM d) = 1;\n</code></pre>"},{"location":"Databases/sql/9%20Indexing/indexes/#heap-index","title":"Heap Index","text":""},{"location":"Databases/sql/9%20Indexing/sql/","title":"SQL","text":""},{"location":"Databases/sql/9%20Indexing/sql/#lifetime-of-query","title":"Lifetime of Query","text":"<ul> <li>Parser : handles the textual form of the statement and verifies whether it is correct or not</li> <li>Re-writer : applying the syntactic rules to rewrite the original SQL statement.</li> <li>Optimiser : finding the fastest path to the data</li> <li>Executor : responsible for effectively going to the storage  and retrieving the data from the physical storage.</li> </ul>"},{"location":"Databases/sql/9%20Indexing/sql/#optimiser","title":"Optimiser","text":"<ul> <li>Finds all the paths and gets the path with cheapest COST</li> <li>LOWEST COST WINS !!</li> </ul>"},{"location":"Databases/sql/9%20Indexing/sql/#scan-nodes","title":"Scan Nodes","text":"<ul> <li>Nodes are available for :</li> <li>every operation</li> <li>every access methods</li> <li>Nodes are stack-able</li> <li>Parent Node ( cost = 0.00 ... )<ul> <li>Child Node</li> <li>Child Node</li> </ul> </li> <li> <p>Types of Nodes</p> </li> <li> <p>Sequential Scan</p> </li> <li>Index Scan, Index Only Scan, Bitmap Index Scan</li> <li>Nested Loop, Hash Join and Merge Join </li> <li>Gather and Merge parallel nodes</li> </ul> <p>Get All Node Types : <code>SELECT * FROM pg_am;</code></p>"},{"location":"Databases/sql/9%20Indexing/sql/#sequential-scan","title":"Sequential Scan","text":"<p>Performs a sequential scan on the whole table.</p> <p></p>"},{"location":"Databases/sql/9%20Indexing/sql/#index-scan","title":"Index Scan","text":"<ul> <li>Index is used to access Data</li> <li>Types</li> <li>Index scan</li> <li>Index only scan</li> <li>Bitmap Index</li> </ul>"},{"location":"Databases/sql/9%20Indexing/sql/#index-only-scan","title":"Index only scan","text":""},{"location":"Databases/sql/9%20Indexing/sql/#hash-join","title":"Hash Join","text":"<ul> <li>Used when joining tables </li> <li>Joins preformed on 2 table at a time, if more tables are joined together, the output at one join in treated as input to a subsequent join</li> <li>When joining large number of tables, the genetic query optimiser settings may effect what combinations of joins are considered.</li> </ul> <p>Types</p> <ul> <li>Inner Table : Build a hash table from the inner table, keyed by the join key.</li> <li>Outer Table : then scan the outer table checking if a corresponding value is present </li> </ul> <p>Memory Size ( used by sort operation and hash table ) : <code>4 MB</code></p> <p></p>"},{"location":"Databases/sql/9%20Indexing/unique-index/","title":"Unique Index","text":"<pre><code>CREATE UNIQUE INDEX idx_products_product_id\n    ON public.products USING btree\n    (product_id ASC NULLS LAST)\n    TABLESPACE pg_default;\n\n\nCREATE UNIQUE INDEX idx_u_emp_emp_id\n    ON public.employees USING btree\n    (employee_id ASC NULLS LAST)\n    TABLESPACE pg_default;\n\nCREATE UNIQUE INDEX idx_orders_customer_id_order_id\n    ON public.orders USING btree\n    (order_id ASC NULLS LAST, customer_id ASC NULLS LAST)\n    TABLESPACE pg_default;\n\n\nCREATE UNIQUE INDEX idx_u_emp_emp_id_hire_date\n    ON public.employees USING btree\n    (employee_id ASC NULLS LAST, hire_date ASC NULLS LAST)\n</code></pre>"},{"location":"Python/main/","title":"Introduction to Python","text":"<ul> <li>check version : <code>$python --version</code></li> <li>print hello world : <code>print(\"hello world\")</code></li> <li>single quotes in places where formating is needed or for shorter string</li> <li>double quotes for longer strings/paragraphs</li> <li><code>exit()</code> and <code>quit()</code> to end the intrepreter</li> <li>python commands as string : <code>$python -c 'print(\"Hello, World\")'</code></li> </ul>"},{"location":"Python/main/#input-output","title":"Input &amp; Output","text":"<pre><code>name = input(\"enter name \")\nprint(name)\nprint(\"name is {}\".format(name))\n</code></pre> <p>Check for internal functions <pre><code>print(dir(__builtins__))\n\n\"\"\"\n['ArithmeticError', 'AssertionError', 'AttributeError'...]\n\"\"\"\n\nprint(dir(math))\n\n\"\"\"\n['__doc__', '__loader__', '__name__', '__package__', '__spec__', \n'acos', 'acosh', 'asin', 'asinh', 'atan', 'atan2', 'atanh', \n'ceil', 'comb', 'copysign', 'cos', 'cosh', 'degrees', 'dist' ...]\n\"\"\"\n\nclass MyClassObject(object):\n    pass\n\ndir(MyClassObject)\n\n# run_hello.py\nif __name__ == '__main__':\n    from hello import say_hello\n    say_hello()\n</code></pre></p>"},{"location":"Python/main/#datatypes","title":"Datatypes","text":"<ul> <li>Python int/string has no limit, its bounded by system resource</li> <li>Float is defined by IEEE 754 double precision</li> <li>Char is string of length 1</li> <li><code>None</code> is a type to represent null</li> </ul>"},{"location":"Python/main/#types","title":"Types","text":"Datatype in Python<pre><code>a:int = 2\nprint(a)\nprint(type(a))\n\nb = 9223372036854775807\nprint(b)\nprint(type(b))\n\npi:float = 3.14\nprint(pi)\nprint(type(pi))\n\nc = 'A'\nprint(c)\nprint(type(c))\n\nname:str = 'John Doe'\nprint(name)\nprint(type(name))\n\nq:bool = True\nprint(q)\nprint(type(q))\n\nx = None\nprint(x)\nprint(type(x))\n\nprint(True + False) # 1\nprint(True*False)   # 0\n</code></pre> <p>Output</p> <pre><code>2\n&lt;class 'int'&gt;\n9223372036854775807\n&lt;class 'int'&gt;\n3.14\n&lt;class 'float'&gt;\nA\n&lt;class 'str'&gt;\nJohn Doe\n&lt;class 'str'&gt;\nTrue\n&lt;class 'bool'&gt;\nNone\n&lt;class 'NoneType'&gt;\n</code></pre> <p>Number <pre><code>int_num = 10 #int value\nfloat_num = 10.2 #float value\ncomplex_num = 3.14j #complex value\nlong_num = 1234567L #long value\n</code></pre></p> <p>String <pre><code>a_str = \"hello world\"\nprint(a_str[0]) #output will be first character. H\nprint(a_str[0:5]) #output will be first five characters. Hello\n</code></pre></p>"},{"location":"Python/main/#data-structures","title":"Data Structures","text":"<pre><code>a = (1,2,3)   # tuple a[0]\nb = [1,2,3]   # list\nc = {1,2,3}   # set\n</code></pre>"},{"location":"Python/main/#set","title":"Set","text":"<ul> <li>Are unordered, meaning that the elements in a set do not have a specific order.</li> <li>Are mutable, meaning that the elements in a set can be changed after they are created.</li> <li>Are denoted by curly braces {}</li> </ul> <pre><code>name = \"abracadabra\"\na = set(name)\nprint(a)\n\na.add('z')\n# {'a', 'c', 'r', 'b', 'z', 'd'}\n\n# Frozen Sets- They are immutable and new elements cannot added after its defined.\nb = frozenset('asdfagsa')\nprint(b)\n\n# Existence check\n2 in {1,2,3} # True\n\n# Intersection &amp; Union\nprint({1, 2, 3, 4, 5}.intersection({3, 4, 5, 6}))\nprint({1, 2, 3, 4, 5}.union({3, 4, 5, 6}))\nprint({1, 2, 3, 4}.symmetric_difference({2, 3, 5})) \n# {1, 4, 5}\n</code></pre>"},{"location":"Python/main/#list","title":"List","text":"<pre><code>names = ['Alice', 'Bob', 'Craig', 'Diana', 'Eric']\nnested_list = [['a', 'b', 'c'], [1, 2, 3]]\nprint(names[-1]) # Eric\nprint(names[-4]) # Bob\n\nnames.append(\"Sia\")\n# Outputs ['Alice', 'Bob', 'Craig', 'Diana', 'Eric', 'Sia']\n\nnames.insert(1, \"Nikki\")\n# Outputs ['Alice', 'Nikki', 'Bob', 'Craig', 'Diana', 'Eric', 'Sia']\n\nnames.remove(\"Bob\")\nprint(names) # Outputs ['Alice', 'Nikki', 'Craig', 'Diana', 'Eric', 'Sia']\n\nname.index(\"Alice\")\n# 0\n\na = [1, 1, 1, 2, 3, 4]\na.reverse()\n# [4, 3, 2, 1, 1, 1]\n# or\na[::-1]\n# [4, 3, 2, 1, 1, 1]\n\nfor name in names:\n    print (name)\n\n# can be an array of any data type or single data type.\nlist = [123, 'abcd', 10.2, 'd']\nlist1 = ['hello', 'world']\nprint(list)  # will output whole list. [123,'abcd',10.2,'d']\nprint(list[0:2])  # will output first two element of list. [123,'abcd']\n# will gave list1 two times. ['hello','world','hello','world']\nprint(list1 * 2)\nprint(list + list1)  # will gave concatenation of both the lists\n</code></pre>"},{"location":"Python/main/#tuple","title":"Tuple","text":"<ul> <li>Are ordered, meaning that the elements in a tuple have a specific order.</li> <li>Are immutable, meaning that the elements in a tuple cannot be changed once they are created.</li> <li>Are denoted by parentheses ().</li> </ul> <pre><code>one_member_tuple = tuple(['Only member'])\nip_address = ('10.20.30.40', 8080)\n</code></pre>"},{"location":"Python/main/#dictionary","title":"Dictionary","text":"<pre><code>state_capitals = {\n    'Arkansas': 'Little Rock',\n    'Colorado': 'Denver',\n    'California': 'Sacramento',\n    'Georgia': 'Atlanta'\n}\nca_capital = state_capitals['California']\n\nfor k in state_capitals.keys():\n    print('{} is the capital of {}'.format(state_capitals[k], k))\n\ndic={'name':'red','age':10}\nprint(dic) \n# will output all the key-value pairs. {'name':'red','age':10}\nprint(dic['name']) \n# will output only value with 'name' key. 'red'\nprint(dic.values()) \n# will output list of values in dic. ['red',10]\nprint(dic.keys()) \n# will output list of keys. ['name','age']\n</code></pre> <p>Default Dictionary <pre><code>from collections  import defaultdict\n\nstate = defaultdict( lambda : \"default\" )\n\nstate['1'] = \"uday\"\nstate['2'] = \"kunal\"\nstate['3'] = \"vishal\"\n\nprint(state['4'])\n# default\n</code></pre></p>"},{"location":"Python/main/#conversion","title":"Conversion","text":"<pre><code>a = 'hello'\nlist(a)     # ['h', 'e', 'l', 'l', 'o']\nset(a)      # {'o', 'e', 'l', 'h'}\ntuple(a)    # ('h', 'e', 'l', 'l', 'o')\n</code></pre>"},{"location":"Python/main/#operator","title":"Operator","text":"<pre><code>x = 0\ny = 1\nprint(x or y) # if x is False then y otherwise x  == 1\nprint(x and y) # if x is False then x otherwise y == 0\nprint(not x) # if x is True then False, otherwise True == True\n</code></pre> <p>Various Declaration</p> <pre><code>a, b, c = 1, 2, 3\na, b, _ = 1, 2, 3\n\nx = y = [7, 8, 9] \nx[0] == y[0] #true\nx[0] = 1\nprint(y[0]) # 1\n\n# nested list\nx = [1, 2, [3, 4, 5], 6, 7]\n</code></pre>"},{"location":"Python/main/#strings","title":"Strings","text":"<pre><code>normal = 'foo\\nbar' # foo \\n bar\nescaped = 'foo\\\\nbar' # foo\\nbar\nraw = r'foo\\nbar' # foo\\nbar\n</code></pre>"},{"location":"Python/main/#miscell-datatype","title":"Miscell Datatype","text":"<pre><code>## Functions\n\n- write on \u0002czjqqkd:2\u0003IS python pass by value or pass by reference\u0002czjqqkd:3\u0003\n\n**Declaration**\n\n```python\ndef myfunc():\n    a = 2\n    return a\n\nprint(myfunc)\n</code></pre> <ul> <li><code>pass</code> to continue with execution</li> </ul>"},{"location":"Python/main/#enum-datatype","title":"Enum Datatype","text":"<pre><code>from enum import Enum\n\nclass Color(Enum):\n    red = 1\n    green = 2\n    blue = 3\n\nprint(Color.red)\nprint(Color(2))\n</code></pre>"},{"location":"Python/main/#import-module","title":"Import Module","text":"<p>Programmatically accessing docstrings</p> <pre><code>\"\"\"This is the module docstring.\"\"\"\ndef sayHello():\n\"\"\"This is the function docstring.\"\"\"\nreturn 'Hello World'\n</code></pre> <pre><code>&gt;&gt;&gt; import helloWorld\n&gt;&gt;&gt; helloWorld.__doc__\n'This is the module docstring.'\n&gt;&gt;&gt; helloWorld.sayHello.__doc__\n'This is the function docstring.'\n</code></pre>"},{"location":"Python/main/#if-elif-and-else","title":"if, elif, and else","text":"<pre><code>number = 5\nif number &gt; 2:\n    print(\"Number is bigger than 2.\")\nelif number &lt; 2: \n    print(\"Number is smaller than 2.\")\nelse:\n    print(\"Number is 2.\")\n</code></pre>"},{"location":"Python/main/#looping-in-python","title":"Looping in Python","text":"<pre><code>i = 0\nwhile i &lt; 7:\n    print(i)\n    if i == 4:\n        print(\"Breaking from loop\")\n        break   \n    i += 1\n\n# 0 1 2 3 4 Breaking from loop\n\nfor i in range(0,6):\n    print(i)\n\n# 0 1 2 3 4 5\n\nfor index, item in enumerate(['one', 'two', 'three', 'four']):\n    print(index, '::', item)\n\n# 0 :: one\n# 1 :: two\n# 2 :: three\n# 3 :: four\n</code></pre> <p>-- 127</p>"}]}